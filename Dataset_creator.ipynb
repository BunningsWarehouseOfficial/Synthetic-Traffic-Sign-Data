{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageChops, ImageDraw, ImageOps, ImageFilter, ImageStat, ImageEnhance \n",
    "from skimage import io, color, exposure, transform\n",
    "import imutils\n",
    "import argparse\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import random\n",
    "import blend_modes\n",
    "\n",
    "original = True # Whether we are using the original exposure_manipulation code or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list with paths of all the files in the directory\n",
    "def load_paths(directory):\n",
    "    paths = []\n",
    "    for filename in os.listdir(directory): # Retrieve names of files in directory\n",
    "        # Concatenate filename with directory path\n",
    "        if not filename.startswith('.'):  # Ignore hidden files\n",
    "            paths.append(directory + \"/\" + filename)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rescale and pad the source image with whitespace to be a perfect square of fixed width\n",
    "def scale_image(image_path):\n",
    "    width = 450\n",
    "\n",
    "    # https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
    "    # Resizing the image\n",
    "    img = Image.open(image_path)\n",
    "    old_size = img.size # old_size is in (width, height) format\n",
    "    ratio = float(width) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    img = img.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    # Padding the image with whitespace\n",
    "    delta_w = width - new_size[0]\n",
    "    delta_h = width - new_size[1]\n",
    "    padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "    new_img = ImageOps.expand(img, padding, (255,255,255))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Alexandros Stergiou's \"find_borders()\"\n",
    "# Returns an alpha channel that matches the white background\n",
    "def create_alpha(img, alpha_channel):\n",
    "    # Read and decode the image contents for pixel access\n",
    "    pix = img.load()\n",
    "\n",
    "    # Note PIL indexes [x,y] while OpenCV indexes [y,x]\n",
    "    # alpha_channel must be indexed [y,x] to merge with OpenCV channels later\n",
    "\n",
    "    min = 200\n",
    "    width, height = img.size\n",
    "    # Loop through each row of the image\n",
    "    for y in range(0, height):\n",
    "        # First loop left to right\n",
    "        for x in range(0, width, 1):\n",
    "            # Retrieve a tuple with RGB values for this pixel\n",
    "            rgb = pix[x,y]\n",
    "            # Make transparent if the pixel is white (or light enough)\n",
    "            if rgb[0] >= min and rgb[1] >= min and rgb[2] >= min:\n",
    "                alpha_channel[y,x] = 0\n",
    "            # If pixel is not white then we've hit the sign so break out of loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Then loop backwards, right to left\n",
    "        for x in range(width-1, -1, -1):\n",
    "            rgb = pix[x,y]\n",
    "            if rgb[0] >= min and rgb[1] >= min and rgb[2] >= min:\n",
    "                alpha_channel[y,x] = 0\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return alpha_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Alexandros Stergiou's \"manipulate_images()\"\n",
    "# Delete the white background from the original sign\n",
    "def delete_background(image_path, save_path):\n",
    "    # Open the image using PIL (don't read contents yet)\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')  # Does this have any effect??\n",
    "\n",
    "    # Open the image again using OpenCV and split into its channels\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    channels = cv2.split(image)\n",
    "\n",
    "    # Create a fully opaque alpha channel, same dimentions and dtype as the image\n",
    "    # Create_alpha() modifies it to make the white background transparent\n",
    "    alpha_channel = np.ones(channels[0].shape, dtype=channels[0].dtype) * 255\n",
    "    alpha_channel = create_alpha(img, alpha_channel)\n",
    "\n",
    "    # Merge alpha channel into original image\n",
    "    image_RGBA = cv2.merge((channels[0], channels[1], channels[2], alpha_channel))\n",
    "\n",
    "    \n",
    "    cv2.imwrite(save_path, image_RGBA)\n",
    "\n",
    "    img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make white backgrounds transparent\n",
    "input_dir = \"Traffic_Signs_Templates/1_Images\"\n",
    "output_dir = \"Traffic_Signs_Templates/2_Processed_Images\"\n",
    "# Create the output directory if it doesn't exist already\n",
    "if (not os.path.exists(output_dir)):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# Get start time to measure the number of clock-cycles it takes\n",
    "start = cv2.getTickCount()\n",
    "\n",
    "paths = load_paths(input_dir)\n",
    "for path in paths:\n",
    "    img = scale_image(path) # Rescale the image\n",
    "\n",
    "    # Remove the extension and save as a png\n",
    "    _, filename = ntpath.split(path)\n",
    "    name, _ = filename.rsplit('.', 1)\n",
    "    save_path = output_dir + \"/\" + name + \".png\"\n",
    "    img.save(save_path)\n",
    "\n",
    "    delete_background(save_path, save_path) # Overwrite the newly rescaled image\n",
    "\n",
    "# Get end time\n",
    "end = cv2.getTickCount()\n",
    "cycles = (end - start) / cv2.getTickFrequency()\n",
    "print(\">>> This took:\", cycles, \"clock-cycles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlays foreground image on background image, keeping transparency\n",
    "# x1, y1: top-left coordinate to place foreground\n",
    "# Image overlay code credit to fireant:\n",
    "# https://stackoverflow.com/questions/14063070/overlay-a-smaller-image-on-a-larger-image-python-opencv\n",
    "# RGB to RGBA conversion credit to kaanoner:\n",
    "# https://stackoverflow.com/questions/32290096/python-opencv-add-alpha-channel-to-rgb-image\n",
    "# https://docs.opencv.org/3.4.2/d3/df2/tutorial_py_basic_ops.html used as reference\n",
    "def overlay(fg, bg, x1 = -1, y1 = -1):\n",
    "    # If the background doesn't have an alpha channel, add one, but keep the entire image opaque\n",
    "    # Code from kaanoner\n",
    "    if len(cv2.split(bg)) == 3:\n",
    "        bg = cv2.cvtColor(bg, cv.COLOR_RGB2RGBA)\n",
    "        bg[:, :, 3] = 255\n",
    "    # Make a copy of the background for making changes\n",
    "    newImage = bg.copy()\n",
    "\n",
    "    # Retrieve image dimentions\n",
    "    heightFG, widthFG, _ = fg.shape  # Discard channel\n",
    "    heightBG, widthBG, _ = bg.shape\n",
    "\n",
    "    # If either of the coordinates were omitted, calculate start/end positions\n",
    "    # using the difference in image size, centring foreground on background\n",
    "    if x1 == -1 or y1 == -1:\n",
    "        # Calculate start coordinates \n",
    "        x1 = (widthBG - widthFG) // 2    # floor division to truncate as\n",
    "        y1 = (heightBG - heightFG) // 2  # coordinates don't need to be exact\n",
    "    # Calculate end coordinates\n",
    "    x2 = x1 + widthFG\n",
    "    y2 = y1 + heightFG\n",
    "\n",
    "    ### Start of code from fireant ###\n",
    "    # Retrieve an array of alpha values from the foreground image\n",
    "    # Divide by 255 to get values between 0.0 and 1.0\n",
    "    alpha = fg[:, :, 3] / 255.0\n",
    "    beta = 1.0 - alpha\n",
    "\n",
    "    # Loop over BGR channels (but not alpha)\n",
    "    for ch in range(0, 3):\n",
    "        newImage[y1:y2, x1:x2, ch] = (alpha * fg[:, :, ch] +\n",
    "                                      beta * newImage[y1:y2, x1:x2, ch])\n",
    "    ### End of code from fireant ###\n",
    "\n",
    "    return newImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the first image if it is larger than the second image\n",
    "def resize(img1, img2):\n",
    "    # retrieve dimentions of both images\n",
    "    height1, width1, _ = img1.shape  # discard channel\n",
    "    height2, width2, _ = img2.shape\n",
    "\n",
    "    # resize if foreground is taller than background\n",
    "    if height1 > height2:\n",
    "        img1 = resizeOnHeight(img1, height2)\n",
    "        height1, width1, _ = img1.shape  #re-retrieve dimentions\n",
    "    # or wider\n",
    "    if width1 > width2:\n",
    "        img1 = resizeOnWidth(img1, width2)\n",
    "\n",
    "    return img1, img2\n",
    "\n",
    "def resizeOnHeight(img, newHeight):\n",
    "    oldHeight, oldWidth, _ = img.shape  # discard channel\n",
    "    newWidth = int( round( newHeight / oldHeight * oldWidth ) )\n",
    "    img = cv2.resize(img, (newHeight, newWidth))\n",
    "    return img\n",
    "\n",
    "def resizeOnWidth(img, newWidth):\n",
    "    oldWidth, oldHeight, _ = img.shape  # discard channel\n",
    "    newHeight = int( round( newWidth / oldWidth * oldHeight ) )\n",
    "    img = cv2.resize(img, (newWidth, newHeight))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the ratio of opacity in first image compared to second image\n",
    "def calcRatio(fg, bg):\n",
    "    fgPixels = countPixels(fg)  # number of non-transparent pixels in graffiti\n",
    "    bgPixels = countPixels(bg)  # total number of pixels, dependent on sign shape\n",
    "    ratio = fgPixels / bgPixels\n",
    "\n",
    "    return ratio\n",
    "\n",
    "# return the number of non-transparent pixels in the imported image\n",
    "def countPixels(img):\n",
    "    count = 0\n",
    "    split = cv2.split(img)\n",
    "    if len(split) is 4:  # only proceed if the image has an alpha channel\n",
    "        alpha = split[3]\n",
    "        # loop through alpha channel, incrementing count if not opaque\n",
    "        for ii in range(0, len(alpha)):\n",
    "            for jj in range(0, len(alpha[0])):\n",
    "                if alpha[ii][jj] > 0:\n",
    "                    count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the ratio of changed opaque pixels between two versions of the same image for each quadrant\n",
    "def calcQuadrantDiff(new, original):\n",
    "    height, width, ch = original.shape\n",
    "    centre_x = int(width / 2)\n",
    "    centre_y = int(height / 2)\n",
    "\n",
    "    # Dividing the two images into quadrants\n",
    "    new_I   = new[0:centre_y, centre_x:width]\n",
    "    new_II  = new[0:centre_y, 0:centre_x]\n",
    "    new_III = new[centre_y:height, 0:centre_x]\n",
    "    new_IV  = new[centre_y:height, centre_x:width]\n",
    "    original_I   = original[0:centre_y, centre_x:width]\n",
    "    original_II  = original[0:centre_y, 0:centre_x]\n",
    "    original_III = original[centre_y:height, 0:centre_x]\n",
    "    original_IV  = original[centre_y:height, centre_x:width]\n",
    "\n",
    "    ratio_I   = countDiffPixels(new_I, original_I) / countPixels(original_I)\n",
    "    ratio_II  = countDiffPixels(new_II, original_II) / countPixels(original_II)\n",
    "    ratio_III = countDiffPixels(new_III, original_III) / countPixels(original_III)\n",
    "    ratio_IV  = countDiffPixels(new_IV, original_IV) / countPixels(original_IV)\n",
    "    return [ratio_I, ratio_II, ratio_III, ratio_IV]\n",
    "\n",
    "# Modified version of countPixels(img) which counts how many opaque pixels are different between two images\n",
    "def countDiffPixels(new, original):\n",
    "    count = 0\n",
    "    split = cv2.split(original)\n",
    "    if len(split) is 4:\n",
    "        alpha = split[3]\n",
    "        for ii in range(0, len(new)):\n",
    "            for jj in range(0, len(new[0])):\n",
    "                if alpha[ii][jj] > 0: # The pixel in the original image must have been opaque\n",
    "                    if np.any(new[ii][jj] != original[ii][jj]): # Check for any changes in the pixel\n",
    "                        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_types = [\"_ORIGINAL\", \"_QUADRANT\", \"_BIG_HOLE\", \"_HOLES\",# \"_YELLOW\",\n",
    "                \"_GRAFFITI\"] #Used for SGTSD\n",
    "\n",
    "#Remove trailing zeroes and prevent automatic scientific exponent notation https://stackoverflow.com/a/37736333/12350950\n",
    "def floatToString(num):\n",
    "    return (\"%.6f\" % num).rstrip(\"0\").rstrip(\".\")\n",
    "\n",
    "def damage_images(paths):\n",
    "    for image_path in paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        img = img.astype('uint8')\n",
    "        height, width, ch = img.shape\n",
    "        centre_x = int(width / 2)\n",
    "        centre_y = int(height / 2)\n",
    "    \n",
    "        #Creating Mask\n",
    "        sign = img.copy() \n",
    "        alpha = sign[:, :, 3] #Extract the alpha channel from sign\n",
    "        _, alpha = cv2.threshold(alpha, 5, 255, cv2.THRESH_BINARY) #Remove gradual transparency\n",
    "        mask = alpha\n",
    "        \n",
    "        #Info for File Writing\n",
    "        ii = 0 #Used for label filenames\n",
    "        path = \"Traffic_Signs_Templates/3_Damaged_Images/\"\n",
    "        head, tail = ntpath.split(image_path)\n",
    "        title, extension = tail.split('.')\n",
    "\n",
    "        direct = \"SGTSD/Labels/\"\n",
    "        head, tail = image_path.split('.')\n",
    "        name = head.split('/')\n",
    "        c_num = name[-1] #name[-1] is the sign's name, which should be a class number\n",
    "        if (not os.path.exists(direct + c_num)):\n",
    "            os.makedirs(direct + c_num)\n",
    "        \n",
    "        \n",
    "        #ORIGINAL\n",
    "        if \"_ORIGINAL\" in damage_types:\n",
    "            dmg = img\n",
    "            \n",
    "            dmg_values = calcQuadrantDiff(dmg, img)\n",
    "            values = [floatToString(round(num, 6)) for num in dmg_values] #Convert the values into decimal strings\n",
    "            text_file = open(direct + c_num + \"/\" + c_num + \"_\" + str(ii) + \".txt\", \"w\")\n",
    "            text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]}\")\n",
    "            ii += 1\n",
    "            cv2.imwrite(path + title + \"_ORIGINAL.png\", dmg)\n",
    "        \n",
    "        \n",
    "        #QUADRANT\n",
    "        if \"_QUADRANT\" in damage_types:\n",
    "            quadrant = mask.copy()\n",
    "            quad_num = random.randint(1, 4) #Selecting which quadrant it will be\n",
    "            #Removing the quadrant: -1 offset is necessary to avoid damaging part of a wrong quadrant\n",
    "            if quad_num is 1:\n",
    "                cv2.rectangle(quadrant, (width, 0), (centre_x, centre_y - 1), (0,0,0), -1)\n",
    "            if quad_num is 2:\n",
    "                cv2.rectangle(quadrant, (0, 0), (centre_x - 1, centre_y - 1), (0,0,0), -1)\n",
    "            if quad_num is 3:\n",
    "                cv2.rectangle(quadrant, (0, height), (centre_x - 1, centre_y), (0,0,0), -1)\n",
    "            if quad_num is 4:\n",
    "                cv2.rectangle(quadrant, (width, height), (centre_x, centre_y), (0,0,0), -1)\n",
    "            dmg1 = cv2.bitwise_and(img, img, mask=quadrant)\n",
    "            \n",
    "            dmg_values = calcQuadrantDiff(dmg1, img)\n",
    "            values = [floatToString(round(num, 6)) for num in dmg_values] #Convert the values into decimal strings\n",
    "            text_file = open(direct + c_num + \"/\" + c_num + \"_\" + str(ii) + \".txt\", \"w\")\n",
    "            text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]}\")\n",
    "            ii += 1\n",
    "            cv2.imwrite(path + title + \"_QUADRANT.png\", dmg1)\n",
    "        \n",
    "        \n",
    "        #BIG HOLE\n",
    "        if \"_BIG_HOLE\" in damage_types:\n",
    "            big_hole = mask.copy()\n",
    "            angle = random.randint(0, 359) #angle from the x axis counter-clockwise through quadrant I\n",
    "            r = int(2 * height / 5) #radius\n",
    "            rad = -(angle * math.pi / 180)\n",
    "            x = centre_x + int(r * math.cos(rad)) #x coordinate of centre\n",
    "            y = centre_y + int(r * math.sin(rad)) #y coordinate of centre\n",
    "            cv2.circle(big_hole, (x,y), r, (0,0,0), -1)\n",
    "            dmg2 = cv2.bitwise_and(img, img, mask=big_hole)\n",
    "\n",
    "            dmg_values = calcQuadrantDiff(dmg2, img)\n",
    "            values = [floatToString(round(num, 6)) for num in dmg_values] #Convert the values into decimal strings\n",
    "            text_file = open(direct + c_num + \"/\" + c_num + \"_\" + str(ii) + \".txt\", \"w\")\n",
    "            text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]} angle={angle}\")\n",
    "            ii += 1\n",
    "            cv2.imwrite(path + title + \"_BIG_HOLE.png\", dmg2)\n",
    "        #FIXME: Images need to be square, with sign centred\n",
    "        \n",
    "        \n",
    "        #RANDOMISED BULLET HOLES\n",
    "        if \"_HOLES\" in damage_types:\n",
    "            bullet_holes = mask.copy()\n",
    "            painted = img.copy() #Another copy of original sign to draw 'flaking paint' from holes on\n",
    "            numHoles = random.randint(7, 30)\n",
    "            for x in range(numHoles):\n",
    "                size = random.randint(2, 12) #Random hole size\n",
    "                h_x = random.randint(0, height) #Random hole position\n",
    "                h_y = random.randint(0, height)\n",
    "                c = random.randint(0, 150) #How black/grey the 'hole' is if it didn't penetrate\n",
    "                s = random.uniform(1.6, 2.2) #Random annulus size\n",
    "\n",
    "                C = 200 #Colour of damaged 'paint' outer annulus\n",
    "                cv2.circle(painted, (h_x, h_y), int(size * s), (C,C,C,255), -1) #Hole annulus to represent damaged 'paint'\n",
    "                if (size < 6): #Did the bullet penetrate through the sign?\n",
    "                    cv2.circle(painted, (h_x, h_y), size, (c,c,c,255), -1) #If not, grey out rather than make transparent\n",
    "                else:\n",
    "                    cv2.circle(bullet_holes, (h_x, h_y), size, (0,0,0), -1)\n",
    "            dmg3 = cv2.bitwise_and(painted, painted, mask=bullet_holes)\n",
    "            \n",
    "            dmg_values = calcQuadrantDiff(dmg3, img)\n",
    "            values = [floatToString(round(num, 6)) for num in dmg_values] #Convert the values into decimal strings\n",
    "            text_file = open(direct + c_num + \"/\" + c_num + \"_\" + str(ii) + \".txt\", \"w\")\n",
    "            text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]} holes={numHoles}\")\n",
    "            ii += 1\n",
    "            cv2.imwrite(path + title + \"_HOLES.png\", dmg3)\n",
    "        \n",
    "        \n",
    "        #TINTED YELLOW\n",
    "        if \"_YELLOW\" in damage_types:\n",
    "            yellow = np.zeros((height,width,ch), np.uint8)\n",
    "            yellow[:,:] = (0,210,210,255)\n",
    "            dmg4 = cv2.bitwise_and(img, yellow)\n",
    "            \n",
    "            #TODO: Use quadrant pixel difference ratio or some other damage metric for labelling?\n",
    "            cv2.imwrite(path + title + \"_YELLOW.png\", dmg4)\n",
    "            \n",
    "            \n",
    "        #GRAFFITI\n",
    "        if \"_GRAFFITI\" in damage_types:\n",
    "            graffiti = \"graffiti_black.png\"\n",
    "            fg = cv2.imread(\"Traffic_Signs_Templates/Graffiti/\" + graffiti, cv2.IMREAD_UNCHANGED)\n",
    "            bg = img.copy()\n",
    "            fg, bg = resize(fg, bg) #Correct graffiti size to fit onto sign\n",
    "            dmg5 = overlay(fg, bg)\n",
    "            ratio = calcRatio(fg, bg) #Calculate the ratio of obscurity\n",
    "            \n",
    "            #Ratio is displayed in filename as a float without the first \"0.\" part\n",
    "            #TODO: check ratio == 1.0 edge case\n",
    "            #cv2.imwrite(path + title + \"_GRAFFITI-0,\" + str(int(round(ratio, 3) * 10**3)) + \".png\", dmg5)\n",
    "            dmg_values = calcQuadrantDiff(dmg5, img)\n",
    "            values = [floatToString(round(num, 6)) for num in dmg_values] #Convert the values into decimal strings\n",
    "            text_file = open(direct + c_num + \"/\" + c_num + \"_\" + str(ii) + \".txt\", \"w\")\n",
    "            text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]} graffiti='{graffiti}'\")\n",
    "            ii += 1\n",
    "            cv2.imwrite(path + title + \"_GRAFFITI.png\", dmg5)\n",
    "            \n",
    "        \n",
    "        #FADE (doesn't work as a damage type with Stergiou's exposure_manipulation function - also outdated)\n",
    "#         if \"_FADE\" in damage_types:\n",
    "#             #Retrieve alpha data from original image\n",
    "#             splitImg = cv2.split(img)\n",
    "#             if len(splitImg) is 4:\n",
    "#                 alphaData = splitImg[3]\n",
    "#                \n",
    "#             for ii in range(5):\n",
    "#                 dmg6 = img.copy()\n",
    "#                 alpha = 1 - (ii * 0.19)\n",
    "#                 beta = (ii + 1) * 40\n",
    "#                 cv2.convertScaleAbs(img, dmg6, alpha, beta) #Scale the contrast and brightness\n",
    "#                 dmg6[:, :, 3] = alphaData\n",
    "#                \n",
    "#                 cv2.imwrite(path + title + \"_FADE-\" + str(ii) + \".png\", dmg6)\n",
    "            \n",
    "        \n",
    "        #TODO: CRACKS (thin crack lines across the sign?)\n",
    "        #TODO: MISSING SECTIONS (missing polygon sections on edges of sign?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating damaged sign templates based on processed images of original templates\n",
    "dmg_dir = \"Traffic_Signs_Templates/3_Damaged_Images\"\n",
    "if os.path.exists(dmg_dir):\n",
    "    shutil.rmtree(dmg_dir) #Clear out any old damage variations\n",
    "os.mkdir(dmg_dir) #Recreate the directory for damaged images\n",
    "paths = load_paths(\"Traffic_Signs_Templates/2_Processed_Images\")\n",
    "damage_images(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(paths):\n",
    "    for image_path in paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        rows,cols,ch = img.shape\n",
    "        t = []\n",
    "        for i in range(0,100):\n",
    "            t.append(i)\n",
    "\n",
    "        #FORWARD FACING\n",
    "        dst = img\n",
    "        \n",
    "        #EAST FACING\n",
    "        pts1 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols/10,rows/2]])\n",
    "        pts2 = np.float32([[cols/5,rows/5],[cols/2,rows/8],[cols/5,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        dst1 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #NORTH-WEST FACING\n",
    "        pts3 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts4 = np.float32([[cols*4.5/5,rows/5],[cols/2,rows/8],[cols*4.5/5,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts3,pts4)\n",
    "        dst2 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #LEFT TILTED FORWARD FACING\n",
    "        pts5 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols/10,rows/2]])\n",
    "        pts6 = np.float32([[cols/12,rows/6],[cols/2.1,rows/8],[cols/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts5,pts6)\n",
    "        dst3 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #RIGHT TILTED FORWARD FACING\n",
    "        pts7 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts8 = np.float32([[cols*10/12,rows/6],[cols/2.2,rows/8],[cols*8.4/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts7,pts8)\n",
    "        dst4 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #WEST FACING\n",
    "        pts9 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts10 = np.float32([[cols/9.95,rows/10],[cols/2.05,rows/9.95],[cols*9/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts9,pts10)\n",
    "        dst5 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #RIGHT TILTED FORWARD FACING\n",
    "        pts11 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts12 = np.float32([[cols*9/10,rows/10],[cols/2,rows/9],[cols*8.95/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts11,pts12)\n",
    "        dst6 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION\n",
    "        pts13 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts14 = np.float32([[cols/9.8,rows/9.8],[cols/2,rows/9.8],[cols*8.8/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts13,pts14)\n",
    "        dst7 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 2\n",
    "        pts15 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts16 = np.float32([[cols/11,rows/10],[cols/2.1,rows/10],[cols*8.5/10,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts15,pts16)\n",
    "        dst8 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 3\n",
    "        pts17 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts18 = np.float32([[cols/11,rows/11],[cols/2.1,rows/10],[cols*10/11,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts17,pts18)\n",
    "        dst9 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 4\n",
    "        pts19 = np.float32([[cols*9.5/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts20 = np.float32([[cols*9.35/10,rows/9.99],[cols/2.05,rows/9.95],[cols*9.05/10,rows/2.03]])\n",
    "        M = cv2.getAffineTransform(pts19,pts20)\n",
    "        dst10 = cv2.warpAffine(img,M,(cols,rows))\n",
    "         \n",
    "        #FORWARD FACING W/ DISTORTION 5\n",
    "        pts21 = np.float32([[cols*9.5/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts22 = np.float32([[cols*9.65/10,rows/9.95],[cols/1.95,rows/9.95],[cols*9.1/10,rows/2.02]])\n",
    "        M = cv2.getAffineTransform(pts21,pts22)\n",
    "        dst11 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 6\n",
    "        pts23 = np.float32([[cols*9.25/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts24 = np.float32([[cols*9.55/10,rows/9.85],[cols/1.9,rows/10],[cols*9.3/10,rows/2.04]])\n",
    "        M = cv2.getAffineTransform(pts23,pts24)\n",
    "        dst12 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #SHRINK 1\n",
    "        pts25 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts26 = np.float32([[cols*8/10,rows/10],[cols*1.34/3,rows/10.5],[cols*8.24/10,rows/2.5]])\n",
    "        M = cv2.getAffineTransform(pts25,pts26)\n",
    "        dst13 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #SHRINK 2\n",
    "        pts27 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts28 = np.float32([[cols*8.5/10,rows*3.1/10],[cols/2,rows*3/10],[cols*8.44/10,rows*1.55/2.5]])\n",
    "        M = cv2.getAffineTransform(pts27,pts28)\n",
    "        dst14 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 7\n",
    "        pts29 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts30 = np.float32([[cols*8.85/10,rows/9.3],[cols/1.9,rows/10.5],[cols*8.8/10,rows/2.11]])\n",
    "        M = cv2.getAffineTransform(pts29,pts30)\n",
    "        dst15 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 8\n",
    "        pts31 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts32 = np.float32([[cols*8.75/10,rows/9.1],[cols/1.95,rows/8],[cols*8.5/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts31,pts32)\n",
    "        dst16 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 9\n",
    "        pts33 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts34 = np.float32([[cols*8.75/10,rows/9.1],[cols/1.95,rows/9],[cols*8.5/10,rows/2.2]])\n",
    "        M = cv2.getAffineTransform(pts33,pts34)\n",
    "        dst17 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 10\n",
    "        pts35 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts36 = np.float32([[cols*8.75/10,rows/8],[cols/1.95,rows/8],[cols*8.75/10,rows/2]])\n",
    "        M = cv2.getAffineTransform(pts35,pts36)\n",
    "        dst18 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 11\n",
    "        pts37 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts38 = np.float32([[cols*8.8/10,rows/7],[cols/1.95,rows/7],[cols*8.8/10,rows/2]])\n",
    "        M = cv2.getAffineTransform(pts37,pts38)\n",
    "        dst19 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        head, tail = ntpath.split(image_path)\n",
    "        title, extension = tail.split('.')\n",
    "        name = title.split('_') #name[0] gives us the sign number\n",
    "        \n",
    "        path = \"Traffic_Signs_Templates/4_Transformed_Images/\" + name[0] + \"/\" + title + \"/\"\n",
    "        cv2.imwrite(path + str(1) + \".png\", dst)\n",
    "        cv2.imwrite(path + str(2) + \".png\", dst1)\n",
    "        cv2.imwrite(path + str(3) + \".png\", dst2)\n",
    "        cv2.imwrite(path + str(4) + \".png\", dst3)\n",
    "        cv2.imwrite(path + str(4) + \".png\", dst4)\n",
    "        cv2.imwrite(path + str(5) + \".png\", dst5)\n",
    "        cv2.imwrite(path + str(6) + \".png\", dst6)\n",
    "        cv2.imwrite(path + str(7) + \".png\", dst7)\n",
    "        cv2.imwrite(path + str(8) + \".png\", dst8)\n",
    "        cv2.imwrite(path + str(9) + \".png\", dst9)\n",
    "        cv2.imwrite(path + str(10) + \".png\", dst10)\n",
    "        cv2.imwrite(path + str(11) + \".png\", dst11)\n",
    "        cv2.imwrite(path + str(12) + \".png\", dst12)\n",
    "        cv2.imwrite(path + str(13) + \".png\", dst13)\n",
    "        cv2.imwrite(path + str(14) + \".png\", dst14)\n",
    "        cv2.imwrite(path + str(15) + \".png\", dst15)\n",
    "        cv2.imwrite(path + str(16) + \".png\", dst16)\n",
    "        cv2.imwrite(path + str(17) + \".png\", dst17)\n",
    "        cv2.imwrite(path + str(18) + \".png\", dst18)\n",
    "        cv2.imwrite(path + str(19) + \".png\", dst19)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory1 = 'Traffic_Signs_Templates/1_Images'\n",
    "directory2 = 'Traffic_Signs_Templates/3_Damaged_Images' #Changed from /2_Processed_Images\n",
    "if (not os.path.exists(\"Traffic_Signs_Templates/4_Transformed_Images\")):\n",
    "    for path1 in load_paths(directory1):\n",
    "        head, tail = ntpath.split(path1)\n",
    "        title, extenstion = tail.split('.')\n",
    "        os.makedirs(\"Traffic_Signs_Templates/4_Transformed_Images/\" + title)\n",
    "    for path2 in load_paths(directory2):\n",
    "        head, tail = ntpath.split(path2)    \n",
    "        title, extension = tail.split('.')\n",
    "        name = title.split('_') #name[0] gives us the sign number\n",
    "        os.makedirs(\"Traffic_Signs_Templates/4_Transformed_Images/\" + name[0] + \"/\" + title)\n",
    "paths = load_paths(directory2)\n",
    "img_transform(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_exposure(paths,channels):\n",
    "    exposures = []\n",
    "    for image_path in paths:\n",
    "        img = Image.open(image_path)\n",
    "        im = Image.open(image_path).convert('LA')\n",
    "        \n",
    "        stat = ImageStat.Stat(im)\n",
    "        \n",
    "        #Average pixel brighness\n",
    "        avg = stat.mean[0]\n",
    "        \n",
    "        #RMS pixel brighness\n",
    "        rms = stat.rms[0]\n",
    "        \n",
    "        stat2 = ImageStat.Stat(img)\n",
    "        \n",
    "        #Consider the number of channels\n",
    "        #background may have RGB while traffic sign has RGBA\n",
    "        if (channels==3):\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "        else:\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b,a = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "\n",
    "    return exposures     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_png(directory):\n",
    "    for files in load_paths(directory):\n",
    "        title, extension = files.split('.')\n",
    "        img = Image.open(files).convert('RGBA')\n",
    "        if (not extension == \"png\"):\n",
    "            os.remove(files)\n",
    "        img.save(title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "for bg_folders in load_paths(\"Backgrounds\"):\n",
    "    to_png(bg_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths, 4)\n",
    "    signs_exposures = find_image_exposure(signs_paths, 4)\n",
    "    \n",
    "    for i in range(0,len(background_paths)):\n",
    "        print(\"Processed: \" + str(float(i) / float(len(background_paths)) * 100) + \" %\")\n",
    "        \n",
    "        img = Image.open(background_exposures[i][0])\n",
    "\n",
    "        for sign_path in signs_paths:        \n",
    "            dirc, sub, el = background_exposures[i][0].split('/')\n",
    "            title, extension = el.split('.')\n",
    "\n",
    "            parent_dir, sub_dir, folder, folder2, element = sign_path.split('/')\n",
    "            head, tail = element.split('.')\n",
    "\n",
    "            \n",
    "            ###   ORIGINAL EXPOSURE IMPLEMENTATION   ###\n",
    "            brightness_avrg = 1.0\n",
    "            brightness_rms = 1.0\n",
    "            brightness_avrg_perceived = 1.0\n",
    "            brightness_rms_perceived = 1.0\n",
    "            brightness_avrg2 = 1.0\n",
    "            brightness_rms2 = 1.0\n",
    "\n",
    "            # abs(desired_brightness - actual_brightness) / abs(brightness_float_value) = ratio\n",
    "            avrg_ratio = 11.0159464507\n",
    "\n",
    "            rms_ratio = 8.30320014372\n",
    "\n",
    "            percieved_avrg_ratio = 3.85546373056\n",
    "\n",
    "            percieved_rms_ratio = 35.6344530649\n",
    "\n",
    "            avrg2_ratio = 1.20354549572\n",
    "\n",
    "            rms2_ratio = 40.1209106864\n",
    "\n",
    "            peak = Image.open(sign_path).convert('LA')\n",
    "            peak2 = Image.open(sign_path).convert('RGBA')\n",
    "\n",
    "            stat = ImageStat.Stat(peak)\n",
    "            avrg = stat.mean[0]\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            #IMAGE MANIPULATION MAIN CODE STARTS\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(avrg - float(background_exposures[i][1]))\n",
    "            \n",
    "            brightness_avrg = margin / avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright = enhancer.enhance(brightness_avrg)\n",
    "            stat = ImageStat.Stat(avrg_bright)\n",
    "            avrg = stat.mean[0]\n",
    "\n",
    "            \n",
    "            #MINIMISE MARGIN BASED ON ROOT MEAN SQUARE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(rms - float(background_exposures[i][2]))\n",
    "\n",
    "            brightness_rms = margin / rms_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright = enhancer.enhance(brightness_rms)\n",
    "            stat = ImageStat.Stat(rms_bright)\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            stat2 = ImageStat.Stat(peak2)\n",
    "            r, g, b, a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            margin = abs(avrg_perceived - float(background_exposures[i][3]))\n",
    "            \n",
    "            brightness_avrg_perceived = margin / percieved_avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright_perceived = enhancer.enhance(brightness_avrg_perceived)\n",
    "            stat2 = ImageStat.Stat(avrg_bright_perceived)\n",
    "            r, g ,b, a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241 * (r**2) + 0.691 * (g**2) + 0.068 * (b**2))        \n",
    "\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON RMS FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            r, g, b, a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241 * (r**2) + 0.691 * (g**2) + 0.068 * (b**2))\n",
    "\n",
    "            margin = abs(rms_perceived - float(background_exposures[i][4]))\n",
    "\n",
    "            brightness_rms_perceived = margin / percieved_rms_ratio \n",
    "\n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright_perceived = enhancer.enhance(brightness_rms_perceived)\n",
    "            stat2 = ImageStat.Stat(rms_bright_perceived)\n",
    "            r, g, b, a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241 * (r**2) + 0.691 * (g**2) + 0.068 * (b**2))        \n",
    "\n",
    "            \n",
    "            stat3 = ImageStat.Stat(peak2)\n",
    "            avrg2 = stat3.mean[0]\n",
    "            rms2 = stat3.rms[0]\n",
    "\n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO AVERAGING METHODS\n",
    "            margin = abs(avrg2-float(background_exposures[i][1]))\n",
    "            brightness_avrg2 = margin/avrg2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright2 = enhancer.enhance(brightness_avrg2)\n",
    "            stat3 = ImageStat.Stat(avrg_bright2)\n",
    "            avrg2 = stat3.mean[0]       \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO RMS METHODS\n",
    "            margin = abs(rms2-float(background_exposures[i][2]))\n",
    "            brightness_rms2 = margin/rms2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright2 = enhancer.enhance(brightness_rms2)\n",
    "            stat3 = ImageStat.Stat(rms_bright2)\n",
    "            rms2 = stat3.rms[0]\n",
    "            \"\"\"\n",
    "            \n",
    "            avrg_bright = avrg_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright = rms_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            avrg_bright_perceived = avrg_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright_perceived = rms_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            #avrg_bright2 = avrg_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "            #rms_bright2 = rms_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "\n",
    "            \n",
    "            exp_dir = \"Traffic_Signs_Exposure_Manipulation/\"\n",
    "            avrg_bright.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE.\"+tail)\n",
    "            rms_bright.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS.\"+tail)\n",
    "            avrg_bright_perceived.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE_PERCEIVED.\"+tail)\n",
    "            rms_bright_perceived.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS_PERCEIVED.\"+tail)\n",
    "            #avrg_bright2.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE2.\"+tail)\n",
    "            #rms_bright2.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS2.\"+tail)\n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths, 4)\n",
    "    signs_exposures = find_image_exposure(signs_paths, 4)\n",
    "    \n",
    "    print(\"Processed: 0.0 %\")\n",
    "    ii = 0\n",
    "    prev = 0\n",
    "    for sign_path in signs_paths:\n",
    "        progress = float(ii) / float(len(signs_paths)) * 100\n",
    "        if progress >= prev + 5: #Prevent spamming of progress prints\n",
    "            prev = prev + 5\n",
    "            print(\"Processed: \" + str(progress) + \" %\")\n",
    "\n",
    "        dirc, sub, el = background_exposures[0][0].split('/')\n",
    "        title, extension = el.split('.')\n",
    "\n",
    "        parent_dir, sub_dir, folder, folder2, element = sign_path.split('/')\n",
    "        head, tail = element.split('.')\n",
    "\n",
    "        img = cv2.imread(sign_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "        ###   GRADUAL FADE IMPLEMENTATION   ###\n",
    "        #Retrieve alpha data from original image\n",
    "        splitImg = cv2.split(img)\n",
    "        if len(splitImg) is 4:\n",
    "            alphaData = splitImg[3]\n",
    "\n",
    "        for jj in range(0,5): #Changed from range(0,6); I thought the last one was too bright\n",
    "            dmg6 = img.copy()\n",
    "            alpha = 1 - (jj * 0.19)\n",
    "            beta = (jj + 1) * 40\n",
    "            if jj > 0:\n",
    "                cv2.convertScaleAbs(img, dmg6, alpha, beta) #Scale the contrast and brightness\n",
    "                dmg6[:, :, 3] = alphaData\n",
    "\n",
    "            dmg6 = cv2.resize(dmg6, (150,150))\n",
    "            fad_dir = \"Traffic_Signs_Fade_Manipulation/\"\n",
    "            cv2.imwrite(fad_dir+\"SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_FADE-\"+str(jj)+\".\"+tail, dmg6)\n",
    "        ii = ii + 1\n",
    "    \n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bg_dir = \"Backgrounds\"\n",
    "\n",
    "for dirs in load_paths(bg_dir):\n",
    "    initial, subd = dirs.split('/')\n",
    "    \n",
    "    if original is True:\n",
    "        for background in load_paths(dirs):\n",
    "            initial, subd, element = background.split('/')\n",
    "            title, extension = element.split('.')\n",
    "\n",
    "            for signp in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "                for sign in load_paths(signp):\n",
    "                    d,s,f,e = sign.split('/') #Eg. s = 4_Transformed_Images, f = 9, e = 9_BOTTOM_HOLE\n",
    "\n",
    "                    exp_dir = \"Traffic_Signs_Exposure_Manipulation/\"\n",
    "                    if (not os.path.exists(exp_dir + subd + \"/\" + title + \"/SIGN_\" + f + \"/\" + e)):\n",
    "                        os.makedirs(exp_dir + subd + \"/\" + title + \"/SIGN_\" + f + \"/\" + e)\n",
    "    else:\n",
    "        for signp in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "            for sign in load_paths(signp):\n",
    "                d,s,f,e = sign.split('/')\n",
    "\n",
    "                fad_dir = \"Traffic_Signs_Fade_Manipulation/\"\n",
    "                if (not os.path.exists(fad_dir + \"SIGN_\" + f + \"/\" + e)):\n",
    "                    os.makedirs(fad_dir + \"SIGN_\" + f + \"/\" + e)\n",
    "\n",
    "signs_paths = []\n",
    "for p in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "    for d in load_paths(p):\n",
    "        signs_paths += load_paths(d)\n",
    "\n",
    "background_paths = [] #Load the b.g. paths from all b.g. folders into a single list\n",
    "for subfolder in load_paths(bg_dir):\n",
    "    background_paths += load_paths(subfolder)\n",
    "print(background_paths)\n",
    "\n",
    "if original is True:\n",
    "    exposure_manipulation(signs_paths, background_paths)\n",
    "else:\n",
    "    fade_manipulation(signs_paths, background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avrg_pixel_rgb(image,chanels):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    if (chanels == 4):\n",
    "        r,g,b,a = stat.rms\n",
    "    else:\n",
    "        r,g,b = stat.rms\n",
    "    \n",
    "    return [r,g,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bw_images(directory):\n",
    "    images = []\n",
    "    for signs in load_paths(directory):\n",
    "        img = Image.open(signs).convert('RGBA')\n",
    "        rgb = avrg_pixel_rgb(img, 4)\n",
    "        rg = abs(rgb[0] - rgb[1])\n",
    "        rb = abs(rgb[0] - rgb[2])\n",
    "        gb = abs(rgb[1] - rgb[2])\n",
    "        \n",
    "        temp = signs.split('/')\n",
    "        head,tail = temp[-1].split('.')\n",
    "                \n",
    "        if (rg <= 1 and rb <= 1 and gb <= 1):\n",
    "            images.append(head)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_useful_signs(directory): #Removes bad signs, such as those which are all white or all black\n",
    "    bw_images = find_bw_images(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "    for background_dir in load_paths(directory):\n",
    "        for signs in load_paths(background_dir):\n",
    "            for dmgs in load_paths(signs):\n",
    "                temp = []\n",
    "                for imgs in load_paths(dmgs):\n",
    "                    temp.append(imgs)\n",
    "                exposures = find_image_exposure(temp,4)\n",
    "                \n",
    "                i = 0\n",
    "                for images in load_paths(dmgs):\n",
    "                    #Find brightness\n",
    "                    img = Image.open(images).convert('RGBA')\n",
    "\n",
    "                    rgb = avrg_pixel_rgb(img,4)\n",
    "                    rg = abs(rgb[0]-rgb[1])\n",
    "                    rb = abs(rgb[0]-rgb[2])\n",
    "                    gb = abs(rgb[1]-rgb[2])\n",
    "\n",
    "                    is_bw = False\n",
    "\n",
    "                    for s in bw_images:\n",
    "                        if s in exposures[i][0]:\n",
    "                            is_bw = True\n",
    "\n",
    "                    if (rg<=16 and rb<=16 and gb<=16):\n",
    "                        if (not is_bw):\n",
    "                            os.remove(images)\n",
    "                        #Threshold values for black and white images\n",
    "                        elif (rgb[0]<70 and rgb[1]<70 and rgb[2]<70):\n",
    "                            os.remove(images)\n",
    "                        elif (rgb[0]>155 and rgb[1]>155 and rgb[2]>155):\n",
    "                            os.remove(images)\n",
    "\n",
    "                    elif (not is_bw):\n",
    "                        #Delete light blue images\n",
    "                        if(rgb[2]>rgb[0] and rgb[2]>=rgb[1]):\n",
    "                            if (gb<=10):\n",
    "                                os.remove(images)\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directory = \"Traffic_Signs_Exposure_Manipulation/GTSDB\"\n",
    "    find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_poisson_noise (image):\n",
    "    vals = len(np.unique(image))\n",
    "    vals = 2.05 ** np.ceil(np.log2(vals))\n",
    "    noisy = np.random.poisson(image * vals) / float(vals)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_Gaussian_noise (image):\n",
    "    row,col,ch= image.shape\n",
    "    mean = 0\n",
    "    var = 0.5\n",
    "    sigma = var**0.5\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_speckle_noise (image):\n",
    "    row,col,ch = image.shape\n",
    "    gauss = np.random.randn(row,col,ch)\n",
    "    gauss = gauss.reshape(row,col,ch)        \n",
    "    noisy = image + image * gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_method (image):\n",
    "    \"\"\"\n",
    "    i = random.randint(1, 3)\n",
    "    if (i == 1):\n",
    "        return insert_poisson_noise(image)\n",
    "    elif (i==2):\n",
    "        return insert_Gaussian_noise(image)\n",
    "    else:\n",
    "        return insert_speckle_noise(image)\n",
    "    \"\"\"\n",
    "    image.setflags(write=1)\n",
    "    #Add noise in every pixel w/ random probability 0.4\n",
    "    for im in image:\n",
    "        px = 0\n",
    "        for pixel in im:\n",
    "            apply_noise = random.randint(0,100)\n",
    "            #if random probability\n",
    "            if apply_noise > 40:\n",
    "                #RGB values\n",
    "                R = pixel[0]\n",
    "                G = pixel[1]\n",
    "                B = pixel[2]\n",
    "                A = pixel[3]\n",
    "                #find current relative lumination for brighness\n",
    "                #based on: https://en.wikipedia.org/wiki/Relative_luminance\n",
    "                relative_lumination = 0.2126*R + 0.7152*G + 0.0722*B\n",
    "                #find differences between RGB values     \n",
    "                R_to_G = float(R)/float(G)\n",
    "                RG = False\n",
    "                if (R_to_G >= 1): RG=True\n",
    "                R_to_B = float(R)/float(B)\n",
    "                RB = False\n",
    "                if (R_to_B >= 1): RB=True\n",
    "                G_to_B = float(G)/float(B)\n",
    "                GB = False\n",
    "                if (G_to_B >= 1): GB=True\n",
    "                equal = False\n",
    "                if (R==G==B):equal==True\n",
    "\n",
    "                #In order to determine the margin in which the new brighness\n",
    "                #should be within, the upper and lower limits need to be foun\n",
    "                #The Relative luminance in colorimetric spaces has normilised\n",
    "                #values between 0 and 255\n",
    "                upper_limit = 255\n",
    "                lower_limit = 0\n",
    "                if (relative_lumination + 40 < 255):\n",
    "                    upper_limit = relative_lumination + 40\n",
    "                if (relative_lumination - 40 > 0):\n",
    "                    lower_limit = relative_lumination - 40\n",
    "\n",
    "                #Compute new brighness value\n",
    "                new_lumination = random.randint(int(lower_limit),int(upper_limit))\n",
    "\n",
    "                #find the three possible solutions that satisfy\n",
    "                #->The new lumination chosen based on the Relative luminance equation\n",
    "                #->The precentages computed between every RGB value\n",
    "\n",
    "                solutions = []\n",
    "\n",
    "                for r in range(1,255):\n",
    "                    for g in range(1,255):\n",
    "                        for b in range(1,255):\n",
    "                            r_to_g = float(r)/float(g)\n",
    "                            rg = False\n",
    "                            if (r_to_g >= 1): rg=True\n",
    "                            r_to_b = float(r)/float(b)\n",
    "                            rb = False\n",
    "                            if (r_to_b >= 1): rb=True\n",
    "                            g_to_b = float(g)/float(b)\n",
    "                            gb = False\n",
    "                            if (g_to_b >= 1): gb=True\n",
    "                            e = False\n",
    "                            if(r==g==b):\n",
    "                                e=True\n",
    "                            if (0.2126*r + 0.7152*g + 0.0722*b == 100) and rg==RG and rb==RB and gb==GB and e==equal:\n",
    "                                solutions.append([r,g,b])\n",
    "\n",
    "                #Find the solution that precentage wise is closer to the original\n",
    "                #difference between the values\n",
    "                percentages = []\n",
    "\n",
    "                for solution in solutions:\n",
    "                    r = solution[0]\n",
    "                    g = solution[1]\n",
    "                    b = solution[2]\n",
    "                    percentages.append((float(r)/float(g))+(float(r)/float(b))+(float(g)/float(b)))\n",
    "\n",
    "                i = 0\n",
    "                pos = 0\n",
    "                best = percentages[0]\n",
    "                for p in percentages[1:]:\n",
    "                    if p < best:\n",
    "                        pos = i\n",
    "                    i = i +1\n",
    "\n",
    "                #Assign new pixel values\n",
    "                im[px] = [solutions[pos][0],solutions[pos][1],solutions[pos][2],A]\n",
    "            px = px+1\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used by bounding_axes(image_dir)\n",
    "# Checks if a line of pixels contains a pixel above a transparency threshold\n",
    "def has_opaque_pixel(line):\n",
    "    opaque = False\n",
    "    for pixel in line:\n",
    "        if pixel[3] > 200: # Check if pixel is opaque\n",
    "            opaque = True\n",
    "            break # Stop searching if one is found\n",
    "    return opaque\n",
    "\n",
    "# Returns the bounding axes of an image with a transparent background\n",
    "def bounding_axes(img):\n",
    "    # Top axis\n",
    "    y_top = 0\n",
    "    for row in img: # Iterate through each row of pixels, starting at top-left\n",
    "        if has_opaque_pixel(row) is False: # Check if the row has an opaque pixel\n",
    "            y_top += 1 #If not, move to the next row\n",
    "        else:\n",
    "            break # If so, break, leaving y_top as the bounding axis\n",
    "\n",
    "    # Bottom axis\n",
    "    height = img.shape[0]\n",
    "    y_bottom = height - 1\n",
    "    for row in reversed(img): # Iterate from the bottom row up\n",
    "        if has_opaque_pixel(row) is False:\n",
    "            y_bottom -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Left axis\n",
    "    r_img = imutils.rotate_bound(img, 90) # Rotate to iterate through what were originally columns\n",
    "    x_left = 0\n",
    "    for column in r_img:\n",
    "        if has_opaque_pixel(column) is False:\n",
    "            x_left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Right axis\n",
    "    r_height = r_img.shape[0]\n",
    "    x_right = r_height - 1\n",
    "    for column in reversed(r_img):\n",
    "        if has_opaque_pixel(column) is False:\n",
    "            x_right -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # FOR TESTING\n",
    "    # img[y_top, :] = (255, 0, 0, 255)\n",
    "    # img[y_bottom, :] = (255, 0, 0, 255)\n",
    "    # img[:, x_left] = (255, 0, 0, 255)\n",
    "    # img[:, x_right] = (255, 0, 0, 255)\n",
    "    # cv2.imwrite(image_dir, img)\n",
    "\n",
    "    return [x_left, x_right, y_top, y_bottom]\n",
    "\n",
    "# FOR TESTING\n",
    "# for img in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images/0/0_ORIGINAL\"):\n",
    "#     bounding_axes(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORIGINAL ### (Outdated - used for creating classification data)\n",
    "# def new_data(image_dir,bg_dir): #Blends synthetic signs with backgrounds\n",
    "#     # Import background image\n",
    "#     background_img_raw = Image.open(bg_dir).convert('RGBA')  \n",
    "#     background_img_raw = background_img_raw.resize((150,150), Image.ANTIALIAS)\n",
    "#     background_img = np.array(background_img_raw)  \n",
    "#     background_img_float = background_img.astype(float)  \n",
    "\n",
    "#     # Import foreground image\n",
    "#     foreground_img_raw = Image.open(image_dir)  \n",
    "#     foreground_img = np.array(foreground_img_raw)  \n",
    "#     foreground_img_float = foreground_img.astype(float)  \n",
    "\n",
    "#     # Blend images\n",
    "#     opacity = 1  \n",
    "#     blended_img_float = blend_modes.grain_merge(background_img_float, foreground_img_float, opacity)\n",
    "\n",
    "#     # Convert blended image back into PIL image\n",
    "#     blended_img = np.uint8(blended_img_float)\n",
    "#     blended_img_raw = Image.fromarray(blended_img)  \n",
    "    \n",
    "#     foreground_img_raw = foreground_img_raw.resize((149,149), Image.ANTIALIAS)\n",
    "#     blended_img_raw.paste(foreground_img_raw, (0, 0), foreground_img_raw)\n",
    "#     blended_img_raw = blended_img_raw.resize((48,48), Image.ANTIALIAS)\n",
    "    \n",
    "#     # temp = np.uint8(blended_img_raw)\n",
    "#     # temp = random_noise_method(temp)\n",
    "    \n",
    "#     # blended_img_raw = Image.fromarray(np.uint8(temp)) \n",
    "    \n",
    "#     return blended_img_raw\n",
    "\n",
    "### FULL BACKGROUND ###\n",
    "def new_data(image_dir, bg_dir, label_file, filename, values): # Blends synthetic signs with backgrounds\n",
    "    bg = cv2.imread(bg_dir, cv2.IMREAD_UNCHANGED)\n",
    "    fg = cv2.imread(image_dir, cv2.IMREAD_UNCHANGED)\n",
    "    bg_height, bg_width, _ = bg.shape\n",
    "    fg_height, fg_width, _ = fg.shape\n",
    "\n",
    "    # Rescaling the sign to correct its size relative to the background\n",
    "    # NOTE: Assumes background aspect ratio somewhat close to 16:9 (1.778)\n",
    "    current_ratio = fg_width / bg_width # Ratio of sign width to the background width\n",
    "    target_ratio = random.uniform(0.033, 0.066) # Aiming for between 3.3% and 6.6% of bg width\n",
    "    scale_factor = target_ratio / current_ratio\n",
    "    new_size = int(fg_width * scale_factor)\n",
    "    fg = cv2.resize(fg, (new_size, new_size))\n",
    "\n",
    "    # Randomise sign placement\n",
    "    x = random.randint(0, bg_width - fg_width)\n",
    "    third = bg_height // 3\n",
    "    y = random.randint(third, bg_height - third)\n",
    "\n",
    "    # Building label\n",
    "    axes = bounding_axes(fg) # Retrieve bounding axes of the sign image\n",
    "    axes[0] += x # Adjusting bounding axis to make it relative to the whole bg image\n",
    "    axes[1] += x\n",
    "    axes[2] += y\n",
    "    axes[3] += y\n",
    "    bounds = str(axes[0]) + \" \" + str(axes[1]) + \" \" + str(axes[2]) + \" \" + str(axes[3])\n",
    "\n",
    "    # It is assumed that the final .jpg -> .png conversion step is executed\n",
    "    label = filename + \".jpg \" + bounds + \" \" + values + \"\\n\"\n",
    "    label_file.write(label)\n",
    "    image = overlay(fg, bg, x, y)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the required directories in SGTSD\n",
    "if (not os.path.exists(\"SGTSD/Images\")):\n",
    "    #Numbered Version\n",
    "    for sign in load_paths(\"Traffic_Signs_Templates/1_Images\"): #Folders for each sign type\n",
    "        head, tail = sign.split('.')\n",
    "        name = head.split('/')\n",
    "        os.makedirs(\"SGTSD/Images/\" + name[-1])\n",
    "        j = 0\n",
    "        for dmg in range(len(damage_types)): #Folders for each damage type\n",
    "            os.makedirs(\"SGTSD/Images/\" + name[-1] + \"/\" + name[-1] + \"_\" + str(j))\n",
    "            j = j + 1\n",
    "    \n",
    "    #Named Version (Outdated?)\n",
    "#     for sign in load_paths(\"Traffic_Signs_Templates/1_Images\"): #Folders for each sign type\n",
    "#         head, tail = sign.split('.')\n",
    "#         name = head.split('/')\n",
    "#         os.makedirs(\"SGTSD/Images/\" + name[-1])\n",
    "#         for dmg in load_paths(\"Traffic_Signs_Templates/3_Damaged_Images\"): #Folders for each damage type\n",
    "#             headD,tailD = dmg.split('.')\n",
    "#             nameD = headD.split('/')\n",
    "#             os.makedirs(\"SGTSD/Images/\" + name[-1] + \"/\" + nameD[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a README file\n",
    "content = '''\n",
    "-----------------------------------------------\n",
    "|                     -*-                     |\n",
    "|Synthetically Generated Traffic Sign Dataset |\n",
    "|                     -*-                     |\n",
    "-----------------------------------------------\n",
    "\n",
    "This directory contains the training set for\n",
    "The Convolutional Neural Network (CNN)\n",
    "Used in this project\n",
    "\n",
    "However, it can be used for any classifier\n",
    "desired by the person using the code and\n",
    "additionally, it is not limited to a specific\n",
    "traffic sign templates.\n",
    " \n",
    "\n",
    "----------------------------------------------\n",
    "Content\n",
    "----------------------------------------------\n",
    "\n",
    "The number of example is based on the number:\n",
    "->of traffic signs that were used as templates\n",
    "->of the image manipulation processes\n",
    "->of the brighness variations values used\n",
    "->of the blending procedures\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Image format and naming\n",
    "----------------------------------------------\n",
    "The images created are of \"jpg\" format\n",
    "with RGBA channels\n",
    "\n",
    "   SIGN_X/XXX_YYY.jpg\n",
    "\n",
    "The initial part (X) is used to distinguish the\n",
    "sign class, while the remaining (XXX_YYY) firstly\n",
    "indicated the sign in the file itself and the\n",
    "example number.\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Additional information\n",
    "----------------------------------------------\n",
    "\n",
    "contact email: \n",
    "    \n",
    "\tasterga@essex.ac.uk\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Alexandros Stergiou\n",
    "\"The Driver's Assistant\"\n",
    "\n",
    "University of Essex,\n",
    "School of Computer Science and\n",
    "Electronic Engineering,\n",
    "UK\n",
    "----------------------------------------------\n",
    "'''\n",
    "text_file = open(\"SGTSD/Readme_Images.txt\", \"w\")\n",
    "text_file.write(content)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using exposure_manipulation\n",
    "def create_paths_list(imgs_directory, bg_directory):\n",
    "    directories = []\n",
    "    for places in load_paths(imgs_directory): #List of places: originally either UK_rural or UK_urban\n",
    "        for imgs in load_paths(places): #Folder for each bg image: eg. IMG_0\n",
    "            dr = imgs.split('/')\n",
    "            bg = bg_directory + '/' + dr[-2] + '/' + dr[-1] + \".png\" #Retrieving relevant bg image\n",
    "            for signs in load_paths(imgs): #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 0_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, bg])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using fade_manipulation; backgrounds are assigned to \n",
    "def create_assigned_paths_list(imgs_directory, bg_directory): #TODO: is this the same as above?\n",
    "    directories = []\n",
    "    for places in load_paths(bg_directory): #Folder for each place: eg. GTSDB\n",
    "        for imgs in load_paths(places): #Iterate through each b.g. image: eg. IMG_0\n",
    "            for signs in load_paths(imgs_directory):  #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 9_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, imgs])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directories = create_paths_list(\"Traffic_Signs_Exposure_Manipulation\",\"Backgrounds\")\n",
    "else:\n",
    "    directories = create_assigned_paths_list(\"Traffic_Signs_Fade_Manipulation\",\"Backgrounds\")\n",
    "print(\"Files to be generated: \" + str(len(directories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths of images needed to generate examples for 'sign' with damage 'dmg'\n",
    "def list_for_sign_x(sign, dmg, directories):\n",
    "    l = []\n",
    "    for elements in directories:\n",
    "        foreground = elements[0].split('/')\n",
    "        if (foreground[-2] == sign + dmg): #Eg. if (9_YELLOW == 4_ORIGINAL)\n",
    "            l.append(elements)\n",
    "    return l #Directory for every single sign and it's relevant background image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_directories = [] #Reformat list to have each sign and damage as their own dimensions\n",
    "signs = load_paths('Traffic_Signs_Templates/1_Images')\n",
    "for i in signs:\n",
    "    head, tail = ntpath.split(i)\n",
    "    sign, extension = tail.split('.') #Eg. sign == \"9\"\n",
    "\n",
    "    sign_list = [] #List of damages, which are each list of signs\n",
    "    for dmg in damage_types: #damage_types is from 'def damage_images:' cell\n",
    "        sign_list.append(list_for_sign_x(sign, dmg, directories))\n",
    "    final_directories.append(sign_list) #List of types -> lists of damages -> lists of signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate and write the new data to it's file\n",
    "direct = \"SGTSD/Images\"\n",
    "direct1 = \"SGTSD/Labels\"\n",
    "folders = load_paths(direct)\n",
    "n = [] #Numbers from the folder names for the signs\n",
    "for folder in folders:\n",
    "    head, tail = ntpath.split(folder)\n",
    "    n.append(tail)\n",
    "\n",
    "#Count how many signs there are; needed for the progress bar\n",
    "total = 0\n",
    "for signs in final_directories:\n",
    "    for damages in signs:\n",
    "        for dirs in damages:\n",
    "            total += 1\n",
    "\n",
    "count = 0\n",
    "i = 0\n",
    "for signs in final_directories: #Iterating through sign types\n",
    "    j = 0\n",
    "    for damages in signs: #Iterating through damage types\n",
    "        #FIXME: For some reason secondary progress counter has broken from initial implementation\n",
    "        print(\"Processed: \" + str(float(count) / float(total) * 100) + \" %\")\n",
    "        k = 0\n",
    "\n",
    "        label_filename = direct1 + \"/\" + n[i] + \"/\" + n[i] + \"_\" + str(j) + \".txt\"\n",
    "        text_file = open(label_filename, \"r\")\n",
    "        values = text_file.read() #Retrieve the damage values created earlier in the damage_images function\n",
    "        text_file = open(label_filename, \"w\")\n",
    "\n",
    "        for dirs in damages: #dirs == the foreground and background images for one generated sign\n",
    "            filename = direct+\"/\"+n[i]+\"/\"+n[i]+\"_\"+str(j)+\"/\"+n[i]+\"_\"+str(j)+\"_\"+str(k)\n",
    "            image = new_data(dirs[0], dirs[1], text_file, filename, values) #Combining b.g. with sign f.g.\n",
    "            cv2.imwrite(filename + \".png\", image)\n",
    "\n",
    "            count += 1\n",
    "            k += 1\n",
    "        j += 1\n",
    "        text_file.close()\n",
    "    i += 1\n",
    "print(\"Processed: \" + str(100) + \" %\") #FIXME: changing damage names broke something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "-------------------------------------\n",
    "BREAKDOWN OF FILES GENERATED BY CLASS\n",
    "-------------------------------------\n",
    "'''\n",
    "total = 0\n",
    "for i in range(len(final_directories)):\n",
    "    current = 0\n",
    "    for j in range(len(final_directories[i])):\n",
    "        current = current + len(final_directories[i][j])\n",
    "    s = \"Generated \" + str(current) + \" examples for sign class \" + str(i + 1)\n",
    "    string = string + '\\n' + s + '\\n'\n",
    "    total = total + current\n",
    "string = string + '\\n' + \"TOTAL: \" + str(total) + '\\n' + \"Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M\") + '\\n'\n",
    "string = string + \"-------------------------------------\"\n",
    "text_file = open(\"SGTSD/generated_images_about.txt\", \"w\")\n",
    "text_file.write(string)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_to_jpeg(filepath):\n",
    "    dirs = filepath.split('/')\n",
    "    title,extension = dirs[-1].split('.')\n",
    "    del dirs[-1]\n",
    "    string = '/'.join(dirs)\n",
    "    string = string + '/' + title + \".jpg\"\n",
    "    png = Image.open(filepath)\n",
    "    png.load() # required for png.split()\n",
    "    background = Image.new(\"RGB\", png.size, (255, 255, 255))\n",
    "    background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
    "    background.save(string, 'JPEG', quality=100)\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirs = direct = \"SGTSD/Images\"\n",
    "i = 1\n",
    "for path in load_paths(dirs):\n",
    "    print(\"Processed: \" + str(float(i - 1) / float(len(final_directories)) * 100) + \" %\")\n",
    "    for damage in load_paths(path):\n",
    "        for image in load_paths(damage):\n",
    "            if (image.endswith(\"png\")):\n",
    "                png_to_jpeg(image)\n",
    "    i += 1\n",
    "print(\"Processed: \" + str(100) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Exposure_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Fade_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Templates/4_Transformed_Images\")\n",
    "shutil.rmtree(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "shutil.rmtree(\"Traffic_Signs_Templates/2_Processed_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"SGTSD\") #Be careful with this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}