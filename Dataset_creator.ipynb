{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageChops, ImageDraw, ImageOps, ImageFilter, ImageStat, ImageEnhance \n",
    "from skimage import io, color, exposure\n",
    "import imutils\n",
    "import argparse\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import random\n",
    "import blend_modes\n",
    "\n",
    "original = True # Whether we are using the original exposure_manipulation code or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list with the paths of all files in the directory\n",
    "def load_paths(directory):\n",
    "    paths = []\n",
    "    for filename in os.listdir(directory): # Retrieve names of files in directory\n",
    "        # Concatenate filename with directory path, ignoring hidden files\n",
    "        path = os.path.join(directory, filename)\n",
    "        if not filename.startswith('.'):\n",
    "            paths.append(path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list with the paths of all non-directory files in the directory\n",
    "def load_files(directory):\n",
    "    paths = []\n",
    "    for filename in os.listdir(directory): # Retrieve names of files in directory\n",
    "        # Concatenate filename with directory path, ignoring hidden files and directories\n",
    "        path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(path) and not filename.startswith('.'):\n",
    "            paths.append(path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to imitate the functionality of path.split('/') while using os.path.split\n",
    "# https://stackoverflow.com/a/3167684/12350950\n",
    "def dir_split(path):\n",
    "    folders = []\n",
    "    while True:\n",
    "        path, folder = os.path.split(path)\n",
    "\n",
    "        if folder != \"\":\n",
    "            folders.append(folder)\n",
    "        else:\n",
    "            if path != \"\":\n",
    "                folders.append(path)\n",
    "            break\n",
    "\n",
    "    folders.reverse()\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rescale and pad the source image with whitespace to be a perfect square of fixed width\n",
    "def scale_image(image_path):\n",
    "    width = 450\n",
    "\n",
    "    # https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
    "    # Resizing the image\n",
    "    img = Image.open(image_path)\n",
    "    old_size = img.size  # old_size is in (width, height) format\n",
    "    ratio = float(width) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    img = img.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    # Padding the image with whitespace\n",
    "    delta_w = width - new_size[0]\n",
    "    delta_h = width - new_size[1]\n",
    "    padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "    new_img = ImageOps.expand(img, padding, (255,255,255))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Alexandros Stergiou's \"find_borders()\"\n",
    "# Returns an alpha channel that matches the white background\n",
    "def create_alpha(img, alpha_channel):\n",
    "    # Read and decode the image contents for pixel access\n",
    "    pix = img.load()\n",
    "\n",
    "    # Note PIL indexes [x,y] while OpenCV indexes [y,x]\n",
    "    # alpha_channel must be indexed [y,x] to merge with OpenCV channels later\n",
    "\n",
    "    min = 200\n",
    "    width, height = img.size\n",
    "    # Loop through each row of the image\n",
    "    for y in range(0, height):\n",
    "        # First loop left to right\n",
    "        for x in range(0, width, 1):\n",
    "            # Retrieve a tuple with RGB values for this pixel\n",
    "            rgb = pix[x,y]\n",
    "            # Make transparent if the pixel is white (or light enough)\n",
    "            if rgb[0] >= min and rgb[1] >= min and rgb[2] >= min:\n",
    "                alpha_channel[y,x] = 0\n",
    "            # If pixel is not white then we've hit the sign so break out of loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Then loop backwards, right to left\n",
    "        for x in range(width-1, -1, -1):\n",
    "            rgb = pix[x,y]\n",
    "            if rgb[0] >= min and rgb[1] >= min and rgb[2] >= min:\n",
    "                alpha_channel[y,x] = 0\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return alpha_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Alexandros Stergiou's \"manipulate_images()\"\n",
    "# Delete the white background from the original sign\n",
    "def delete_background(image_path, save_path):\n",
    "    # Open the image using PIL (don't read contents yet)\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')  # Does this have any effect??\n",
    "\n",
    "    # Open the image again using OpenCV and split into its channels\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    channels = cv2.split(image)\n",
    "\n",
    "    # Create a fully opaque alpha channel, same dimentions and dtype as the image\n",
    "    # create_alpha() modifies it to make the white background transparent\n",
    "    alpha_channel = np.ones(channels[0].shape, dtype=channels[0].dtype) * 255\n",
    "    alpha_channel = create_alpha(img, alpha_channel)\n",
    "\n",
    "    # Merge alpha channel into original image\n",
    "    image_RGBA = cv2.merge((channels[0], channels[1], channels[2], alpha_channel))\n",
    "\n",
    "    cv2.imwrite(save_path, image_RGBA)\n",
    "\n",
    "    img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make white backgrounds transparent\n",
    "input_dir = os.path.join(\"Traffic_Signs_Templates\", \"1_Images\")\n",
    "output_dir = os.path.join(\"Traffic_Signs_Templates\", \"2_Processed_Images\")\n",
    "# Create the output directory if it doesn't exist already\n",
    "if (not os.path.exists(output_dir)):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "paths = load_files(input_dir)\n",
    "for path in paths:\n",
    "    img = scale_image(path) # Rescale the image\n",
    "\n",
    "    # Remove the extension and save as a png\n",
    "    _, filename = ntpath.split(path)\n",
    "    name, _ = filename.rsplit('.', 1)\n",
    "    save_path = os.path.join(output_dir, name) + \".png\"\n",
    "    img.save(save_path)\n",
    "\n",
    "    delete_background(save_path, save_path) # Overwrite the newly rescaled image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the first image if it is larger than the second image\n",
    "# No longer needed?\n",
    "'''\n",
    "def resize(img1, img2):\n",
    "    # Retrieve dimentions of both images\n",
    "    height1, width1, _ = img1.shape  # Discard channel\n",
    "    height2, width2, _ = img2.shape\n",
    "    \n",
    "    # Resize if foreground is taller than background\n",
    "    if height1 > height2:\n",
    "        img1 = match_height(img1, height2)\n",
    "        height1, width1, _ = img1.shape  # Re-retrieve dimentions\n",
    "    # Or wider\n",
    "    if width1 > width2:\n",
    "        img1 = match_width(img1, width2)\n",
    "    \n",
    "    return img1, img2\n",
    "\n",
    "\n",
    "def match_height(img, new_height):\n",
    "    old_height, old_width, _ = img.shape  # Discard channel\n",
    "    new_width = int(round( new_height / old_height * old_width ))\n",
    "    img = cv2.resize(img, (new_height, new_width))\n",
    "    return img\n",
    "\n",
    "\n",
    "def match_width(img, new_width):\n",
    "    old_width, old_height, _ = img.shape\n",
    "    new_height = int(round( new_width / old_width * old_height ))\n",
    "    img = cv2.resize(img, (new_width, new_height))\n",
    "    return img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlays foreground image on background image, keeping transparency\n",
    "# x1, y1: top-left coordinate to place foreground\n",
    "# Image overlay code credit to fireant:\n",
    "# https://stackoverflow.com/questions/14063070/overlay-a-smaller-image-on-a-larger-image-python-opencv\n",
    "def overlay(fg, bg, x1=-1, y1=-1):\n",
    "    # If the background doesn't have an alpha channel, add one, but keep the entire image opaque\n",
    "    if len(cv2.split(bg)) == 3:\n",
    "        bg = cv2.cvtColor(bg, cv2.COLOR_RGB2RGBA)\n",
    "        bg[:, :, 3] = 255\n",
    "    # Make a copy of the background for making changes\n",
    "    new_img = bg.copy()\n",
    "\n",
    "    # Retrieve image dimentions\n",
    "    height_FG, width_FG, _ = fg.shape  # Discard channel\n",
    "    height_BG, width_BG, _ = bg.shape\n",
    "\n",
    "    # If either of the coordinates were omitted, calculate start/end positions\n",
    "    # using the difference in image size, centring foreground on background\n",
    "    if x1 == -1 or y1 == -1:\n",
    "        # Start coordinates \n",
    "        x1 = (width_BG - width_FG) // 2    # Floor division to truncate as\n",
    "        y1 = (height_BG - height_FG) // 2  # coordinates don't need to be exact\n",
    "    # End coordinates\n",
    "    x2 = x1 + width_FG\n",
    "    y2 = y1 + height_FG\n",
    "\n",
    "    ### Start of code from fireant ###\n",
    "    # Retrieve an array of alpha values from the foreground image\n",
    "    # Divide by 255 to get values between 0.0 and 1.0\n",
    "    alpha = fg[:,:,3] / 255.0\n",
    "    beta = 1.0 - alpha\n",
    "\n",
    "    # Loop over BGR channels (but not alpha)\n",
    "    for ch in range(0, 3):\n",
    "        new_img[y1:y2, x1:x2, ch] = (alpha * fg[:, :, ch] +\n",
    "                                     beta * new_img[y1:y2, x1:x2, ch])\n",
    "    ### End of code from fireant ###\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of obscurity of graffiti\n",
    "# Calculates the ratio of opacity in first image compared to second image\n",
    "def calc_ratio(fg, bg):\n",
    "    fg_pixels = count_pixels(fg)  # Number of non-transparent pixels in graffiti\n",
    "    bg_pixels = count_pixels(bg)  # Total number of pixels, dependent on sign shape\n",
    "    ratio = fg_pixels / bg_pixels\n",
    "\n",
    "    return ratio\n",
    "\n",
    "\n",
    "# Return the number of non-transparent pixels in the imported image\n",
    "def count_pixels(img):\n",
    "    sum = 0  # Initialise to 0 in case there is no alpha channel\n",
    "    split = cv2.split(img)\n",
    "    if len(split) is 4:  # Only proceed if the image has an alpha channel\n",
    "        alpha = split[3]\n",
    "        # Loop through alpha channel\n",
    "        for ii in range(0, len(alpha)):\n",
    "            for jj in range(0, len(alpha[0])):\n",
    "                sum += alpha[ii][jj] / 255  # Divide by 255 to weight the transparency\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the ratio of changed opaque pixels between two versions of the same image for each quadrant\n",
    "def calc_quadrant_diff(new, original):\n",
    "    height, width, _ = original.shape\n",
    "    centre_x = int(round( width / 2 ))\n",
    "    centre_y = int(round( height / 2 ))\n",
    "\n",
    "    # Divide both images into quadrants\n",
    "    new_I   = new[0:centre_y, centre_x:width]\n",
    "    new_II  = new[0:centre_y, 0:centre_x]\n",
    "    new_III = new[centre_y:height, 0:centre_x]\n",
    "    new_IV  = new[centre_y:height, centre_x:width]\n",
    "    original_I   = original[0:centre_y, centre_x:width]\n",
    "    original_II  = original[0:centre_y, 0:centre_x]\n",
    "    original_III = original[centre_y:height, 0:centre_x]\n",
    "    original_IV  = original[centre_y:height, centre_x:width]\n",
    "\n",
    "    ratio_I   = count_diff_pixels(new_I, original_I) / count_pixels(original_I)\n",
    "    ratio_II  = count_diff_pixels(new_II, original_II) / count_pixels(original_II)\n",
    "    ratio_III = count_diff_pixels(new_III, original_III) / count_pixels(original_III)\n",
    "    ratio_IV  = count_diff_pixels(new_IV, original_IV) / count_pixels(original_IV)\n",
    "    return [ratio_I, ratio_II, ratio_III, ratio_IV]\n",
    "\n",
    "# Modified version of count_pixels() which counts how many opaque pixels are different between two images\n",
    "def count_diff_pixels(new, original):\n",
    "    count = 0\n",
    "    split = cv2.split(original)\n",
    "    if len(split) is 4:\n",
    "        alpha = split[3]\n",
    "        for ii in range(0, len(new)):\n",
    "            for jj in range(0, len(new[0])):\n",
    "                if alpha[ii][jj] > 0:  # The pixel in the original image must have been opaque\n",
    "                    if np.any(new[ii][jj] != original[ii][jj]):  # Check for any changes in the pixel\n",
    "                        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obscure a sign with scribbled graffiti\n",
    "# Returns the image and the ratio of oscurity\n",
    "def graffiti_scribble(img):\n",
    "    # Create a new image the same dimentions as the image but completely transparent\n",
    "    height,width,_ = img.shape\n",
    "    blank = np.zeros((height, width, 4), dtype=np.uint8)\n",
    "    # Retrieve the original alpha data to reapply after overlaying\n",
    "    alpha_ch = cv2.split(img)[3]\n",
    "\n",
    "    dmg = {}  # Dictionary to map the images to their obscurity percentages\n",
    "    scribbles = []  # Temporary list to add the graffiti to, for combining them\n",
    "\n",
    "    # Turning point coordinates, for a 1x1 image\n",
    "    pts1 = ( (0.28,0.12), (0.10,0.44), (0.46,0.12), (0.14,0.68), (0.70,0.16),\n",
    "             (0.30,0.84), (0.86,0.32), (0.54,0.88), (0.90,0.56), (0.72,0.88) )\n",
    "    # Create a horizontal reflection of the points\n",
    "    pts2 = ()\n",
    "    for ii in range(len(pts1)):\n",
    "        pts2 = pts2 + ( (1.0 - pts1[ii][0], pts1[ii][1]), )\n",
    "\n",
    "    # Put the colours and direction of the scribble in a tuple\n",
    "    settings = ( (pts1, (0,0,0)), (pts2, (230,170,0)), (pts1, (140,70,30)), (pts2, (240,240,240)) )\n",
    "    # Kernel size for Gaussian blur, which must be odd\n",
    "    k = (int(round( width/15 )) // 2) * 2 + 1\n",
    "    \n",
    "    for setting in settings:\n",
    "        grft = blank.copy()\n",
    "        grft = draw_lines(grft, pts=setting[0], color_bgr=setting[1])\n",
    "        # Apply a Gaussian blur to smooth the edges\n",
    "        grft = cv2.GaussianBlur(grft, (k,k), 0)\n",
    "        scribbles.append(grft.copy())  # Add to the temp list for combining the scribbles\n",
    "        # Remove any graffiti that spilt over the edge\n",
    "        grft[:,:,3] = cv2.bitwise_and(grft[:,:,3], alpha_ch)\n",
    "\n",
    "        damaged_sign = overlay(grft, img)\n",
    "        # Use the two separate images to calculate the ratio of obscurity\n",
    "        ratio = \"{:0.3f}\".format( calc_ratio(grft, img) )\n",
    "        dmg[ratio] = damaged_sign  # Add to dictionary\n",
    "    \n",
    "    # Remove the last item if there are an odd number of scribbles\n",
    "    if len(scribbles) % 2 != 0:\n",
    "        scribbles.pop()\n",
    "    # Overlay 2 scribbles to create a more obscured sign\n",
    "    it = iter(scribbles)  # Use an interator to iterate over 2 items at a time\n",
    "    for grft1 in it:\n",
    "        grft2 = next(it)\n",
    "        # Modify grft2's alpha channel to the union of both scribbles to allow combining\n",
    "        grft2[:,:,3] = cv2.bitwise_or( cv2.split(grft1)[3], cv2.split(grft2)[3] )\n",
    "        grft = overlay(grft1, grft2)\n",
    "        # Remove any graffiti that spilt over the edge\n",
    "        grft[:,:,3] = cv2.bitwise_and(grft[:,:,3], alpha_ch)\n",
    "\n",
    "        damaged_sign = overlay(grft, img)  # Overlay with the sign\n",
    "        ratio = \"{:0.3f}\".format( calc_ratio(grft, img) )\n",
    "        dmg[ratio] = damaged_sign  # Add to dictionary\n",
    "\n",
    "    return dmg\n",
    "\n",
    "\n",
    "def draw_lines(img, pts, color_bgr):\n",
    "    height,width,_ = img.shape\n",
    "    # Randomly choose a thickness so that the scribbles won't all have the same obscurity\n",
    "    thickness = width / random.randint(10,30)\n",
    "    for ii in range(len(pts)-1):\n",
    "        # Generate some randomness in line thickness and colour transparency\n",
    "        this_thickness = int(round( random.uniform(0.8, 1.2) * thickness ))\n",
    "        alpha = random.randint(240, 255)\n",
    "        color = color_bgr + (alpha,)\n",
    "        # Multiply by width or height (and round) to get coordinates for drawing the lines\n",
    "        pt1 = ( int(round(width * pts[ ii ][0])), int(round(height * pts[ ii ][1])) )\n",
    "        pt2 = ( int(round(width * pts[ii+1][0])), int(round(height * pts[ii+1][1])) )\n",
    "        cv2.line(img, pt1, pt2, color, this_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw some text to represent graffiti\n",
    "# Returns the image and the ratio of oscurity\n",
    "def graffiti_writing(img):\n",
    "    # Create a new image the same dimentions as the image but completely transparent\n",
    "    height,width,_ = img.shape\n",
    "    grft = np.zeros((height, width, 4), dtype=np.uint8)\n",
    "    # Retrieve the original alpha data to reapply later\n",
    "    alpha_ch = cv2.split(img)[3]\n",
    "\n",
    "    # Draw some graffiti onto the transparent image\n",
    "    xx,yy = int(width * 0.1), int(height * 0.5)\n",
    "    grft = write(grft, \"GRFT\", xx, yy, scale=3, color_bgr=(25,25,25))\n",
    "    xx,yy = int(width * 0.3), int(height * 0.8)\n",
    "    grft = write(grft, \"DAMAGE\", xx, yy, scale=2, color_bgr=(230,170,0))\n",
    "    \n",
    "    # Apply a Gaussian blur to smooth the edges\n",
    "    k = (int(round( width/40 )) // 2) * 2 + 1  # Kernel size must be odd\n",
    "    grft = cv2.GaussianBlur(grft, (k,k), 0)\n",
    "    # Remove any graffiti that spilt over the edge\n",
    "    grft[:,:,3] = cv2.bitwise_and(grft[:,:,3], alpha_ch)\n",
    "    \n",
    "    # Overlay the graffiti onto the sign\n",
    "    img = overlay(grft, img)\n",
    "    # Use the two separate images to calculate the ratio of obscurity\n",
    "    ratio = \"{:0.3f}\".format( calc_ratio(grft, img) )\n",
    "    \n",
    "    return img, ratio\n",
    "\n",
    "\n",
    "def write(img, text, xx, yy, scale, color_bgr):\n",
    "    fontFace = cv2.FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    "    _,width,_ = img.shape\n",
    "    fontScale = width / 400 * scale\n",
    "    thickness = int(round( width / 120 * scale ))\n",
    "\n",
    "    (wd,ht),_ = cv2.getTextSize(text, fontFace, fontScale, thickness)\n",
    "    # Put one letter at a time, overlapping the letters slightly\n",
    "    for letter in text:\n",
    "        # Use a random number to generate some range in letter size and transparency\n",
    "        size = random.uniform(0.7, 1.3)\n",
    "        alpha = random.randint(240, 255)\n",
    "        color = color_bgr + (alpha,)  # Create a new tuple\n",
    "        \n",
    "        origin = ( xx, yy - int(round( ht*(1-size)/2 )) )  # Centre vertical position\n",
    "        cv2.putText(img, letter, origin, fontFace, fontScale*size, color, thickness, cv2.LINE_AA)\n",
    "        # Calculate the horizontal starting position for the next letter, based on this character's size\n",
    "        offset = int(round( wd / len(text) * 0.6 * size ))  # 60% character width\n",
    "        xx = xx + offset\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply perspective warp to tilt images and combine to produce bent signs\n",
    "# Returns a dictionary with the bent signs mapped to the filenames to save as\n",
    "def bend_vertical(img_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    ht,wd,_ = img.shape  # Retrieve image dimentions\n",
    "\n",
    "    dmg = {}  # Dictionary to map the images to filenames\n",
    "\n",
    "    # TILT 1\n",
    "    pt = wd // 24\n",
    "    xx = pt * 3\n",
    "    yy = pt // 2\n",
    "    # Keep top-middle and bottom-middle unchanged to bend on the vertical axis\n",
    "    # Right             Top-left Top-middle  Bottom-left Bottom-middle\n",
    "    src = np.float32( [ [pt,pt], [wd//2,pt], [pt,ht-pt], [wd//2,ht-pt] ] )\n",
    "    dst = np.float32( [ [xx,yy], [wd//2,pt], [xx,ht-yy], [wd//2,ht-pt] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    right = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    # Left              Top-middle  Top-right   Bottom-middle  Bottom-right\n",
    "    src = np.float32( [ [wd//2,pt], [wd-pt,pt], [wd//2,ht-pt], [wd-pt,ht-pt] ] )\n",
    "    dst = np.float32( [ [wd//2,pt], [wd-xx,yy], [wd//2,ht-pt], [wd-xx,ht-yy] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    left = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    \n",
    "    # Combine the left and right tilt with the original forward-facing image\n",
    "    dmg[\"0_40\"] = combine(img, right)\n",
    "    dmg[\"40_0\"]  = combine(left, img)\n",
    "    # Combine the left and right tilt\n",
    "    dmg[\"40_40\"] = combine(left, right)\n",
    "\n",
    "    # TILT 2\n",
    "    pt = wd // 12\n",
    "    xx = pt * 3\n",
    "    yy = pt // 2\n",
    "    # Right             Top-left Top-middle  Bottom-left Bottom-middle\n",
    "    src = np.float32( [ [pt,pt], [wd//2,pt], [pt,ht-pt], [wd//2,ht-pt] ] )\n",
    "    dst = np.float32( [ [xx,yy], [wd//2,pt], [xx,ht-yy], [wd//2,ht-pt] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    right = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    # Left              Top-middle  Top-right   Bottom-middle  Bottom-right\n",
    "    src = np.float32( [ [wd//2,pt], [wd-pt,pt], [wd//2,ht-pt], [wd-pt,ht-pt] ] )\n",
    "    dst = np.float32( [ [wd//2,pt], [wd-xx,yy], [wd//2,ht-pt], [wd-xx,ht-yy] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    left = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    \n",
    "    # Combine the left and right tilt with the original forward-facing image\n",
    "    dmg[\"0_60\"] = combine(img, right)\n",
    "    dmg[\"60_0\"]  = combine(left, img)\n",
    "    # Combine the left and right tilt\n",
    "    dmg[\"60_60\"] = combine(left, right)\n",
    "\n",
    "    return dmg\n",
    "\n",
    "\n",
    "# Combines the left half of img1 with right half of img2 and returns the result\n",
    "def combine(img1, img2):\n",
    "    _,wd,_ = img1.shape\n",
    "    result = img1.copy()\n",
    "\n",
    "    # Save the alpha data (to replace later), as convertScaleAbs() will affect the transparency\n",
    "    alpha_ch = cv2.split(result)[3]\n",
    "    \n",
    "    # Darken the entire image of the copy\n",
    "    alpha = 1   # No change to contrast\n",
    "    beta = -20  # Decrease brightness\n",
    "    cv2.convertScaleAbs(result, result, alpha, beta)\n",
    "    # Replace the alpha data\n",
    "    result[:,:,3] = alpha_ch\n",
    "    \n",
    "    # Copy over the right half of img2 onto the darkened image\n",
    "    result[:,wd//2:wd] = img2[:,wd//2:wd]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for SGTSD\n",
    "damage_types = [\"ORIGINAL\", \"QUADRANT\", \"BIG_HOLE\", \"HOLES\", \"YELLOW\",\n",
    "                \"GRAFFITI\", \"BEND\", \"GREY\"]\n",
    "\n",
    "def damage_image(image_path, output_dir):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    img = img.astype('uint8')\n",
    "    height, width, ch = img.shape\n",
    "    centre_x = int(round( width / 2 ))\n",
    "    centre_y = int(round( height / 2 ))\n",
    "    \n",
    "    # Create file writing info: filename, class number, output directory, and labels directory\n",
    "    labels_dir = \"SGTSD/Labels\"\n",
    "    _, filename = ntpath.split(image_path) # Remove parent directories to retrieve the image filename\n",
    "    c_num, _ = filename.rsplit('.', 1)     # Remove extension to get the sign/class number\n",
    "\n",
    "    output_path = os.path.join(output_dir, c_num)\n",
    "    # Create the output directory if it doesn't exist already\n",
    "    if (not os.path.exists(output_path)):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Create the labels directory for this sign if it doesn't exist already\n",
    "    if (not os.path.exists(os.path.join(labels_dir, c_num))):\n",
    "        os.makedirs(os.path.join(labels_dir, c_num))\n",
    "    text_base = os.path.join(labels_dir, c_num, c_num + \"_\") # The base filename for the label files\n",
    "    ii = 0 # Class number that's appended to end of base filename\n",
    "    \n",
    "    # Create a mask\n",
    "    sign = img.copy()\n",
    "    alpha_ch = sign[:, :, 3]  # Extract the alpha channel from sign\n",
    "    _, mask = cv2.threshold(alpha_ch, 5, 255, cv2.THRESH_BINARY)  # Remove gradual transparency\n",
    "    \n",
    "    # ORIGINAL UNDAMAGED\n",
    "    dmg = img\n",
    "    \n",
    "    dmg_values = calc_quadrant_diff(dmg, img)\n",
    "    values = [float_to_string(round(num, 6)) for num in dmg_values] # Convert the values into decimal strings\n",
    "    text_file = open(text_base + str(ii) + \".txt\", \"w\")\n",
    "    text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]}\")\n",
    "    ii += 1\n",
    "    cv2.imwrite(os.path.join(output_path, c_num + \".png\"), dmg)\n",
    "    \n",
    "    \n",
    "    # QUADRANT\n",
    "    quadrant = mask.copy()\n",
    "    quad_num = random.randint(1, 4)  # For selecting which quadrant it will be\n",
    "    # Removing the quadrant: -1 offset is necessary to avoid damaging part of a wrong quadrant\n",
    "    if quad_num is 1:\n",
    "        cv2.rectangle(quadrant, (width, 0), (centre_x, centre_y-1), (0,0,0), -1)\n",
    "    if quad_num is 2:\n",
    "        cv2.rectangle(quadrant, (0, 0), (centre_x-1, centre_y-1), (0,0,0), -1)\n",
    "    if quad_num is 3:\n",
    "        cv2.rectangle(quadrant, (0, height), (centre_x-1, centre_y), (0,0,0), -1)\n",
    "    if quad_num is 4:\n",
    "        cv2.rectangle(quadrant, (width, height), (centre_x, centre_y), (0,0,0), -1)\n",
    "    dmg1 = cv2.bitwise_and(img, img, mask=quadrant)\n",
    "    \n",
    "    dmg_values = calc_quadrant_diff(dmg1, img)\n",
    "    values = [float_to_string(round(num, 6)) for num in dmg_values]  # Convert the values into decimal strings\n",
    "    text_file = open(text_base + str(ii) + \".txt\", \"w\")\n",
    "    text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]}\")\n",
    "    ii += 1\n",
    "    cv2.imwrite(os.path.join(output_path, c_num + damage_types[1] + \".png\"), dmg1)\n",
    "    \n",
    "    \n",
    "    # BIG HOLE\n",
    "    big_hole = mask.copy()\n",
    "    angle = random.randint(0, 359) # Angle from the x axis counter-clockwise through quadrant I\n",
    "    r = int(2 * height / 5) # Radius\n",
    "    rad = -(angle * math.pi / 180)\n",
    "    x = centre_x + int(r * math.cos(rad)) # x coordinate of centre\n",
    "    y = centre_y + int(r * math.sin(rad)) # y coordinate of centre\n",
    "    cv2.circle(big_hole, (x,y), r, (0,0,0), -1)\n",
    "    dmg2 = cv2.bitwise_and(img, img, mask=big_hole)\n",
    "    \n",
    "    dmg_values = calc_quadrant_diff(dmg2, img)\n",
    "    values = [float_to_string(round(num, 6)) for num in dmg_values] # Convert the values into decimal strings\n",
    "    text_file = open(text_base + str(ii) + \".txt\", \"w\")\n",
    "    text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]} angle={angle}\")\n",
    "    ii += 1\n",
    "    cv2.imwrite(os.path.join(output_path, c_num + damage_types[2] + \".png\"), dmg2)\n",
    "    \n",
    "\n",
    "    # RANDOMISED BULLET HOLES\n",
    "    bullet_holes = mask.copy()\n",
    "    painted = img.copy() # Another copy of original sign to draw 'flaking paint' from holes on\n",
    "    numHoles = random.randint(7, 30) # Random number of holes\n",
    "    for x in range(numHoles):\n",
    "        size = random.randint(2, 12) # Random hole size\n",
    "        h_x = random.randint(0, height) # Random hole position\n",
    "        h_y = random.randint(0, height)\n",
    "        c = random.randint(0, 150) # How black/grey the 'hole' is if it didn't penetrate\n",
    "        s = random.uniform(1.6, 2.2) # Random annulus size\n",
    "    \n",
    "        C = 200 # Colour of damaged 'paint' outer annulus\n",
    "        cv2.circle(painted, (h_x, h_y), int(size * s), (C,C,C,255), -1) # Hole annulus to represent damaged 'paint'\n",
    "        # Did the bullet penetrate through the sign?\n",
    "        if (size < 6):\n",
    "            cv2.circle(painted, (h_x, h_y), size, (c,c,c,255), -1)\n",
    "        # If not, grey out rather than make transparent\n",
    "        else:\n",
    "            cv2.circle(bullet_holes, (h_x, h_y), size, (0,0,0), -1)\n",
    "    dmg3 = cv2.bitwise_and(painted, painted, mask=bullet_holes)\n",
    "    \n",
    "    dmg_values = calc_quadrant_diff(dmg3, img)\n",
    "    values = [float_to_string(round(num, 6)) for num in dmg_values] # Convert the values into decimal strings\n",
    "    text_file = open(text_base + str(ii) + \".txt\", \"w\")\n",
    "    text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]} holes={numHoles}\")\n",
    "    ii += 1\n",
    "    cv2.imwrite(os.path.join(output_path, c_num + damage_types[3] + \".png\"), dmg3)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # TINTED YELLOW\n",
    "    yellow = np.zeros((height,width,ch), dtype=np.uint8)\n",
    "    yellow[:,:] = (0,210,210,255)\n",
    "    dmg4 = cv2.bitwise_and(img, yellow)\n",
    "    \n",
    "    # TODO: Use quadrant pixel difference ratio or some other damage metric for labelling?\n",
    "    cv2.imwrite(os.path.join(output_path, c_num + damage_types[4] + \".png\"), dmg4)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # GRAFFITI\n",
    "    dmg5 = graffiti_scribble(img) # Returns a dictionary with the images mapped to their filename\n",
    "    graffiti,ratio = graffiti_writing(img)\n",
    "    dmg5[ratio] = graffiti # Add to the dictionary\n",
    "    \n",
    "    # Loop through the key,value pairs of dictionary\n",
    "    for ratio,dmg in dmg5.items():\n",
    "        dmg_values = calc_quadrant_diff(dmg, img)\n",
    "        values = [float_to_string(round(num, 6)) for num in dmg_values] # Convert the values into decimal strings\n",
    "        text_file = open(text_base + str(ii) + \".txt\", \"w\")\n",
    "        text_file.write(f\"{values[0]} {values[1]} {values[2]} {values[3]}\") # TODO: graffiti='__' field (may be no longer appropriate)\n",
    "        ii += 1\n",
    "        cv2.imwrite(os.path.join(output_path, c_num + damage_types[5] + \"_\" + ratio + \".png\"), dmg)\n",
    "    \n",
    "    \n",
    "    # BEND\n",
    "    dmg6 = bend_vertical(image_path)\n",
    "\n",
    "    for degrees,dmg in dmg6.items():\n",
    "        cv2.imwrite(os.path.join(output_path, c_num + damage_types[6] + \"_\" + degrees + \".png\"), dmg)\n",
    "    # TODO: Write values to file\n",
    "    \n",
    "\n",
    "    # GREY\n",
    "    dmg7 = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)  # Convert to greyscale\n",
    "    # Threshold the image to get a uniform saturation\n",
    "    _, dmg7 = cv2.threshold(dmg7, 100, 255, cv2.THRESH_BINARY)\n",
    "    cv2.convertScaleAbs(dmg7, dmg7, alpha=1, beta=200)  # No change to contrast, scale brightness\n",
    "    dmg7 = cv2.cvtColor(dmg7, cv2.COLOR_GRAY2BGRA)  # Convert back to BGRA to add back the alpha channel\n",
    "    dmg7[:,:,3] = alpha_ch\n",
    "    cv2.imwrite(os.path.join(output_path, c_num + damage_types[7] + \".png\"), dmg7)\n",
    "    # TODO: Test with exposure_manipulation()\n",
    "    # TODO: Write values to file\n",
    "    \n",
    "\n",
    "    # TODO: CRACKS (thin crack lines across the sign?)\n",
    "    # TODO: MISSING SECTIONS (missing polygon sections on edges of sign?)\n",
    "\n",
    "\n",
    "# Remove trailing zeroes and prevent automatic scientific exponent notation\n",
    "# https://stackoverflow.com/a/37736333/12350950\n",
    "def float_to_string(num):\n",
    "    return (\"%.6f\" % num).rstrip(\"0\").rstrip(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create damaged signs using the images with backgrounds removed\n",
    "input_path = os.path.join(\"Traffic_Signs_Templates\", \"2_Processed_Images\")\n",
    "output_dir = os.path.join(\"Traffic_Signs_Templates\", \"3_Damaged_Images\")\n",
    "\n",
    "# Remove any old output and recreate the output directory\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "for image_path in load_files(input_path):\n",
    "    damage_image(image_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a pole to the imported sign\n",
    "def add_pole(filename, color):\n",
    "    img = cv.imread(filename, cv.IMREAD_UNCHANGED)\n",
    "    # Retrieve image dimentions and calculate the new height\n",
    "    height,width,_ = img.shape  # Discard channel\n",
    "    new_height = height * 3\n",
    "\n",
    "    # Create a new blank image with dtype=uint8 (to hold numbers 0-255)\n",
    "    # Initialise values to 0 so that the extended image is fully transparent\n",
    "    new_img = np.zeros((new_height, width, 4), dtype=np.uint8)\n",
    "    # Copy over the original image onto the top portion\n",
    "    new_img[0:height, :] = img\n",
    "\n",
    "    # Calculate coordinates\n",
    "    midpt = width // 2\n",
    "    offset = width // 40\n",
    "    x1, x2 = midpt - offset, midpt + offset\n",
    "    y1, y2 = height, new_height\n",
    "    # Draw the pole\n",
    "    cv.rectangle(new_img, (x1, y1), (x2, y2), color, cv.FILLED)\n",
    "\n",
    "    # Colour any transparent pixels between the sign and the rectangle\n",
    "    lim = 50  # Max alpha value for the pixel to be considered \"transparent\"\n",
    "    margin = int(round( height * 0.9 ))  # Loop over bottom 10% of the sign\n",
    "    for y in range(margin, height):\n",
    "        for x in range(x1, x2+1):  # Loop to x2 inclusive\n",
    "            if new_img[y,x,3] < lim:\n",
    "                new_img[y,x] = color\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(image_path, output_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    height,width,_ = img.shape\n",
    "\n",
    "    dst = []\n",
    "    #0 FORWARD FACING\n",
    "    dst.append( img )\n",
    "\n",
    "    #1 EAST FACING\n",
    "    pts1 = np.float32( [[width/10,height/10], [width/2,height/10], [width/10,height/2]] )\n",
    "    pts2 = np.float32( [[width/5,height/5], [width/2,height/8], [width/5,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts1,pts2)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #2 NORTH-WEST FACING\n",
    "    pts3 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts4 = np.float32( [[width*9/10,height/5], [width/2,height/8], [width*9/10,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts3,pts4)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #3 LEFT TILTED FORWARD FACING\n",
    "    pts5 = np.float32( [[width/10,height/10], [width/2,height/10], [width/10,height/2]] )\n",
    "    pts6 = np.float32( [[width/12,height/6], [width/2.1,height/8], [width/10,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts5,pts6)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #4 RIGHT TILTED FORWARD FACING\n",
    "    pts7 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts8 = np.float32( [[width*10/12,height/6], [width/2.2,height/8], [width*8.4/10,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts7,pts8)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #5 WEST FACING\n",
    "    pts9  = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts10 = np.float32( [[width/9.95,height/10], [width/2.05,height/9.95], [width*9/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts9,pts10)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #6 RIGHT TILTED FORWARD FACING\n",
    "    pts11 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts12 = np.float32( [[width*9/10,height/10], [width/2,height/9], [width*8.95/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts11,pts12)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #7 FORWARD FACING W/ DISTORTION\n",
    "    pts13 = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts14 = np.float32( [[width/9.8,height/9.8], [width/2,height/9.8], [width*8.8/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts13,pts14)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #8 FORWARD FACING W/ DISTORTION 2\n",
    "    pts15 = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts16 = np.float32( [[width/11,height/10], [width/2.1,height/10], [width*8.5/10,height/1.95]] )\n",
    "    M = cv2.getAffineTransform(pts15,pts16)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #9 FORWARD FACING W/ DISTORTION 3\n",
    "    pts17 = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts18 = np.float32( [[width/11,height/11], [width/2.1,height/10], [width*10/11,height/1.95]] )\n",
    "    M = cv2.getAffineTransform(pts17,pts18)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #10 FORWARD FACING W/ DISTORTION 4\n",
    "    pts19 = np.float32( [[width*9.5/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts20 = np.float32( [[width*9.35/10,height/9.99], [width/2.05,height/9.95], [width*9.05/10,height/2.03]] )\n",
    "    M = cv2.getAffineTransform(pts19,pts20)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #11 FORWARD FACING W/ DISTORTION 5\n",
    "    pts21 = np.float32( [[width*9.5/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts22 = np.float32( [[width*9.65/10,height/9.95], [width/1.95,height/9.95], [width*9.1/10,height/2.02]] )\n",
    "    M = cv2.getAffineTransform(pts21,pts22)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #12 FORWARD FACING W/ DISTORTION 6\n",
    "    pts23 = np.float32( [[width*9.25/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts24 = np.float32( [[width*9.55/10,height/9.85], [width/1.9,height/10], [width*9.3/10,height/2.04]] )\n",
    "    M = cv2.getAffineTransform(pts23,pts24)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #13 SHRINK 1\n",
    "    pts25 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts26 = np.float32( [[width*8/10,height/10], [width*1.34/3,height/10.5], [width*8.24/10,height/2.5]] )\n",
    "    M = cv2.getAffineTransform(pts25,pts26)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #14 SHRINK 2\n",
    "    pts27 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts28 = np.float32( [[width*8.5/10,height*3.1/10], [width/2,height*3/10], [width*8.44/10,height*1.55/2.5]] )\n",
    "    M = cv2.getAffineTransform(pts27,pts28)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #15 FORWARD FACING W/ DISTORTION 7\n",
    "    pts29 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts30 = np.float32( [[width*8.85/10,height/9.3], [width/1.9,height/10.5], [width*8.8/10,height/2.11]] )\n",
    "    M = cv2.getAffineTransform(pts29,pts30)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #16 FORWARD FACING W/ DISTORTION 8\n",
    "    pts31 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts32 = np.float32( [[width*8.75/10,height/9.1], [width/1.95,height/8], [width*8.5/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts31,pts32)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #17 FORWARD FACING W/ DISTORTION 9\n",
    "    pts33 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts34 = np.float32( [[width*8.75/10,height/9.1], [width/1.95,height/9], [width*8.5/10,height/2.2]] )\n",
    "    M = cv2.getAffineTransform(pts33,pts34)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #18 FORWARD FACING W/ DISTORTION 10\n",
    "    pts35 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts36 = np.float32( [[width*8.75/10,height/8], [width/1.95,height/8], [width*8.75/10,height/2]] )\n",
    "    M = cv2.getAffineTransform(pts35,pts36)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #19 FORWARD FACING W/ DISTORTION 11\n",
    "    pts37 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts38 = np.float32( [[width*8.8/10,height/7], [width/1.95,height/7], [width*8.8/10,height/2]] )\n",
    "    M = cv2.getAffineTransform(pts37,pts38)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "\n",
    "    # Retrieve the filename to save as\n",
    "    _,tail = ntpath.split(image_path)  # Filename of the image, parent directories removed\n",
    "    title,_ = tail.rsplit('.', 1)      # Discard extension\n",
    "    \n",
    "    # Save the transformed images\n",
    "    for ii in range(1, len(dst)):\n",
    "        save_path = os.path.join(output_path, title)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        cv2.imwrite(os.path.join(save_path, str(ii) + \".png\"), dst[ii])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_path = os.path.join(\"Traffic_Signs_Templates\", \"1_Images\")\n",
    "#input_path = os.path.join(\"Traffic_Signs_Templates\", \"2_Processed_Images\")\n",
    "input_path = os.path.join(\"Traffic_Signs_Templates\", \"3_Damaged_Images\")\n",
    "output_dir = os.path.join(\"Traffic_Signs_Templates\", \"4_Transformed_Images\")\n",
    "\n",
    "# Loop through each folder in the input directory\n",
    "for folder_path in load_paths(input_path):\n",
    "    # Retrieve the sign number to create a directory\n",
    "    _,number = os.path.split(folder_path)\n",
    "    save_dir = os.path.join(output_dir, number)\n",
    "    \n",
    "    for image_path in load_files(folder_path):\n",
    "        img_transform(image_path, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_png(directory):\n",
    "    for files in load_paths(directory):\n",
    "        title,extension = files.split('.')\n",
    "        img = Image.open(files).convert('RGBA')\n",
    "        if (not extension == \"png\"):\n",
    "            os.remove(files)\n",
    "        img.save(title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "for bg_folders in load_paths(\"Backgrounds\"):\n",
    "    to_png(bg_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_exposure(paths, channels):\n",
    "    exposures = []\n",
    "    for image_path in paths:\n",
    "        img_grey = Image.open(image_path).convert('LA')  # Greyscale with alpha\n",
    "        img_rgba = Image.open(image_path)\n",
    "        \n",
    "        stat1 = ImageStat.Stat(img_grey)\n",
    "        # Average pixel brighness\n",
    "        avg = stat1.mean[0]\n",
    "        # RMS pixel brighness\n",
    "        rms = stat1.rms[0]\n",
    "        \n",
    "        stat2 = ImageStat.Stat(img_rgba)\n",
    "        # Consider the number of channels\n",
    "        # Background may have RGB while traffic sign has RGBA\n",
    "        if (channels == 3):\n",
    "            # Average pixels preceived brightness\n",
    "            r,g,b = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            # RMS pixels perceived brightness\n",
    "            r,g,b = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "        else:\n",
    "            # Average pixels preceived brightness\n",
    "            r,g,b,a = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            # RMS pixels perceived brightness\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "        exposures.append( [image_path,avg,rms,avg_perceived,rms_perceived] )\n",
    "\n",
    "    return exposures     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths, 4)\n",
    "    signs_exposures = find_image_exposure(signs_paths, 4)\n",
    "    \n",
    "    for ii in range(0,len(background_paths)):\n",
    "        print(\"Processed: \" + str(float(ii) / float(len(background_paths)) * 100) + \" %\")\n",
    "        \n",
    "        img = Image.open(background_exposures[ii][0])\n",
    "\n",
    "        for sign_path in signs_paths:        \n",
    "            dirc, sub, el = dir_split(background_exposures[ii][0])\n",
    "            title, extension = el.rsplit('.', 1)\n",
    "\n",
    "            parent_dir, sub_dir, folder, folder2, element = dir_split(sign_path)\n",
    "            head, tail = element.rsplit('.', 1)\n",
    "\n",
    "            \n",
    "            ###   ORIGINAL EXPOSURE IMPLEMENTATION   ###\n",
    "            brightness_avrg = 1.0\n",
    "            brightness_rms = 1.0\n",
    "            brightness_avrg_perceived = 1.0\n",
    "            brightness_rms_perceived = 1.0\n",
    "            brightness_avrg2 = 1.0\n",
    "            brightness_rms2 = 1.0\n",
    "\n",
    "            # abs(desired_brightness - actual_brightness) / abs(brightness_float_value) = ratio\n",
    "            avrg_ratio = 11.0159464507\n",
    "\n",
    "            rms_ratio = 8.30320014372\n",
    "\n",
    "            percieved_avrg_ratio = 3.85546373056\n",
    "\n",
    "            percieved_rms_ratio = 35.6344530649\n",
    "\n",
    "            avrg2_ratio = 1.20354549572\n",
    "\n",
    "            rms2_ratio = 40.1209106864\n",
    "\n",
    "            peak1 = Image.open(sign_path).convert('LA')\n",
    "            peak2 = Image.open(sign_path).convert('RGBA')\n",
    "\n",
    "            stat = ImageStat.Stat(peak1)\n",
    "            avrg = stat.mean[0]\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            # IMAGE MANIPULATION MAIN CODE STARTS\n",
    "\n",
    "            # MINIMISE MARGIN BASED ON AVERAGE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(avrg - float(background_exposures[ii][1]))\n",
    "            \n",
    "            brightness_avrg = margin / avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright = enhancer.enhance(brightness_avrg)\n",
    "            stat = ImageStat.Stat(avrg_bright)\n",
    "            avrg = stat.mean[0]\n",
    "\n",
    "            \n",
    "            # MINIMISE MARGIN BASED ON ROOT MEAN SQUARE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(rms - float(background_exposures[ii][2]))\n",
    "\n",
    "            brightness_rms = margin / rms_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright = enhancer.enhance(brightness_rms)\n",
    "            stat = ImageStat.Stat(rms_bright)\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            # MINIMISE MARGIN BASED ON AVERAGE FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            # REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            stat2 = ImageStat.Stat(peak2)\n",
    "            r, g, b, a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            margin = abs(avrg_perceived - float(background_exposures[ii][3]))\n",
    "            \n",
    "            brightness_avrg_perceived = margin / percieved_avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright_perceived = enhancer.enhance(brightness_avrg_perceived)\n",
    "            stat2 = ImageStat.Stat(avrg_bright_perceived)\n",
    "            r, g ,b, a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241 * (r**2) + 0.691 * (g**2) + 0.068 * (b**2))        \n",
    "\n",
    "\n",
    "            # MINIMISE MARGIN BASED ON RMS FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            # REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            r, g, b, a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241 * (r**2) + 0.691 * (g**2) + 0.068 * (b**2))\n",
    "\n",
    "            margin = abs(rms_perceived - float(background_exposures[ii][4]))\n",
    "\n",
    "            brightness_rms_perceived = margin / percieved_rms_ratio \n",
    "\n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright_perceived = enhancer.enhance(brightness_rms_perceived)\n",
    "            stat2 = ImageStat.Stat(rms_bright_perceived)\n",
    "            r, g, b, a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241 * (r**2) + 0.691 * (g**2) + 0.068 * (b**2))        \n",
    "\n",
    "            \n",
    "            stat3 = ImageStat.Stat(peak2)\n",
    "            avrg2 = stat3.mean[0]\n",
    "            rms2 = stat3.rms[0]\n",
    "\n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO AVERAGING METHODS\n",
    "            margin = abs(avrg2-float(background_exposures[ii][1]))\n",
    "            brightness_avrg2 = margin/avrg2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright2 = enhancer.enhance(brightness_avrg2)\n",
    "            stat3 = ImageStat.Stat(avrg_bright2)\n",
    "            avrg2 = stat3.mean[0]       \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO RMS METHODS\n",
    "            margin = abs(rms2-float(background_exposures[ii][2]))\n",
    "            brightness_rms2 = margin/rms2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright2 = enhancer.enhance(brightness_rms2)\n",
    "            stat3 = ImageStat.Stat(rms_bright2)\n",
    "            rms2 = stat3.rms[0]\n",
    "            \"\"\"\n",
    "            \n",
    "            avrg_bright = avrg_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright = rms_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            avrg_bright_perceived = avrg_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright_perceived = rms_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            #avrg_bright2 = avrg_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "            #rms_bright2 = rms_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "\n",
    "            \n",
    "            exp_dir = \"Traffic_Signs_Exposure_Manipulation\"\n",
    "            avrg_bright.save(os.path.join(exp_dir,sub,title,\"SIGN_\"+folder,folder2,head+\"_AVERAGE.\"+tail))\n",
    "            rms_bright.save(os.path.join(exp_dir,sub,title,\"SIGN_\"+folder,folder2,head+\"_RMS.\"+tail))\n",
    "            avrg_bright_perceived.save(os.path.join(exp_dir,sub,title,\"SIGN_\"+folder,folder2,head+\"_AVERAGE_PERCEIVED.\"+tail))\n",
    "            rms_bright_perceived.save(os.path.join(exp_dir,sub,title,\"SIGN_\"+folder,folder2,head+\"_RMS_PERCEIVED.\"+tail))\n",
    "            #avrg_bright2.save(os.path.join(exp_dir,sub,title,\"SIGN_\"+folder,folder2,head+\"_AVERAGE2.\"+tail))\n",
    "            #rms_bright2.save(os.path.join(exp_dir,sub,title,\"SIGN_\"+folder,folder2,head+\"_RMS2.\"+tail))\n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths, 4)\n",
    "    signs_exposures = find_image_exposure(signs_paths, 4)\n",
    "    \n",
    "    print(\"Processed: 0.0 %\")\n",
    "    ii = 0\n",
    "    prev = 0\n",
    "    for sign_path in signs_paths:\n",
    "        progress = float(ii) / float(len(signs_paths)) * 100\n",
    "        if progress >= prev + 5: #Prevent spamming of progress prints\n",
    "            prev = prev + 5\n",
    "            print(\"Processed: \" + str(progress) + \" %\")\n",
    "\n",
    "        dirc, sub, el = dir_split(background_exposures[0][0])\n",
    "        title, extension = el.split('.')\n",
    "\n",
    "        parent_dir, sub_dir, folder, folder2, element = dir_split(sign_path)\n",
    "        head, tail = element.split('.')\n",
    "\n",
    "        img = cv2.imread(sign_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "        ###   GRADUAL FADE IMPLEMENTATION   ###\n",
    "        #Retrieve alpha data from original image\n",
    "        splitImg = cv2.split(img)\n",
    "        if len(splitImg) is 4:\n",
    "            alphaData = splitImg[3]\n",
    "\n",
    "        for jj in range(0,5):\n",
    "            dmg6 = img.copy()\n",
    "            alpha = 1 - (jj * 0.19)\n",
    "            beta = (jj + 1) * 40\n",
    "            if jj > 0:\n",
    "                cv2.convertScaleAbs(img, dmg6, alpha, beta) #Scale the contrast and brightness\n",
    "                dmg6[:, :, 3] = alphaData\n",
    "\n",
    "            dmg6 = cv2.resize(dmg6, (150,150))\n",
    "            fad_dir = \"Traffic_Signs_Fade_Manipulation\"\n",
    "            cv2.imwrite(os.path.join(fad_dir,\"SIGN_\"+folder,folder2,head+\"_FADE-\"+str(jj)+\".\"+tail, dmg6))\n",
    "        ii = ii + 1\n",
    "    \n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bg_dir = \"Backgrounds\"\n",
    "transform_dir = os.path.join(\"Traffic_Signs_Templates\", \"4_Transformed_Images\")\n",
    "\n",
    "for dirs in load_paths(bg_dir):\n",
    "    sep = os.sep # Directory separator: '/' or '\\'\n",
    "\n",
    "    if original is True:\n",
    "        for background in load_paths(dirs):\n",
    "            iniitial, subd, element = dir_split(background)\n",
    "            title, extension = element.split('.')\n",
    "\n",
    "            for signp in load_paths(transform_dir):\n",
    "                for sign in load_paths(signp):\n",
    "                    d,s,f,e = dir_split(sign)  # Eg. s = 4_Transformed_Images, f = 9, e = 9_BOTTOM_HOLE\n",
    "\n",
    "                    exp_dir = \"Traffic_Signs_Exposure_Manipulation\" + sep\n",
    "                    if (not os.path.exists(exp_dir + subd + sep + title + sep + \"SIGN_\" + f + sep + e)):\n",
    "                        os.makedirs(exp_dir + subd + sep + title + sep + \"SIGN_\" + f + sep + e)\n",
    "    else:\n",
    "        for signp in load_paths(transform_dir):\n",
    "            for sign in load_paths(signp):\n",
    "                d,s,f,e = dir_split(sign)\n",
    "\n",
    "                fade_dir = \"Traffic_Signs_Fade_Manipulation\" + sep\n",
    "                if (not os.path.exists(fade_dir + \"SIGN_\" + f + sep + e)):\n",
    "                    os.makedirs(fade_dir + \"SIGN_\" + f + sep + e)\n",
    "\n",
    "signs_paths = []\n",
    "for p in load_paths(transform_dir):\n",
    "    for d in load_paths(p):\n",
    "        signs_paths += load_paths(d)\n",
    "\n",
    "background_paths = []  # Load the paths of the background images into a single list\n",
    "for subfolder in load_paths(bg_dir):\n",
    "    background_paths += load_paths(subfolder)\n",
    "\n",
    "if original is True:\n",
    "    exposure_manipulation(signs_paths, background_paths)\n",
    "else:\n",
    "    fade_manipulation(signs_paths, background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avrg_pixel_rgb(image, chanels):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    if (chanels == 4):\n",
    "        r,g,b,a = stat.rms\n",
    "    else:\n",
    "        r,g,b = stat.rms\n",
    "    \n",
    "    return [r,g,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bw_images(directory):\n",
    "    images = []\n",
    "    for sign in load_paths(directory):\n",
    "        for damage in load_paths(sign):\n",
    "            img = Image.open(damage).convert('RGBA')\n",
    "            rgb = avrg_pixel_rgb(img, 4)\n",
    "            rg = abs(rgb[0] - rgb[1])\n",
    "            rb = abs(rgb[0] - rgb[2])\n",
    "            gb = abs(rgb[1] - rgb[2])\n",
    "            \n",
    "            temp = dir_split(damage)\n",
    "            head,tail = temp[-1].split('.')\n",
    "                    \n",
    "            if (rg <= 1 and rb <= 1 and gb <= 1):\n",
    "                images.append(head)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_useful_signs(directory): #Removes bad signs, such as those which are all white or all black\n",
    "    bw_images = find_bw_images(os.path.join(\"Traffic_Signs_Templates\", \"3_Damaged_Images\"))\n",
    "    for background_dir in load_paths(directory):\n",
    "        for background in load_paths(background_dir):\n",
    "            for signs in load_paths(background):\n",
    "                for dmgs in load_paths(signs):\n",
    "                    temp = []\n",
    "                    for imgs in load_paths(dmgs):\n",
    "                        temp.append(imgs)\n",
    "                    exposures = find_image_exposure(temp,4)\n",
    "                    \n",
    "                    i = 0\n",
    "                    for images in load_paths(dmgs):\n",
    "                        #Find brightness\n",
    "                        img = Image.open(images).convert('RGBA')\n",
    "\n",
    "                        rgb = avrg_pixel_rgb(img,4)\n",
    "                        rg = abs(rgb[0]-rgb[1])\n",
    "                        rb = abs(rgb[0]-rgb[2])\n",
    "                        gb = abs(rgb[1]-rgb[2])\n",
    "\n",
    "                        is_bw = False\n",
    "\n",
    "                        for s in bw_images:\n",
    "                            if s in exposures[i][0]:\n",
    "                                is_bw = True\n",
    "\n",
    "                        if (rg<=16 and rb<=16 and gb<=16):\n",
    "                            if (not is_bw):\n",
    "                                os.remove(images)\n",
    "                            #Threshold values for black and white images\n",
    "                            elif (rgb[0]<70 and rgb[1]<70 and rgb[2]<70):\n",
    "                                os.remove(images)\n",
    "                            elif (rgb[0]>155 and rgb[1]>155 and rgb[2]>155):\n",
    "                                os.remove(images)\n",
    "\n",
    "                        elif (not is_bw):\n",
    "                            #Delete light blue images\n",
    "                            if(rgb[2]>rgb[0] and rgb[2]>=rgb[1]):\n",
    "                                if (gb<=10):\n",
    "                                    os.remove(images)\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directory = \"Traffic_Signs_Exposure_Manipulation\"\n",
    "    find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_poisson_noise (image):\n",
    "    vals = len(np.unique(image))\n",
    "    vals = 2.05 ** np.ceil(np.log2(vals))\n",
    "    noisy = np.random.poisson(image * vals) / float(vals)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_Gaussian_noise (image):\n",
    "    row,col,ch= image.shape\n",
    "    mean = 0\n",
    "    var = 0.5\n",
    "    sigma = var**0.5\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_speckle_noise (image):\n",
    "    row,col,ch = image.shape\n",
    "    gauss = np.random.randn(row,col,ch)\n",
    "    gauss = gauss.reshape(row,col,ch)        \n",
    "    noisy = image + image * gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_method (image):\n",
    "    \"\"\"\n",
    "    i = random.randint(1, 3)\n",
    "    if (i == 1):\n",
    "        return insert_poisson_noise(image)\n",
    "    elif (i==2):\n",
    "        return insert_Gaussian_noise(image)\n",
    "    else:\n",
    "        return insert_speckle_noise(image)\n",
    "    \"\"\"\n",
    "    image.setflags(write=1)\n",
    "    #Add noise in every pixel w/ random probability 0.4\n",
    "    for im in image:\n",
    "        px = 0\n",
    "        for pixel in im:\n",
    "            apply_noise = random.randint(0,100)\n",
    "            #if random probability\n",
    "            if apply_noise > 40:\n",
    "                #RGB values\n",
    "                R = pixel[0]\n",
    "                G = pixel[1]\n",
    "                B = pixel[2]\n",
    "                A = pixel[3]\n",
    "                #find current relative lumination for brighness\n",
    "                #based on: https://en.wikipedia.org/wiki/Relative_luminance\n",
    "                relative_lumination = 0.2126*R + 0.7152*G + 0.0722*B\n",
    "                #find differences between RGB values     \n",
    "                R_to_G = float(R)/float(G)\n",
    "                RG = False\n",
    "                if (R_to_G >= 1): RG=True\n",
    "                R_to_B = float(R)/float(B)\n",
    "                RB = False\n",
    "                if (R_to_B >= 1): RB=True\n",
    "                G_to_B = float(G)/float(B)\n",
    "                GB = False\n",
    "                if (G_to_B >= 1): GB=True\n",
    "                equal = False\n",
    "                if (R==G==B):equal==True\n",
    "\n",
    "                #In order to determine the margin in which the new brighness\n",
    "                #should be within, the upper and lower limits need to be foun\n",
    "                #The Relative luminance in colorimetric spaces has normilised\n",
    "                #values between 0 and 255\n",
    "                upper_limit = 255\n",
    "                lower_limit = 0\n",
    "                if (relative_lumination + 40 < 255):\n",
    "                    upper_limit = relative_lumination + 40\n",
    "                if (relative_lumination - 40 > 0):\n",
    "                    lower_limit = relative_lumination - 40\n",
    "\n",
    "                #Compute new brighness value\n",
    "                new_lumination = random.randint(int(lower_limit),int(upper_limit))\n",
    "\n",
    "                #find the three possible solutions that satisfy\n",
    "                #->The new lumination chosen based on the Relative luminance equation\n",
    "                #->The precentages computed between every RGB value\n",
    "\n",
    "                solutions = []\n",
    "\n",
    "                for r in range(1,255):\n",
    "                    for g in range(1,255):\n",
    "                        for b in range(1,255):\n",
    "                            r_to_g = float(r)/float(g)\n",
    "                            rg = False\n",
    "                            if (r_to_g >= 1): rg=True\n",
    "                            r_to_b = float(r)/float(b)\n",
    "                            rb = False\n",
    "                            if (r_to_b >= 1): rb=True\n",
    "                            g_to_b = float(g)/float(b)\n",
    "                            gb = False\n",
    "                            if (g_to_b >= 1): gb=True\n",
    "                            e = False\n",
    "                            if(r==g==b):\n",
    "                                e=True\n",
    "                            if (0.2126*r + 0.7152*g + 0.0722*b == 100) and rg==RG and rb==RB and gb==GB and e==equal:\n",
    "                                solutions.append([r,g,b])\n",
    "\n",
    "                #Find the solution that precentage wise is closer to the original\n",
    "                #difference between the values\n",
    "                percentages = []\n",
    "\n",
    "                for solution in solutions:\n",
    "                    r = solution[0]\n",
    "                    g = solution[1]\n",
    "                    b = solution[2]\n",
    "                    percentages.append((float(r)/float(g))+(float(r)/float(b))+(float(g)/float(b)))\n",
    "\n",
    "                i = 0\n",
    "                pos = 0\n",
    "                best = percentages[0]\n",
    "                for p in percentages[1:]:\n",
    "                    if p < best:\n",
    "                        pos = i\n",
    "                    i = i +1\n",
    "\n",
    "                #Assign new pixel values\n",
    "                im[px] = [solutions[pos][0],solutions[pos][1],solutions[pos][2],A]\n",
    "            px = px+1\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used by bounding_axes(image_dir)\n",
    "# Checks if a line of pixels contains a pixel above a transparency threshold\n",
    "def has_opaque_pixel(line):\n",
    "    opaque = False\n",
    "    for pixel in line:\n",
    "        if pixel[3] > 200: # Check if pixel is opaque\n",
    "            opaque = True\n",
    "            break # Stop searching if one is found\n",
    "    return opaque\n",
    "\n",
    "# Returns the bounding axes of an image with a transparent background\n",
    "def bounding_axes(img):\n",
    "    # Top axis\n",
    "    y_top = 0\n",
    "    for row in img: # Iterate through each row of pixels, starting at top-left\n",
    "        if has_opaque_pixel(row) is False: # Check if the row has an opaque pixel\n",
    "            y_top += 1 #If not, move to the next row\n",
    "        else:\n",
    "            break # If so, break, leaving y_top as the bounding axis\n",
    "\n",
    "    # Bottom axis\n",
    "    height = img.shape[0]\n",
    "    y_bottom = height - 1\n",
    "    for row in reversed(img): # Iterate from the bottom row up\n",
    "        if has_opaque_pixel(row) is False:\n",
    "            y_bottom -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Left axis\n",
    "    r_img = imutils.rotate_bound(img, 90) # Rotate to iterate through what were originally columns\n",
    "    x_left = 0\n",
    "    for column in r_img:\n",
    "        if has_opaque_pixel(column) is False:\n",
    "            x_left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Right axis\n",
    "    r_height = r_img.shape[0]\n",
    "    x_right = r_height - 1\n",
    "    for column in reversed(r_img):\n",
    "        if has_opaque_pixel(column) is False:\n",
    "            x_right -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # FOR TESTING\n",
    "    # img[y_top, :] = (255, 0, 0, 255)\n",
    "    # img[y_bottom, :] = (255, 0, 0, 255)\n",
    "    # img[:, x_left] = (255, 0, 0, 255)\n",
    "    # img[:, x_right] = (255, 0, 0, 255)\n",
    "    # cv2.imwrite(image_dir, img)\n",
    "\n",
    "    return [x_left, x_right, y_top, y_bottom]\n",
    "\n",
    "# FOR TESTING\n",
    "# for img in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images/0/0_ORIGINAL\"):\n",
    "#     bounding_axes(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORIGINAL ### (Outdated - used for creating classification data)\n",
    "# def new_data(image_dir,bg_dir): #Blends synthetic signs with backgrounds\n",
    "#     # Import background image\n",
    "#     background_img_raw = Image.open(bg_dir).convert('RGBA')  \n",
    "#     background_img_raw = background_img_raw.resize((150,150), Image.ANTIALIAS)\n",
    "#     background_img = np.array(background_img_raw)  \n",
    "#     background_img_float = background_img.astype(float)  \n",
    "\n",
    "#     # Import foreground image\n",
    "#     foreground_img_raw = Image.open(image_dir)  \n",
    "#     foreground_img = np.array(foreground_img_raw)  \n",
    "#     foreground_img_float = foreground_img.astype(float)  \n",
    "\n",
    "#     # Blend images\n",
    "#     opacity = 1  \n",
    "#     blended_img_float = blend_modes.grain_merge(background_img_float, foreground_img_float, opacity)\n",
    "\n",
    "#     # Convert blended image back into PIL image\n",
    "#     blended_img = np.uint8(blended_img_float)\n",
    "#     blended_img_raw = Image.fromarray(blended_img)  \n",
    "    \n",
    "#     foreground_img_raw = foreground_img_raw.resize((149,149), Image.ANTIALIAS)\n",
    "#     blended_img_raw.paste(foreground_img_raw, (0, 0), foreground_img_raw)\n",
    "#     blended_img_raw = blended_img_raw.resize((48,48), Image.ANTIALIAS)\n",
    "    \n",
    "#     # temp = np.uint8(blended_img_raw)\n",
    "#     # temp = random_noise_method(temp)\n",
    "    \n",
    "#     # blended_img_raw = Image.fromarray(np.uint8(temp)) \n",
    "    \n",
    "#     return blended_img_raw\n",
    "\n",
    "### FULL BACKGROUND ###\n",
    "def new_data(image_dir, bg_dir, label_file, filename, values): # Blends synthetic signs with backgrounds\n",
    "    bg = cv2.imread(bg_dir, cv2.IMREAD_UNCHANGED)\n",
    "    fg = cv2.imread(image_dir, cv2.IMREAD_UNCHANGED)\n",
    "    bg_height, bg_width, _ = bg.shape\n",
    "    fg_height, fg_width, _ = fg.shape\n",
    "\n",
    "    # Rescaling the sign to correct its size relative to the background\n",
    "    # NOTE: Assumes background aspect ratio somewhat close to 16:9 (1.778)\n",
    "    current_ratio = fg_width / bg_width # Ratio of sign width to the background width\n",
    "    target_ratio = random.uniform(0.033, 0.066) # Aiming for between 3.3% and 6.6% of bg width\n",
    "    scale_factor = target_ratio / current_ratio\n",
    "    new_size = int(fg_width * scale_factor)\n",
    "    fg = cv2.resize(fg, (new_size, new_size))\n",
    "\n",
    "    # Randomise sign placement within middle third of background\n",
    "    x = random.randint(0, bg_width - fg_width)\n",
    "    third = bg_height // 3\n",
    "    y = random.randint(third, bg_height - third)\n",
    "\n",
    "    # Building label\n",
    "    axes = bounding_axes(fg) # Retrieve bounding axes of the sign image\n",
    "    axes[0] += x # Adjusting bounding axis to make it relative to the whole bg image\n",
    "    axes[1] += x\n",
    "    axes[2] += y\n",
    "    axes[3] += y\n",
    "    bounds = str(axes[0]) + \" \" + str(axes[1]) + \" \" + str(axes[2]) + \" \" + str(axes[3])\n",
    "\n",
    "    # It is assumed that the final .jpg -> .png conversion step is executed\n",
    "    label = filename + \".jpg \" + bounds + \" \" + values + \"\\n\"\n",
    "    label_file.write(label)\n",
    "    image = overlay(fg, bg, x, y)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the required directories in SGTSD\n",
    "if (not os.path.exists(\"SGTSD/Images\")):\n",
    "    #Numbered Version\n",
    "    for sign in load_paths(os.path.join(\"Traffic_Signs_Templates\", \"1_Images\")): #Dir. for each sign type\n",
    "        head, tail = sign.split('.')\n",
    "        name = dir_split(head)\n",
    "        os.makedirs(os.path.join(\"SGTSD\", \"Images\", name[-1]))\n",
    "        j = 0\n",
    "        for dmg in range(len(damage_types)): #Dir. for each damage type\n",
    "            os.makedirs(os.path.join(\"SGTSD\", \"Images\", name[-1], name[-1] + \"_\" + str(j)))\n",
    "            j = j + 1\n",
    "    \n",
    "    #Named Version (Outdated?)\n",
    "#     for sign in load_paths(os.path.join(\"Traffic_Signs_Templates\", \"1_Images\")): #Dir. for each sign type\n",
    "#         head, tail = sign.split('.')\n",
    "#         name = dir_split(head)\n",
    "#         os.makedirs(os.path.join(\"SGTSD\", \"Images\", name[-1]))\n",
    "#         for dmg in load_paths(os.path.join(\"Traffic_Signs_Templates\", \"3_Damaged_Images\")): #Dir. for each damage type\n",
    "#             headD,tailD = dmg.split('.')\n",
    "#             nameD = dir_split(headD)\n",
    "#             os.makedirs(os.path.join(\"SGTSD\", \"Images\", name[-1], nameD[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a README file\n",
    "content = '''\n",
    "-----------------------------------------------\n",
    "|                     -*-                     |\n",
    "|Synthetically Generated Traffic Sign Dataset |\n",
    "|                     -*-                     |\n",
    "-----------------------------------------------\n",
    "\n",
    "This directory contains the training set for\n",
    "The Convolutional Neural Network (CNN)\n",
    "Used in this project\n",
    "\n",
    "However, it can be used for any classifier\n",
    "desired by the person using the code and\n",
    "additionally, it is not limited to a specific\n",
    "traffic sign templates.\n",
    " \n",
    "\n",
    "----------------------------------------------\n",
    "Content\n",
    "----------------------------------------------\n",
    "\n",
    "The number of example is based on the number:\n",
    "->of traffic signs that were used as templates\n",
    "->of the image manipulation processes\n",
    "->of the brighness variations values used\n",
    "->of the blending procedures\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Image format and naming\n",
    "----------------------------------------------\n",
    "The images created are of \"jpg\" format\n",
    "with RGBA channels\n",
    "\n",
    "   SIGN_X/XXX_YYY.jpg\n",
    "\n",
    "The initial part (X) is used to distinguish the\n",
    "sign class, while the remaining (XXX_YYY) firstly\n",
    "indicated the sign in the file itself and the\n",
    "example number.\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Additional information\n",
    "----------------------------------------------\n",
    "\n",
    "contact email: \n",
    "    \n",
    "\tasterga@essex.ac.uk\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Alexandros Stergiou\n",
    "\"The Driver's Assistant\"\n",
    "\n",
    "University of Essex,\n",
    "School of Computer Science and\n",
    "Electronic Engineering,\n",
    "UK\n",
    "----------------------------------------------\n",
    "'''\n",
    "text_file = open(\"SGTSD/Readme_Images.txt\", \"w\")\n",
    "text_file.write(content)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using exposure_manipulation\n",
    "def create_paths_list(imgs_directory, bg_directory):\n",
    "    directories = []\n",
    "    for places in load_paths(imgs_directory): #List of places: originally either UK_rural or UK_urban\n",
    "        for imgs in load_paths(places): #Folder for each bg image: eg. IMG_0\n",
    "            dr = dir_split(imgs)\n",
    "            bg = os.path.join(bg_directory, dr[-2], dr[-1] + \".png\") #Retrieving relevant bg image\n",
    "            for signs in load_paths(imgs): #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 0_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, bg])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using fade_manipulation; backgrounds are assigned to \n",
    "def create_assigned_paths_list(imgs_directory, bg_directory): #TODO: is this the same as above?\n",
    "    directories = []\n",
    "    for places in load_paths(bg_directory): #Folder for each place: eg. GTSDB\n",
    "        for imgs in load_paths(places): #Iterate through each b.g. image: eg. IMG_0\n",
    "            for signs in load_paths(imgs_directory):  #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 9_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, imgs])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directories = create_paths_list(\"Traffic_Signs_Exposure_Manipulation\", \"Backgrounds\")\n",
    "else:\n",
    "    directories = create_assigned_paths_list(\"Traffic_Signs_Fade_Manipulation\", \"Backgrounds\")\n",
    "print(\"Files to be generated: \" + str(len(directories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths of images needed to generate examples for 'sign' with damage 'dmg'\n",
    "def list_for_sign_x(sign, dmg, directories):\n",
    "    l = []\n",
    "    for elements in directories:\n",
    "        foreground = dir_split(elements[0])\n",
    "        if (foreground[-2] == sign + dmg): #Eg. if (9_YELLOW == 4_ORIGINAL)\n",
    "            l.append(elements)\n",
    "    return l #Directory for every single sign and it's relevant background image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_directories = [] #Reformat list to have each sign and damage as their own dimensions\n",
    "signs = load_paths(os.path.join(\"Traffic_Signs_Templates\", \"1_Images\"))\n",
    "for i in signs:\n",
    "    head, tail = ntpath.split(i)\n",
    "    sign, extension = tail.split('.') #Eg. sign == \"9\"\n",
    "\n",
    "    sign_list = [] #List of damages, which are each list of signs\n",
    "    for dmg in damage_types: #damage_types is from 'def damage_images:' cell\n",
    "        sign_list.append(list_for_sign_x(sign, dmg, directories))\n",
    "    final_directories.append(sign_list) #List of types -> lists of damages -> lists of signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate and write the new data to it's file\n",
    "direct = os.path.join(\"SGTSD\", \"Images\")\n",
    "direct1 = os.path.join(\"SGTSD\", \"Labels\")\n",
    "folders = load_paths(direct)\n",
    "n = [] #Numbers from the folder names for the signs\n",
    "for folder in folders:\n",
    "    head, tail = ntpath.split(folder)\n",
    "    n.append(tail)\n",
    "\n",
    "#Count how many signs there are; needed for the progress bar\n",
    "total = 0\n",
    "for signs in final_directories:\n",
    "    for damages in signs:\n",
    "        for dirs in damages:\n",
    "            total += 1\n",
    "\n",
    "count = 0\n",
    "i = 0\n",
    "for signs in final_directories: #Iterating through sign types\n",
    "    j = 0\n",
    "    for damages in signs: #Iterating through damage types\n",
    "        #FIXME: For some reason secondary progress counter has broken from initial implementation\n",
    "        print(\"Processed: \" + str(float(count) / float(total) * 100) + \" %\")\n",
    "        k = 0\n",
    "\n",
    "        label_filename = os.path.join(direct1, n[i], n[i] + \"_\" + str(j) + \".txt\")\n",
    "        text_file = open(label_filename, \"r\")\n",
    "        values = text_file.read() #Retrieve the damage values created earlier in the damage_images function\n",
    "        text_file = open(label_filename, \"w\")\n",
    "\n",
    "        for dirs in damages: #dirs == the foreground and background images for one generated sign\n",
    "            filename = os.path.join(direct, n[i], n[i] + \"_\" + str(j), n[i] + \"_\" + str(j) + \"_\" + str(k))\n",
    "            image = new_data(dirs[0], dirs[1], text_file, filename, values) #Combining b.g. with sign f.g.\n",
    "            cv2.imwrite(filename + \".png\", image)\n",
    "\n",
    "            count += 1\n",
    "            k += 1\n",
    "        j += 1\n",
    "        text_file.close()\n",
    "    i += 1\n",
    "print(\"Processed: \" + str(100) + \" %\") #FIXME: changing damage names broke something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "-------------------------------------\n",
    "BREAKDOWN OF FILES GENERATED BY CLASS\n",
    "-------------------------------------\n",
    "'''\n",
    "total = 0\n",
    "for i in range(len(final_directories)):\n",
    "    current = 0\n",
    "    for j in range(len(final_directories[i])):\n",
    "        current = current + len(final_directories[i][j])\n",
    "    s = \"Generated \" + str(current) + \" examples for sign class \" + str(i + 1)\n",
    "    string = string + '\\n' + s + '\\n'\n",
    "    total = total + current\n",
    "string = string + '\\n' + \"TOTAL: \" + str(total) + '\\n' + \"Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M\") + '\\n'\n",
    "string = string + \"-------------------------------------\"\n",
    "text_file = open(os.path.join(\"SGTSD\", \"generated_images_about.txt\"), \"w\")\n",
    "text_file.write(string)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_to_jpeg(filepath):\n",
    "    sep = os.sep\n",
    "\n",
    "    dirs = dir_split(filepath)\n",
    "    title,extension = dirs[-1].split('.')\n",
    "    del dirs[-1]\n",
    "    string = sep.join(dirs)\n",
    "    string = os.path.join(string, title + \".jpg\")\n",
    "    png = Image.open(filepath)\n",
    "    png.load() # Required for png.split()\n",
    "    background = Image.new(\"RGB\", png.size, (255, 255, 255))\n",
    "    background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
    "    background.save(string, 'JPEG', quality=100)\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirs = direct = os.path.join(\"SGTSD\", \"Images\")\n",
    "i = 1\n",
    "for path in load_paths(dirs):\n",
    "    print(\"Processed: \" + str(float(i - 1) / float(len(final_directories)) * 100) + \" %\")\n",
    "    for damage in load_paths(path):\n",
    "        for image in load_paths(damage):\n",
    "            if (image.endswith(\"png\")):\n",
    "                png_to_jpeg(image)\n",
    "    i += 1\n",
    "print(\"Processed: \" + str(100) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"Traffic_Signs_Exposure_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"Traffic_Signs_Fade_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"Traffic_Signs_Templates/4_Transformed_Images\")\n",
    "# shutil.rmtree(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "# shutil.rmtree(\"Traffic_Signs_Templates/2_Processed_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"SGTSD\") #Be careful with this one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tensorflow_cpu': conda)",
   "language": "python",
   "name": "python_defaultSpec_1599781709470"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}