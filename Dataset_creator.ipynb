{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ntpath\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageChops, ImageDraw, ImageOps, ImageFilter, ImageStat, ImageEnhance \n",
    "#from skimage import io, color, exposure, transform\n",
    "#import imutils\n",
    "import argparse\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import random\n",
    "#from blend_modes import blend_modes\n",
    "import blend_modes\n",
    "\n",
    "original = True # Whether we are using the original exposure_manipulation code or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list with paths of all files in the directory\n",
    "def load_paths(directory):\n",
    "    paths = []\n",
    "    for filename in os.listdir(directory): # Retrieve names of files in directory\n",
    "        # Concatenate filename with directory path, ignoring hidden files\n",
    "        path = os.path.join(directory, filename)\n",
    "        if not filename.startswith('.'):\n",
    "            paths.append(path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list with paths of all non-directory files in the directory\n",
    "def load_files(directory):\n",
    "    paths = []\n",
    "    for filename in os.listdir(directory): # Retrieve names of files in directory\n",
    "        # Concatenate filename with directory path, ignoring hidden files and directories\n",
    "        path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(path) and not filename.startswith('.'):\n",
    "            paths.append(path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Alexandros Stergiou's \"find_borders()\"\n",
    "# Returns an alpha channel that matches the white background\n",
    "def create_alpha(img, alpha_channel):\n",
    "    # Read and decode the image contents for pixel access\n",
    "    pix = img.load()\n",
    "\n",
    "    # Note PIL indexes [x,y] while OpenCV indexes [y,x]\n",
    "    # alpha_channel must be indexed [y,x] to merge with OpenCV channels later\n",
    "\n",
    "    min = 200\n",
    "    width, height = img.size\n",
    "    # Loop through each row of the image\n",
    "    for y in range(0, height):\n",
    "        # First loop left to right\n",
    "        for x in range(0, width, 1):\n",
    "            # Retrieve a tuple with RGB values for this pixel\n",
    "            rgb = pix[x,y]\n",
    "            # Make transparent if the pixel is white (or light enough)\n",
    "            if rgb[0] >= min and rgb[1] >= min and rgb[2] >= min:\n",
    "                alpha_channel[y,x] = 0\n",
    "            # If pixel is not white then we've hit the sign so break out of loop\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Then loop backwards, right to left\n",
    "        for x in range(width-1, -1, -1):\n",
    "            rgb = pix[x,y]\n",
    "            if rgb[0] >= min and rgb[1] >= min and rgb[2] >= min:\n",
    "                alpha_channel[y,x] = 0\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return alpha_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Alexandros Stergiou's \"manipulate_images()\"\n",
    "# Delete the white background from the original sign\n",
    "def delete_background(image_path, output_dir):\n",
    "    # Open the image using PIL (don't read contents yet)\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')  # Does this have any effect?\n",
    "\n",
    "    # Open the image again using OpenCV and split into its channels\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    channels = cv2.split(image)\n",
    "\n",
    "    # Create a fully opaque alpha channel, same dimentions and dtype as the image\n",
    "    # create_alpha() modifies it to make the white background transparent\n",
    "    alpha_channel = np.ones(channels[0].shape, dtype=channels[0].dtype) * 255\n",
    "    alpha_channel = create_alpha(img, alpha_channel)\n",
    "\n",
    "    # Merge alpha channel into original image\n",
    "    image_RGBA = cv2.merge((channels[0], channels[1], channels[2], alpha_channel))\n",
    "\n",
    "    _,filename = ntpath.split(image_path)\n",
    "    # Remove the extension and save as a png\n",
    "    name,_ = filename.rsplit('.', 1)\n",
    "    save_path = os.path.join(output_dir, name) + \".png\"\n",
    "    cv2.imwrite(save_path, image_RGBA)\n",
    "\n",
    "    img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make white backgrounds transparent\n",
    "input_path = \"Original Sets/Images/\"\n",
    "output_path = \"Traffic_Signs_Templates/2_Processed_Images/\"\n",
    "# Create the output directory if it doesn't exist already\n",
    "if (not os.path.exists(output_path)):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "paths = load_files(input_path)\n",
    "for path in paths:\n",
    "    delete_background(path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the first image if it is larger than the second image\n",
    "'''\n",
    "def resize(img1, img2):\n",
    "    # Retrieve dimentions of both images\n",
    "    height1, width1, _ = img1.shape  # Discard channel\n",
    "    height2, width2, _ = img2.shape\n",
    "\n",
    "    # Resize if foreground is taller than background\n",
    "    if height1 > height2:\n",
    "        img1 = match_height(img1, height2)\n",
    "        height1, width1, _ = img1.shape  # Re-retrieve dimentions\n",
    "    # Or wider\n",
    "    if width1 > width2:\n",
    "        img1 = match_width(img1, width2)\n",
    "\n",
    "    return img1, img2\n",
    "\n",
    "\n",
    "def match_height(img, new_height):\n",
    "    old_height, old_width, _ = img.shape  # Discard channel\n",
    "    new_width = int(round( new_height / old_height * old_width ))\n",
    "    img = cv2.resize(img, (new_height, new_width))\n",
    "    return img\n",
    "\n",
    "\n",
    "def match_width(img, new_width):\n",
    "    old_width, old_height, _ = img.shape\n",
    "    new_height = int(round( new_width / old_width * old_height ))\n",
    "    img = cv2.resize(img, (new_width, new_height))\n",
    "    return img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlays foreground image on background image, keeping transparency\n",
    "# x1, y1: top-left coordinate to place foreground\n",
    "# Image overlay code credit to fireant:\n",
    "# https://stackoverflow.com/questions/14063070/overlay-a-smaller-image-on-a-larger-image-python-opencv\n",
    "def overlay(fg, bg, x1=-1, y1=-1):\n",
    "    # If the background doesn't have an alpha channel, add one, but keep the entire image opaque\n",
    "    if len(cv2.split(bg)) == 3:\n",
    "        bg = cv2.cvtColor(bg, cv2.COLOR_RGB2RGBA)\n",
    "        bg[:, :, 3] = 255\n",
    "    # Make a copy of the background for making changes\n",
    "    new_img = bg.copy()\n",
    "\n",
    "    # Retrieve image dimentions\n",
    "    height_FG, width_FG, _ = fg.shape  # Discard channel\n",
    "    height_BG, width_BG, _ = bg.shape\n",
    "\n",
    "    # If either of the coordinates were omitted, calculate start/end positions\n",
    "    # using the difference in image size, centring foreground on background\n",
    "    if x1 == -1 or y1 == -1:\n",
    "        # Start coordinates \n",
    "        x1 = (width_BG - width_FG) // 2    # Floor division to truncate as\n",
    "        y1 = (height_BG - height_FG) // 2  # coordinates don't need to be exact\n",
    "    # End coordinates\n",
    "    x2 = x1 + width_FG\n",
    "    y2 = y1 + height_FG\n",
    "\n",
    "    ### Start of code from fireant ###\n",
    "    # Retrieve an array of alpha values from the foreground image\n",
    "    # Divide by 255 to get values between 0.0 and 1.0\n",
    "    alpha = fg[:,:,3] / 255.0\n",
    "    beta = 1.0 - alpha\n",
    "\n",
    "    # Loop over BGR channels (but not alpha)\n",
    "    for ch in range(0, 3):\n",
    "        new_img[y1:y2, x1:x2, ch] = (alpha * fg[:, :, ch] +\n",
    "                                     beta * new_img[y1:y2, x1:x2, ch])\n",
    "    ### End of code from fireant ###\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of obscurity of graffiti\n",
    "# calculates the ratio of opacity in first image compared to second image\n",
    "def calc_ratio(fg, bg):\n",
    "    fgPixels = count_pixels(fg)  # number of non-transparent pixels in graffiti\n",
    "    bgPixels = count_pixels(bg)  # total number of pixels, dependent on sign shape\n",
    "    ratio = fgPixels / bgPixels\n",
    "\n",
    "    return ratio\n",
    "\n",
    "\n",
    "# Return the number of non-transparent pixels in the imported image\n",
    "def count_pixels(img):\n",
    "    sum = 0  # Initialise to 0 in case there is no alpha channel\n",
    "    split = cv2.split(img)\n",
    "    if len(split) is 4:  # Only proceed if the image has an alpha channel\n",
    "        alpha = split[3]\n",
    "        # Loop through alpha channel\n",
    "        # Divide alpha value by 255 to weight the transparency\n",
    "        for ii in range(0, len(alpha)):\n",
    "            for jj in range(0, len(alpha[0])):\n",
    "                sum += alpha[ii][jj] / 255\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw some text to represent graffiti\n",
    "# Returns the image and the ratio of oscurity\n",
    "def graffiti_writing(img):\n",
    "    # Create a new image the same dimentions as the image but completely transparent\n",
    "    height,width,_ = img.shape\n",
    "    grft = np.zeros((height, width, 4), dtype=np.uint8)\n",
    "    # Retrieve the original alpha data to reapply later\n",
    "    alphaData = cv2.split(img)[3]\n",
    "\n",
    "    # Draw some graffiti onto the transparent image\n",
    "    xx,yy = int(width * 0.1), int(height * 0.5)\n",
    "    grft = write(grft, \"GRFT\", xx, yy, scale=3, color_bgr=(25,25,25))\n",
    "    xx,yy = int(width * 0.3), int(height * 0.8)\n",
    "    grft = write(grft, \"DAMAGE\", xx, yy, scale=2, color_bgr=(230,170,0))\n",
    "    # Apply a Gaussian blur to smooth the edges\n",
    "    k = (int(round( width/40 )) // 2) * 2 + 1  # Kernel size must be odd\n",
    "    grft = cv2.GaussianBlur(grft, (k,k), 0)\n",
    "\n",
    "    # Use the two separate images to calculate the ratio of obscurity\n",
    "    temp = calc_ratio(grft, img)\n",
    "    ratio = \"{:0.3f}\".format( temp )\n",
    "\n",
    "    # Overlay the graffiti onto the sign\n",
    "    img = overlay(grft, img)\n",
    "    # Reapply the alpha data to cover up any graffiti that spilled over the sign\n",
    "    img[:,:,3] = alphaData\n",
    "    \n",
    "    return img, ratio\n",
    "\n",
    "\n",
    "def write(img, text, xx, yy, scale, color_bgr):\n",
    "    fontFace = cv2.FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    "    _,width,_ = img.shape\n",
    "    fontScale = width / 400 * scale\n",
    "    thickness = int(round( width / 120 * scale ))\n",
    "\n",
    "    # Put one letter at a time, overlapping the letters slightly\n",
    "    (wd,ht),_ = cv2.getTextSize(text, fontFace, fontScale, thickness)\n",
    "    for letter in text:\n",
    "        # Gnerate some randomness in transparency\n",
    "        alpha = random.randint(240,255)\n",
    "        color = color_bgr + (alpha,)\n",
    "        # Use a random number to generate some range in letter size\n",
    "        rand = random.uniform(0.7, 1.3)\n",
    "        origin = ( xx, yy - int(round(ht*(1-rand)/2)) )  # Centre vertical position\n",
    "        cv2.putText(img, letter, origin, fontFace, fontScale*rand, color, thickness, cv2.LINE_AA)\n",
    "        # Calculate the horizontal starting position for the next letter, based on this character's size\n",
    "        offset = int(round( wd / len(text) * 0.6 * rand ))  # 60% character width\n",
    "        xx = xx + offset\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obscure a sign with scribbled graffiti\n",
    "# Returns the image and the ratio of oscurity\n",
    "def graffiti_scribble(img):\n",
    "    # Create a new image the same dimentions as the image but completely transparent\n",
    "    height,width,_ = img.shape\n",
    "    blank = np.zeros((height, width, 4), dtype=np.uint8)\n",
    "    # Retrieve the original alpha data to reapply after overlaying\n",
    "    alphaData = cv2.split(img)[3]\n",
    "\n",
    "    dmg = {}  # Dictionary to map the images to their obscurity percentages\n",
    "\n",
    "    # Turning point coordinates, for a 1x1 image\n",
    "    pts1 = ( (0.28,0.12), (0.10,0.44), (0.46,0.12), (0.14,0.68), (0.70,0.16),\n",
    "             (0.30,0.84), (0.86,0.32), (0.54,0.88), (0.90,0.56), (0.72,0.88) )\n",
    "    # Create a horizontal reflection of the points\n",
    "    pts2 = ()\n",
    "    for ii in range(len(pts1)):\n",
    "        pts2 = pts2 + ( (1.0 - pts1[ii][0], pts1[ii][1]), )\n",
    "\n",
    "    # Scribble in 4 different colours\n",
    "    colors = ( (240,240,240), (230,170,0), (140,70,30), (0,0,0) )\n",
    "    # Kernel size for Gaussian blur, which must be odd\n",
    "    k = (int(round( width/15 )) // 2) * 2 + 1\n",
    "\n",
    "    for color in colors:\n",
    "        grft = blank.copy()\n",
    "        grft = draw_lines(grft, pts1, color)\n",
    "        # Apply a Gaussian blur to smooth the edges\n",
    "        grft = cv2.GaussianBlur(grft, (k,k), 0)\n",
    "\n",
    "        temp = overlay(grft, img)\n",
    "        # Reapply the alpha data to cover up any graffiti that spilled over the sign\n",
    "        temp[:,:,3] = alphaData\n",
    "\n",
    "        ratio = \"{:0.3f}\".format( calc_ratio(grft, img) )\n",
    "        dmg[ratio] = temp  # Add to dictionary\n",
    "\n",
    "    # Overlay 2 scribbles to create a more obscured sign\n",
    "    # Left facing\n",
    "    grft1 = blank.copy()\n",
    "    grft1 = draw_lines(grft1, pts1, colors[1])\n",
    "    grft1 = cv2.GaussianBlur(grft1, (k,k), 0)\n",
    "    # Right facing\n",
    "    grft2 = blank.copy()\n",
    "    grft2 = draw_lines(grft2, pts2, colors[2])\n",
    "    grft2 = cv2.GaussianBlur(grft2, (k,k), 0)\n",
    "    # Merge the 2 scribbles\n",
    "    # Modify grft2's alpha channel to the union of both scribbles before overlaying \n",
    "    alpha_combined = cv2.bitwise_or(cv2.split(grft1)[3], cv2.split(grft2)[3])\n",
    "    grft2[:,:,3] = alpha_combined\n",
    "    grft = overlay(grft1, grft2)\n",
    "\n",
    "    temp = overlay(grft, img)  # Overlay with the sign\n",
    "    temp[:,:,3] = alphaData  # Reapply alpha data\n",
    "\n",
    "    ratio = \"{:0.3f}\".format( calc_ratio(grft, img) )\n",
    "    dmg[ratio] = temp  # Add to dictionary\n",
    "\n",
    "    return dmg\n",
    "\n",
    "\n",
    "def draw_lines(img, pts, color_bgr):\n",
    "    height,width,_ = img.shape\n",
    "    for ii in range(len(pts)-1):\n",
    "        # Generate some randomness in line thickness and colour transparency\n",
    "        thickness = int(round( random.uniform(0.8, 1.2) * width/15 ))\n",
    "        alpha = random.randint(245, 255)\n",
    "        color = color_bgr + (alpha,)\n",
    "        # Multiply by width or height (and round) to get coordinates for drawing the lines\n",
    "        pt1 = ( int(round(width * pts[ ii ][0])), int(round(height * pts[ ii ][1])) )\n",
    "        pt2 = ( int(round(width * pts[ii+1][0])), int(round(height * pts[ii+1][1])) )\n",
    "        cv2.line(img, pt1, pt2, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply perspective warp to tilt images and combine to produce bent signs\n",
    "# Returns a dictionary with the bent signs mapped to the filenames to save as\n",
    "def bend_vertical(img_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    ht,wd,_ = img.shape  # Retrieve image dimentions\n",
    "\n",
    "    dmg = {}  # Dictionary to map the images to filenames\n",
    "\n",
    "    # TILT 1\n",
    "    pt = wd // 24\n",
    "    xx = pt * 3\n",
    "    yy = pt // 2\n",
    "    # Right             Top-left Top-middle  Bottom-left Bottom-middle\n",
    "    src = np.float32( [ [pt,pt], [wd//2,pt], [pt,ht-pt], [wd//2,ht-pt] ] )\n",
    "    dst = np.float32( [ [xx,yy], [wd//2,pt], [xx,ht-yy], [wd//2,ht-pt] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    right = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    # Left              Top-middle  Top-right   Bottom-middle  Bottom-right\n",
    "    src = np.float32( [ [wd//2,pt], [wd-pt,pt], [wd//2,ht-pt], [wd-pt,ht-pt] ] )\n",
    "    dst = np.float32( [ [wd//2,pt], [wd-xx,yy], [wd//2,ht-pt], [wd-xx,ht-yy] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    left = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    \n",
    "    # Combine the left and right tilt with the original forward-facing image\n",
    "    dmg[\"0_40\"] = combine(img, right)\n",
    "    dmg[\"40_0\"]  = combine(left, img)\n",
    "    # Combine the left and right tilt\n",
    "    dmg[\"40_40\"] = combine(left, right)\n",
    "\n",
    "    # TILT 2\n",
    "    pt = wd // 12\n",
    "    xx = pt * 3\n",
    "    yy = pt // 2\n",
    "    # Right             Top-left Top-middle  Bottom-left Bottom-middle\n",
    "    src = np.float32( [ [pt,pt], [wd//2,pt], [pt,ht-pt], [wd//2,ht-pt] ] )\n",
    "    dst = np.float32( [ [xx,yy], [wd//2,pt], [xx,ht-yy], [wd//2,ht-pt] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    right = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    # Left              Top-middle  Top-right   Bottom-middle  Bottom-right\n",
    "    src = np.float32( [ [wd//2,pt], [wd-pt,pt], [wd//2,ht-pt], [wd-pt,ht-pt] ] )\n",
    "    dst = np.float32( [ [wd//2,pt], [wd-xx,yy], [wd//2,ht-pt], [wd-xx,ht-yy] ] )\n",
    "    matrix = cv2.getPerspectiveTransform(src, dst)\n",
    "    left = cv2.warpPerspective(img, matrix, (wd,ht))\n",
    "    \n",
    "    # Combine the left and right tilt with the original forward-facing image\n",
    "    dmg[\"0_60\"] = combine(img, right)\n",
    "    dmg[\"60_0\"]  = combine(left, img)\n",
    "    # Combine the left and right tilt\n",
    "    dmg[\"60_60\"] = combine(left, right)\n",
    "\n",
    "    return dmg\n",
    "\n",
    "\n",
    "# Combines the left half of img1 with right half of img2 and returns the result\n",
    "def combine(img1, img2):\n",
    "    _,wd,_ = img1.shape\n",
    "    result = img1.copy()\n",
    "\n",
    "    # Save the alpha data (to replace later), as convertScaleAbs() will affect the transparency\n",
    "    alphaData = cv2.split(result)[3]\n",
    "\n",
    "    # Darken the entire image of the copy\n",
    "    alpha = 1   # No change to contrast\n",
    "    beta = -20  # Decrease brightness\n",
    "    cv2.convertScaleAbs(result, result, alpha, beta)\n",
    "    # Replace the alpha data\n",
    "    result[:,:,3] = alphaData\n",
    "\n",
    "    # Copy over the right half of img2 onto the darkened image\n",
    "    result[:,wd//2:wd] = img2[:,wd//2:wd]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for SGTSD\n",
    "damage_types = [\"_ORIGINAL\", \"_QUADRANT_4\", \"_BOTTOM_HOLE\", \"_BULLET\", \"_YELLOW\", \\\n",
    "                \"_GRAFFITI\", \"_BEND\", \"_GREY\"]\n",
    "\n",
    "def damage_images(image_path, output_dir):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    img = img.astype('uint8')\n",
    "    height,width,ch = img.shape\n",
    "    centre_x = int(width/2)\n",
    "    centre_y = int(height/2)\n",
    "    \n",
    "    # Create a directory for this sign number\n",
    "    _,tail = ntpath.split(image_path)  # Retrieve the filename of the image\n",
    "    title,_ = tail.split('.')          # Discard extension\n",
    "    output_path = os.path.join(output_dir, title)\n",
    "    # Create the output directory if it doesn't exit already\n",
    "    if (not os.path.exists(output_path)):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Create a mask\n",
    "    sign = img.copy() \n",
    "    alpha_ch = sign[:, :, 3] # Extract the alpha channel from sign\n",
    "    _,mask = cv2.threshold(alpha_ch, 5, 255, cv2.THRESH_BINARY) # Remove gradual transparency\n",
    "\n",
    "    \n",
    "    # ORIGINAL UNDAMAGED\n",
    "    cv2.imwrite(os.path.join(output_path, title + \".png\"), img)\n",
    "    \n",
    "\n",
    "    # TOP-LEFT QUADRANT\n",
    "    top_left = mask.copy()\n",
    "    cv2.rectangle(top_left, (0,0), (centre_x,centre_y), (0,0,0), -1)\n",
    "    dmg1 = cv2.bitwise_and(img, img, mask=top_left)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(output_path, title + damage_types[1] + \".png\"), dmg1)\n",
    "    \n",
    "\n",
    "    # BOTTOM HOLE\n",
    "    bottom_hole = mask.copy()\n",
    "    cv2.circle(bottom_hole, (centre_x,height), int(height/2), (0,0,0), -1)\n",
    "    dmg2 = cv2.bitwise_and(img, img, mask=bottom_hole)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(output_path, title + damage_types[2] + \".png\"), dmg2)\n",
    "    \n",
    "\n",
    "    # RANDOMISED BULLET HOLES\n",
    "    bullet_holes = mask.copy()\n",
    "    painted = img.copy() # Another copy of original sign to draw 'flaking paint' from holes on\n",
    "    numHoles = random.randint(7, 30)\n",
    "    for x in range(numHoles):\n",
    "        size = random.randint(2, 12) # Random hole size\n",
    "        h_x = random.randint(0, width) # Random hole position\n",
    "        h_y = random.randint(0, height)\n",
    "        c = random.randint(0, 150) # How black/grey the 'hole' is if it didn't penetrate\n",
    "        s = random.uniform(1.6, 2.2) # Random annulus size\n",
    "\n",
    "        C = 200 # Colour of damaged 'paint' outer annulus\n",
    "        cv2.circle(painted, (h_x, h_y), int(size * s), (C,C,C,255), -1) # Hole annulus to represent damaged 'paint'\n",
    "        # Did the bullet penetrate through the sign?\n",
    "        if (size < 6):\n",
    "            cv2.circle(painted, (h_x, h_y), size, (c,c,c,255), -1)\n",
    "        # If not, grey out rather than make transparent\n",
    "        else:\n",
    "            cv2.circle(bullet_holes, (h_x, h_y), size, (0,0,0), -1)\n",
    "    dmg3 = cv2.bitwise_and(painted, painted, mask=bullet_holes)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(output_path, title + damage_types[3] + \".png\"), dmg3)\n",
    "    \n",
    "\n",
    "    # TINTED YELLOW\n",
    "    yellow = np.zeros((height,width,ch), np.uint8)\n",
    "    yellow[:,:] = (0,210,210,255)\n",
    "    dmg4 = cv2.bitwise_and(img, yellow)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(output_path, title + damage_types[4] + \".png\"), dmg4)\n",
    "    \n",
    "\n",
    "    # GRAFFITI\n",
    "    dmg5,ratio = graffiti_writing(img)\n",
    "    cv2.imwrite(os.path.join(output_path, title + damage_types[5] + \"_\" + ratio + \".png\"), dmg5)\n",
    "    \n",
    "    dmg5 = graffiti_scribble(img)\n",
    "    for ratio,dmg in dmg5.items():\n",
    "        cv2.imwrite(os.path.join(output_path, title + damage_types[5] + \"_\" + ratio + \".png\"), dmg)\n",
    "    \n",
    "\n",
    "    # BEND\n",
    "    dmg6 = bend_vertical(image_path)\n",
    "    for detail,dmg in dmg6.items():\n",
    "        cv2.imwrite(os.path.join(output_path, title + damage_types[6] + \"_\" + detail + \".png\"), dmg)\n",
    "    \n",
    "\n",
    "    # GREY\n",
    "    dmg7 = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)\n",
    "    _,dmg7 = cv2.threshold(dmg7, 200, 255, cv2.THRESH_BINARY)\n",
    "    cv2.convertScaleAbs(dmg7, dmg7, alpha=1, beta=200)\n",
    "    dmg7 = cv2.cvtColor(dmg7, cv2.COLOR_GRAY2BGRA)\n",
    "    dmg7[:,:,3] = alpha_ch\n",
    "    cv2.imwrite(os.path.join(output_path, title + damage_types[7] + \".png\"), dmg7)\n",
    "\n",
    "    # CRACKS (thin crack lines across the sign?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create damaged sign templates based on images with backgrounds removed\n",
    "input_path = \"Traffic_Signs_Templates/2_Processed_Images/\"\n",
    "output_dir = \"Traffic_Signs_Templates/3_Damaged_Images/\"\n",
    "\n",
    "for image_path in load_files(input_path):\n",
    "    damage_images(image_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a pole to the imported sign\n",
    "def add_pole(filename, color):\n",
    "    img = cv.imread(filename, cv.IMREAD_UNCHANGED)\n",
    "    # Retrieve image dimentions and calculate the new height\n",
    "    height, width, _ = img.shape  # Discard channel\n",
    "    new_height = height * 3\n",
    "\n",
    "    # Create a new blank image with dtype=uint8 (to hold numbers 0-255)\n",
    "    # Initialise values to 0 so that the extended image is fully transparent\n",
    "    newImage = np.zeros((new_height, width, 4), dtype=np.uint8)\n",
    "    # Copy over the original image onto the top portion\n",
    "    newImage[0:height, :] = img\n",
    "\n",
    "    # Calculate coordinates\n",
    "    midpt = width // 2\n",
    "    offset = width // 40\n",
    "    x1, x2 = midpt - offset, midpt + offset\n",
    "    y1, y2 = height, new_height\n",
    "    # Draw the pole\n",
    "    cv.rectangle(newImage, (x1, y1), (x2, y2), color, cv.FILLED)\n",
    "\n",
    "    # Colour any transparent pixels between the sign and the rectangle\n",
    "    lim = 50  # Max alpha value for the pixel to be considered \"transparent\"\n",
    "    margin = int(round( height * 0.9 ))  # Loop over bottom 10% of the sign\n",
    "    for y in range(margin, height):\n",
    "        for x in range(x1, x2+1):  # Loop to x2 inclusive\n",
    "            if newImage[y,x,3] < lim:\n",
    "                newImage[y,x] = color\n",
    "\n",
    "    return newImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a pole and place on cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(image_path, output_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    height,width,_ = img.shape\n",
    "\n",
    "    dst = []\n",
    "    #0 FORWARD FACING\n",
    "    dst.append( img )\n",
    "\n",
    "    #1 EAST FACING\n",
    "    pts1 = np.float32( [[width/10,height/10], [width/2,height/10], [width/10,height/2]] )\n",
    "    pts2 = np.float32( [[width/5,height/5], [width/2,height/8], [width/5,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts1,pts2)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #2 NORTH-WEST FACING\n",
    "    pts3 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts4 = np.float32( [[width*9/10,height/5], [width/2,height/8], [width*9/10,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts3,pts4)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #3 LEFT TILTED FORWARD FACING\n",
    "    pts5 = np.float32( [[width/10,height/10], [width/2,height/10], [width/10,height/2]] )\n",
    "    pts6 = np.float32( [[width/12,height/6], [width/2.1,height/8], [width/10,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts5,pts6)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #4 RIGHT TILTED FORWARD FACING\n",
    "    pts7 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts8 = np.float32( [[width*10/12,height/6], [width/2.2,height/8], [width*8.4/10,height/1.8]] )\n",
    "    M = cv2.getAffineTransform(pts7,pts8)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #5 WEST FACING\n",
    "    pts9  = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts10 = np.float32( [[width/9.95,height/10], [width/2.05,height/9.95], [width*9/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts9,pts10)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #6 RIGHT TILTED FORWARD FACING\n",
    "    pts11 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts12 = np.float32( [[width*9/10,height/10], [width/2,height/9], [width*8.95/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts11,pts12)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #7 FORWARD FACING W/ DISTORTION\n",
    "    pts13 = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts14 = np.float32( [[width/9.8,height/9.8], [width/2,height/9.8], [width*8.8/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts13,pts14)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #8 FORWARD FACING W/ DISTORTION 2\n",
    "    pts15 = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts16 = np.float32( [[width/11,height/10], [width/2.1,height/10], [width*8.5/10,height/1.95]] )\n",
    "    M = cv2.getAffineTransform(pts15,pts16)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #9 FORWARD FACING W/ DISTORTION 3\n",
    "    pts17 = np.float32( [[width/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts18 = np.float32( [[width/11,height/11], [width/2.1,height/10], [width*10/11,height/1.95]] )\n",
    "    M = cv2.getAffineTransform(pts17,pts18)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #10 FORWARD FACING W/ DISTORTION 4\n",
    "    pts19 = np.float32( [[width*9.5/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts20 = np.float32( [[width*9.35/10,height/9.99], [width/2.05,height/9.95], [width*9.05/10,height/2.03]] )\n",
    "    M = cv2.getAffineTransform(pts19,pts20)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #11 FORWARD FACING W/ DISTORTION 5\n",
    "    pts21 = np.float32( [[width*9.5/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts22 = np.float32( [[width*9.65/10,height/9.95], [width/1.95,height/9.95], [width*9.1/10,height/2.02]] )\n",
    "    M = cv2.getAffineTransform(pts21,pts22)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #12 FORWARD FACING W/ DISTORTION 6\n",
    "    pts23 = np.float32( [[width*9.25/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts24 = np.float32( [[width*9.55/10,height/9.85], [width/1.9,height/10], [width*9.3/10,height/2.04]] )\n",
    "    M = cv2.getAffineTransform(pts23,pts24)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #13 SHRINK 1\n",
    "    pts25 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts26 = np.float32( [[width*8/10,height/10], [width*1.34/3,height/10.5], [width*8.24/10,height/2.5]] )\n",
    "    M = cv2.getAffineTransform(pts25,pts26)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #14 SHRINK 2\n",
    "    pts27 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts28 = np.float32( [[width*8.5/10,height*3.1/10], [width/2,height*3/10], [width*8.44/10,height*1.55/2.5]] )\n",
    "    M = cv2.getAffineTransform(pts27,pts28)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #15 FORWARD FACING W/ DISTORTION 7\n",
    "    pts29 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts30 = np.float32( [[width*8.85/10,height/9.3], [width/1.9,height/10.5], [width*8.8/10,height/2.11]] )\n",
    "    M = cv2.getAffineTransform(pts29,pts30)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #16 FORWARD FACING W/ DISTORTION 8\n",
    "    pts31 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts32 = np.float32( [[width*8.75/10,height/9.1], [width/1.95,height/8], [width*8.5/10,height/2.05]] )\n",
    "    M = cv2.getAffineTransform(pts31,pts32)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #17 FORWARD FACING W/ DISTORTION 9\n",
    "    pts33 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts34 = np.float32( [[width*8.75/10,height/9.1], [width/1.95,height/9], [width*8.5/10,height/2.2]] )\n",
    "    M = cv2.getAffineTransform(pts33,pts34)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #18 FORWARD FACING W/ DISTORTION 10\n",
    "    pts35 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts36 = np.float32( [[width*8.75/10,height/8], [width/1.95,height/8], [width*8.75/10,height/2]] )\n",
    "    M = cv2.getAffineTransform(pts35,pts36)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "    \n",
    "    #19 FORWARD FACING W/ DISTORTION 11\n",
    "    pts37 = np.float32( [[width*9/10,height/10], [width/2,height/10], [width*9/10,height/2]] )\n",
    "    pts38 = np.float32( [[width*8.8/10,height/7], [width/1.95,height/7], [width*8.8/10,height/2]] )\n",
    "    M = cv2.getAffineTransform(pts37,pts38)\n",
    "    dst.append( cv2.warpAffine(img,M,(width,height)) )\n",
    "\n",
    "    # Retrieve the filename to save as\n",
    "    _,tail = ntpath.split(image_path)  # Filename of the image, parent directories removed\n",
    "    title,_ = tail.rsplit('.', 1)      # Discard extension\n",
    "    \n",
    "    # Save the transformed images\n",
    "    for ii in range(1, len(dst)):\n",
    "        save_path = os.path.join(output_path, title)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        cv2.imwrite(os.path.join(save_path, str(ii) + \".png\"), dst[ii])\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_path = \"Traffic_Signs_Templates/1_Images/\"\n",
    "#input_path = \"Traffic_Signs_Templates/2_Processed_Images/\"\n",
    "input_path = \"Traffic_Signs_Templates/3_Damaged_Images/\"\n",
    "output_dir = \"Traffic_Signs_Templates/4_Transformed_Images/\"\n",
    "\n",
    "# Loop through each folder in the input directory\n",
    "for folder_path in load_paths(input_path):\n",
    "    # Retrieve the sign number to create a directory\n",
    "    _,number = os.path.split(folder_path)\n",
    "    save_dir = os.path.join(output_dir, number)\n",
    "    \n",
    "    for image_path in load_files(folder_path):\n",
    "        img_transform(image_path, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_png(directory):\n",
    "    for files in load_paths(directory):\n",
    "        title,extension = files.split('.')\n",
    "        img = Image.open(files).convert('RGBA')\n",
    "        if (not extension == \"png\"):\n",
    "            os.remove(files)\n",
    "        img.save(title+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#to_png(\"Google_search_backgrounds/UK_urban\")\n",
    "to_png(\"Google_search_backgrounds/UK_rural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_exposure(paths, channels):\n",
    "    exposures = []\n",
    "    for image_path in paths:\n",
    "        img = Image.open(image_path)\n",
    "        im = Image.open(image_path).convert('LA')\n",
    "        \n",
    "        stat = ImageStat.Stat(im)\n",
    "        \n",
    "        #Average pixel brighness\n",
    "        avg = stat.mean[0]\n",
    "        \n",
    "        #RMS pixel brighness\n",
    "        rms = stat.rms[0]\n",
    "        \n",
    "        stat2 = ImageStat.Stat(img)\n",
    "        \n",
    "        #Consider the number of channels\n",
    "        #background may have RGB while traffic sign has RGBA\n",
    "        if (channels==3):\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "        else:\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b,a = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "\n",
    "    return exposures     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths,4)\n",
    "    signs_exposures = find_image_exposure(signs_paths,4)\n",
    "    \n",
    "    for i in range(0,len(background_paths)):\n",
    "        print(\"Processed: \" + str(float(i) / float(len(background_paths)) * 100) + \" %\")\n",
    "        \n",
    "        img = Image.open(background_exposures[i][0])\n",
    "\n",
    "        for sign_path in signs_paths:        \n",
    "            dirc,sub,el = background_exposures[i][0].split('/')\n",
    "            title,extension = el.split('.')\n",
    "\n",
    "            parent_dir,sub_dir,folder,folder2,element = sign_path.split('/')\n",
    "            head,tail = element.split('.')\n",
    "\n",
    "            \n",
    "            ###   ORIGINAL EXPOSURE IMPLEMENTATION   ###\n",
    "            brightness_avrg = 1.0\n",
    "            brightness_rms = 1.0\n",
    "            brightness_avrg_perceived = 1.0\n",
    "            brightness_rms_perceived = 1.0\n",
    "            brightness_avrg2 = 1.0\n",
    "            brightness_rms2 = 1.0\n",
    "\n",
    "            # abs(desired_brightness - actual_brightness) / abs(brightness_float_value) = ratio\n",
    "            avrg_ratio = 11.0159464507\n",
    "\n",
    "            rms_ratio = 8.30320014372\n",
    "\n",
    "            percieved_avrg_ratio = 3.85546373056\n",
    "\n",
    "            percieved_rms_ratio = 35.6344530649\n",
    "\n",
    "            avrg2_ratio = 1.20354549572\n",
    "\n",
    "            rms2_ratio = 40.1209106864\n",
    "\n",
    "            peak = Image.open(sign_path).convert('LA')\n",
    "            peak2 = Image.open(sign_path).convert('RGBA')\n",
    "\n",
    "            stat = ImageStat.Stat(peak)\n",
    "            avrg = stat.mean[0]\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            #IMAGE MANIPULATION MAIN CODE STARTS\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(avrg-float(background_exposures[i][1]))\n",
    "            \n",
    "            brightness_avrg = margin/avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright = enhancer.enhance(brightness_avrg)\n",
    "            stat = ImageStat.Stat(avrg_bright)\n",
    "            avrg = stat.mean[0]\n",
    "\n",
    "            \n",
    "            #MINIMISE MARGIN BASED ON ROOT MEAN SQUARE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(rms-float(background_exposures[i][2]))\n",
    "\n",
    "            brightness_rms = margin/rms_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright = enhancer.enhance(brightness_rms)\n",
    "            stat = ImageStat.Stat(rms_bright)\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            stat2 = ImageStat.Stat(peak2)\n",
    "            r,g,b,a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            margin = abs(avrg_perceived-float(background_exposures[i][3]))\n",
    "            \n",
    "            brightness_avrg_perceived = margin/percieved_avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright_perceived = enhancer.enhance(brightness_avrg_perceived)\n",
    "            stat2 = ImageStat.Stat(avrg_bright_perceived)\n",
    "            r,g,b,a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))        \n",
    "\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON RMS FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            margin = abs(rms_perceived-float(background_exposures[i][4]))\n",
    "\n",
    "            brightness_rms_perceived = margin/percieved_rms_ratio \n",
    "\n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright_perceived = enhancer.enhance(brightness_rms_perceived)\n",
    "            stat2 = ImageStat.Stat(rms_bright_perceived)\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))        \n",
    "\n",
    "            \n",
    "            stat3 = ImageStat.Stat(peak2)\n",
    "            avrg2 = stat3.mean[0]\n",
    "            rms2 = stat3.rms[0]\n",
    "\n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO AVERAGING METHODS\n",
    "            margin = abs(avrg2-float(background_exposures[i][1]))\n",
    "            brightness_avrg2 = margin/avrg2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright2 = enhancer.enhance(brightness_avrg2)\n",
    "            stat3 = ImageStat.Stat(avrg_bright2)\n",
    "            avrg2 = stat3.mean[0]       \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO RMS METHODS\n",
    "            margin = abs(rms2-float(background_exposures[i][2]))\n",
    "            brightness_rms2 = margin/rms2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright2 = enhancer.enhance(brightness_rms2)\n",
    "            stat3 = ImageStat.Stat(rms_bright2)\n",
    "            rms2 = stat3.rms[0]\n",
    "            \"\"\"\n",
    "            \n",
    "            avrg_bright = avrg_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright = rms_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            avrg_bright_perceived = avrg_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright_perceived = rms_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            #avrg_bright2 = avrg_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "            #rms_bright2 = rms_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "\n",
    "            \n",
    "            exp_dir = \"Traffic_Signs_Exposure_Manipulation/\"\n",
    "            avrg_bright.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE.\"+tail)\n",
    "            rms_bright.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS.\"+tail)\n",
    "            avrg_bright_perceived.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE_PERCEIVED.\"+tail)\n",
    "            rms_bright_perceived.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS_PERCEIVED.\"+tail)\n",
    "            #avrg_bright2.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE2.\"+tail)\n",
    "            #rms_bright2.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS2.\"+tail)\n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths,4)\n",
    "    signs_exposures = find_image_exposure(signs_paths,4)\n",
    "    \n",
    "    print(\"Processed: 0.0 %\")\n",
    "    ii = 0\n",
    "    prev = 0\n",
    "    for sign_path in signs_paths:\n",
    "        progress = float(ii) / float(len(signs_paths)) * 100\n",
    "        if progress >= prev + 5: #Prevent spamming of progress prints\n",
    "            prev = prev + 5\n",
    "            print(\"Processed: \" + str(progress) + \" %\")\n",
    "\n",
    "        dirc,sub,el = background_exposures[0][0].split('/')\n",
    "        title,extension = el.split('.')\n",
    "\n",
    "        parent_dir,sub_dir,folder,folder2,element = sign_path.split('/')\n",
    "        head,tail = element.split('.')\n",
    "\n",
    "        img = cv2.imread(sign_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "        ###   GRADUAL FADE IMPLEMENTATION   ###\n",
    "        #Retrieve alpha data from original image\n",
    "        splitImg = cv2.split(img)\n",
    "        if len(splitImg) is 4:\n",
    "            alphaData = splitImg[3]\n",
    "\n",
    "        for jj in range(0,5):\n",
    "            dmg6 = img.copy()\n",
    "            alpha = 1 - (jj * 0.19)\n",
    "            beta = (jj + 1) * 40\n",
    "            if jj > 0:\n",
    "                cv2.convertScaleAbs(img, dmg6, alpha, beta) #Scale the contrast and brightness\n",
    "                dmg6[:, :, 3] = alphaData\n",
    "\n",
    "            dmg6 = cv2.resize(dmg6, (150,150))\n",
    "            fad_dir = \"Traffic_Signs_Fade_Manipulation/\"\n",
    "            cv2.imwrite(fad_dir+\"SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_FADE-\"+str(jj)+\".\"+tail, dmg6)\n",
    "        ii = ii + 1\n",
    "    \n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bg_dir = \"Google_search_backgrounds\"\n",
    "\n",
    "for dirs in load_paths(bg_dir):\n",
    "    initial, subd = dirs.split('/')\n",
    "\n",
    "    if original is True:\n",
    "        for background in load_paths(dirs):\n",
    "            initial, subd, element = background.split('/')\n",
    "            title, extension = element.split('.')\n",
    "\n",
    "            for signp in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "                for sign in load_paths(signp):\n",
    "                    d,s,f,e = sign.split('/') #Eg. s = 4_Transformed_Images, f = 9, e = 9_BOTTOM_HOLE\n",
    "\n",
    "                    exp_dir = \"Traffic_Signs_Exposure_Manipulation/\"\n",
    "                    if (not os.path.exists(exp_dir + subd + \"/\" + title + \"/SIGN_\" + f + \"/\" + e)):\n",
    "                        os.makedirs(exp_dir + subd + \"/\" + title + \"/SIGN_\" + f + \"/\" + e)\n",
    "    else:\n",
    "        for signp in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "            for sign in load_paths(signp):\n",
    "                d,s,f,e = sign.split('/')\n",
    "\n",
    "                fade_dir = \"Traffic_Signs_Fade_Manipulation/\"\n",
    "                if (not os.path.exists(fade_dir + \"SIGN_\" + f + \"/\" + e)):\n",
    "                    os.makedirs(fade_dir + \"SIGN_\" + f + \"/\" + e)\n",
    "\n",
    "signs_paths = []\n",
    "for p in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "    for d in load_paths(p):\n",
    "        signs_paths = signs_paths + load_paths(d)\n",
    "\n",
    "background_paths = load_paths(\"Google_search_backgrounds/UK_rural\")\n",
    "if original is True:\n",
    "    exposure_manipulation(signs_paths, background_paths)\n",
    "else:\n",
    "    fade_manipulation(signs_paths, background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#background_paths = load_paths(\"Google_search_backgrounds/UK_urban\") #Don't do, takes too long to do both\n",
    "#exposure_manipulation(signs_paths,background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avrg_pixel_rgb(image,chanels):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    if (chanels == 4):\n",
    "        r,g,b,a = stat.rms\n",
    "    else:\n",
    "        r,g,b = stat.rms\n",
    "    \n",
    "    return [r,g,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bw_images(directory):\n",
    "    images = []\n",
    "    for signs in load_paths(directory):\n",
    "        img = Image.open(signs).convert('RGBA')\n",
    "        rgb = avrg_pixel_rgb(img,4)\n",
    "        rg = abs(rgb[0]-rgb[1])\n",
    "        rb = abs(rgb[0]-rgb[2])\n",
    "        gb = abs(rgb[1]-rgb[2])\n",
    "        \n",
    "        temp = signs.split('/')\n",
    "        head,tail = temp[-1].split('.')\n",
    "                \n",
    "        if (rg<=1 and rb<=1 and gb<=1):\n",
    "            images.append(head)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_useful_signs(directory): #Removes bad signs, such as those which are all white or all black\n",
    "    bw_images = find_bw_images(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "    for background_dir in load_paths(directory):\n",
    "        for signs in load_paths(background_dir):\n",
    "            for dmgs in load_paths(signs):\n",
    "                temp = []\n",
    "                for imgs in load_paths(dmgs):\n",
    "                    temp.append(imgs)\n",
    "                exposures = find_image_exposure(temp,4)\n",
    "                \n",
    "                i = 0\n",
    "                for images in load_paths(dmgs):\n",
    "                    #Find brightness\n",
    "                    img = Image.open(images).convert('RGBA')\n",
    "\n",
    "                    rgb = avrg_pixel_rgb(img,4)\n",
    "                    rg = abs(rgb[0]-rgb[1])\n",
    "                    rb = abs(rgb[0]-rgb[2])\n",
    "                    gb = abs(rgb[1]-rgb[2])\n",
    "\n",
    "                    is_bw = False\n",
    "\n",
    "                    for s in bw_images:\n",
    "                        if s in exposures[i][0]:\n",
    "                            is_bw = True\n",
    "\n",
    "                    if (rg<=16 and rb<=16 and gb<=16):\n",
    "                        if (not is_bw):\n",
    "                            os.remove(images)\n",
    "                        #Threshold values for black and white images\n",
    "                        elif (rgb[0]<70 and rgb[1]<70 and rgb[2]<70):\n",
    "                            os.remove(images)\n",
    "                        elif (rgb[0]>155 and rgb[1]>155 and rgb[2]>155):\n",
    "                            os.remove(images)\n",
    "\n",
    "                    elif (not is_bw):\n",
    "                        #Delete light blue images\n",
    "                        if(rgb[2]>rgb[0] and rgb[2]>=rgb[1]):\n",
    "                            if (gb<=10):\n",
    "                                os.remove(images)\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directory = \"Traffic_Signs_Exposure_Manipulation/UK_rural\"\n",
    "    find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if original is True:\n",
    "#     directory= \"Traffic_Signs_Exposure_Manipulation/UK_urban\"\n",
    "#     find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_poisson_noise (image):\n",
    "    vals = len(np.unique(image))\n",
    "    vals = 2.05 ** np.ceil(np.log2(vals))\n",
    "    noisy = np.random.poisson(image * vals) / float(vals)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_Gaussian_noise (image):\n",
    "    row,col,ch= image.shape\n",
    "    mean = 0\n",
    "    var = 0.5\n",
    "    sigma = var**0.5\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_speckle_noise (image):\n",
    "    row,col,ch = image.shape\n",
    "    gauss = np.random.randn(row,col,ch)\n",
    "    gauss = gauss.reshape(row,col,ch)        \n",
    "    noisy = image + image * gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_method (image):\n",
    "    \"\"\"\n",
    "    i = random.randint(1, 3)\n",
    "    if (i == 1):\n",
    "        return insert_poisson_noise(image)\n",
    "    elif (i==2):\n",
    "        return insert_Gaussian_noise(image)\n",
    "    else:\n",
    "        return insert_speckle_noise(image)\n",
    "    \"\"\"\n",
    "    image.setflags(write=1)\n",
    "    #Add noise in every pixel w/ random probability 0.4\n",
    "    for im in image:\n",
    "        px = 0\n",
    "        for pixel in im:\n",
    "            apply_noise = random.randint(0,100)\n",
    "            #if random probability\n",
    "            if apply_noise > 40:\n",
    "                #RGB values\n",
    "                R = pixel[0]\n",
    "                G = pixel[1]\n",
    "                B = pixel[2]\n",
    "                A = pixel[3]\n",
    "                #find current relative lumination for brighness\n",
    "                #based on: https://en.wikipedia.org/wiki/Relative_luminance\n",
    "                relative_lumination = 0.2126*R + 0.7152*G + 0.0722*B\n",
    "                #find differences between RGB values     \n",
    "                R_to_G = float(R)/float(G)\n",
    "                RG = False\n",
    "                if (R_to_G >= 1): RG=True\n",
    "                R_to_B = float(R)/float(B)\n",
    "                RB = False\n",
    "                if (R_to_B >= 1): RB=True\n",
    "                G_to_B = float(G)/float(B)\n",
    "                GB = False\n",
    "                if (G_to_B >= 1): GB=True\n",
    "                equal = False\n",
    "                if (R==G==B):equal==True\n",
    "\n",
    "                #In order to determine the margin in which the new brighness\n",
    "                #should be within, the upper and lower limits need to be foun\n",
    "                #The Relative luminance in colorimetric spaces has normilised\n",
    "                #values between 0 and 255\n",
    "                upper_limit = 255\n",
    "                lower_limit = 0\n",
    "                if (relative_lumination + 40 < 255):\n",
    "                    upper_limit = relative_lumination + 40\n",
    "                if (relative_lumination - 40 > 0):\n",
    "                    lower_limit = relative_lumination - 40\n",
    "\n",
    "                #Compute new brighness value\n",
    "                new_lumination = random.randint(int(lower_limit),int(upper_limit))\n",
    "\n",
    "                #find the three possible solutions that satisfy\n",
    "                #->The new lumination chosen based on the Relative luminance equation\n",
    "                #->The precentages computed between every RGB value\n",
    "\n",
    "                solutions = []\n",
    "\n",
    "                for r in range(1,255):\n",
    "                    for g in range(1,255):\n",
    "                        for b in range(1,255):\n",
    "                            r_to_g = float(r)/float(g)\n",
    "                            rg = False\n",
    "                            if (r_to_g >= 1): rg=True\n",
    "                            r_to_b = float(r)/float(b)\n",
    "                            rb = False\n",
    "                            if (r_to_b >= 1): rb=True\n",
    "                            g_to_b = float(g)/float(b)\n",
    "                            gb = False\n",
    "                            if (g_to_b >= 1): gb=True\n",
    "                            e = False\n",
    "                            if(r==g==b):\n",
    "                                e=True\n",
    "                            if (0.2126*r + 0.7152*g + 0.0722*b == 100) and rg==RG and rb==RB and gb==GB and e==equal:\n",
    "                                solutions.append([r,g,b])\n",
    "\n",
    "                #Find the solution that precentage wise is closer to the original\n",
    "                #difference between the values\n",
    "                percentages = []\n",
    "\n",
    "                for solution in solutions:\n",
    "                    r = solution[0]\n",
    "                    g = solution[1]\n",
    "                    b = solution[2]\n",
    "                    percentages.append((float(r)/float(g))+(float(r)/float(b))+(float(g)/float(b)))\n",
    "\n",
    "                i = 0\n",
    "                pos = 0\n",
    "                best = percentages[0]\n",
    "                for p in percentages[1:]:\n",
    "                    if p < best:\n",
    "                        pos = i\n",
    "                    i = i +1\n",
    "\n",
    "                #Assign new pixel values\n",
    "                im[px] = [solutions[pos][0],solutions[pos][1],solutions[pos][2],A]\n",
    "            px = px+1\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORIGINAL ###\n",
    "# def new_data(image_dir,bg_dir): #Blends synthetic signs with backgrounds\n",
    "#     # Import background image\n",
    "#     background_img_raw = Image.open(bg_dir).convert('RGBA')  \n",
    "#     background_img_raw = background_img_raw.resize((150,150), Image.ANTIALIAS)\n",
    "#     background_img = np.array(background_img_raw)  \n",
    "#     background_img_float = background_img.astype(float)  \n",
    "\n",
    "#     # Import foreground image\n",
    "#     foreground_img_raw = Image.open(image_dir)  \n",
    "#     foreground_img = np.array(foreground_img_raw)  \n",
    "#     foreground_img_float = foreground_img.astype(float)  \n",
    "\n",
    "#     # Blend images\n",
    "#     opacity = 1  \n",
    "#     blended_img_float = blend_modes.grain_merge(background_img_float, foreground_img_float, opacity)\n",
    "\n",
    "#     # Convert blended image back into PIL image\n",
    "#     blended_img = np.uint8(blended_img_float)\n",
    "#     blended_img_raw = Image.fromarray(blended_img)  \n",
    "    \n",
    "#     foreground_img_raw = foreground_img_raw.resize((149,149), Image.ANTIALIAS)\n",
    "#     blended_img_raw.paste(foreground_img_raw, (0, 0), foreground_img_raw)\n",
    "#     blended_img_raw = blended_img_raw.resize((48,48), Image.ANTIALIAS)\n",
    "    \n",
    "#     #temp = np.uint8(blended_img_raw)\n",
    "#     #temp = random_noise_method(temp)\n",
    "    \n",
    "#     #blended_img_raw = Image.fromarray(np.uint8(temp)) \n",
    "    \n",
    "#     return blended_img_raw\n",
    "\n",
    "### FULL BACKGROUND ###\n",
    "def new_data(image_dir,bg_dir): #Blends synthetic signs with backgrounds\n",
    "    bg = cv2.imread(bg_dir, cv2.IMREAD_UNCHANGED)\n",
    "    fg = cv2.imread(image_dir, cv2.IMREAD_UNCHANGED)\n",
    "    return overlay(fg, bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the required directories in SGTSD\n",
    "directory = 'SGTSD/Images'\n",
    "if (not os.path.exists(\"SGTSD/Images\")):\n",
    "    #Numbered Version\n",
    "    for sign in load_paths(\"Traffic_Signs_Templates/1_Images\"): #Folders for each sign type\n",
    "        head, tail = sign.split('.')\n",
    "        name = head.split('/')\n",
    "        os.makedirs(\"SGTSD/Images/\" + name[-1])\n",
    "        j = 0\n",
    "        for dmg in range(len(damage_types)): #Folders for each damage type\n",
    "            os.makedirs(\"SGTSD/Images/\" + name[-1] + \"/\" + name[-1] + \"_\" + str(j))\n",
    "            j = j + 1\n",
    "    \n",
    "    #Named Version\n",
    "#     for sign in load_paths(\"Traffic_Signs_Templates/1_Images\"): #Folders for each sign type\n",
    "#         head, tail = sign.split('.')\n",
    "#         name = head.split('/')\n",
    "#         os.makedirs(\"SGTSD/Images/\" + name[-1])\n",
    "#         for dmg in load_paths(\"Traffic_Signs_Templates/3_Damaged_Images\"): #Folders for each damage type\n",
    "#             headD,tailD = dmg.split('.')\n",
    "#             nameD = headD.split('/')\n",
    "#             os.makedirs(\"SGTSD/Images/\" + name[-1] + \"/\" + nameD[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a README file\n",
    "content = '''\n",
    "-----------------------------------------------\n",
    "|                     -*-                     |\n",
    "|Synthetically Generated Traffic Sign Dataset |\n",
    "|                     -*-                     |\n",
    "-----------------------------------------------\n",
    "\n",
    "This directory contains the training set for\n",
    "The Convolutional Neural Network (CNN)\n",
    "Used in this project\n",
    "\n",
    "However, it can be used for any classifier\n",
    "desired by the person using the code and\n",
    "additionally, it is not limited to a specific\n",
    "traffic sign templates.\n",
    " \n",
    "\n",
    "----------------------------------------------\n",
    "Content\n",
    "----------------------------------------------\n",
    "\n",
    "The number of example is based on the number:\n",
    "->of traffic signs that were used as templates\n",
    "->of the image manipulation processes\n",
    "->of the brighness variations values used\n",
    "->of the blending procedures\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Image format and naming\n",
    "----------------------------------------------\n",
    "The images created are of \"jpg\" format\n",
    "with RGBA channels\n",
    "\n",
    "   SIGN_X/XXX_YYY.jpg\n",
    "\n",
    "The initial part (X) is used to distinguish the\n",
    "sign class, while the remaining (XXX_YYY) firstly\n",
    "indicated the sign in the file itself and the\n",
    "example number.\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Additional information\n",
    "----------------------------------------------\n",
    "\n",
    "contact email: \n",
    "    \n",
    "\tasterga@essex.ac.uk\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Alexandros Stergiou\n",
    "\"The Driver's Assistant\"\n",
    "\n",
    "University of Essex,\n",
    "School of Computer Science and\n",
    "Electronic Engineering,\n",
    "UK\n",
    "----------------------------------------------\n",
    "'''\n",
    "text_file = open(\"SGTSD/Readme_Images.txt\", \"w\")\n",
    "text_file.write(content)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using exposure_manipulation\n",
    "def create_paths_list(imgs_directory, bg_directory):\n",
    "    directories = []\n",
    "    for places in load_paths(imgs_directory): #List of places: either UK_rural or UK_urban\n",
    "        for imgs in load_paths(places): #Folder for each bg image: eg. IMG_0\n",
    "            dr = imgs.split('/')\n",
    "            bg = bg_directory + '/' + dr[-2] + '/' + dr[-1] + \".png\" #Retrieving relevant bg image\n",
    "            for signs in load_paths(imgs): #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 0_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, bg])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using fade_manipulation; backgrounds are assigned to \n",
    "def create_assigned_paths_list(imgs_directory, bg_directory): #TODO: is this the same as above?\n",
    "    directories = []\n",
    "    for places in load_paths(bg_directory): #Folder for each place: eg. UK_rural\n",
    "        for imgs in load_paths(places): #Iterate through each b.g. image: eg. IMG_0\n",
    "            for signs in load_paths(imgs_directory):  #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 9_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, imgs])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directories = create_paths_list(\"Traffic_Signs_Exposure_Manipulation\",\"Google_search_backgrounds\")\n",
    "else:\n",
    "    directories = create_assigned_paths_list(\"Traffic_Signs_Fade_Manipulation\",\"Google_search_backgrounds\")\n",
    "print(\"Files to be generated: \" + str(len(directories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths of images needed to generate examples for 'sign' with damage 'dmg'\n",
    "def list_for_sign_x(sign, dmg, directories):\n",
    "    l = []\n",
    "    for elements in directories:\n",
    "        foreground = elements[0].split('/')\n",
    "        if (foreground[-2] == sign + dmg): #Eg. if (9_YELLOW == 4_ORIGINAL)\n",
    "            l.append(elements)\n",
    "    return l #Directory for every single sign and it's relevant background image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_directories = [] #Reformat list to have each sign and damage as their own dimensions\n",
    "signs = load_paths('Traffic_Signs_Templates/1_Images')\n",
    "dmgs = load_paths('Traffic_Signs_Templates/3_Damaged_Images')\n",
    "for i in signs:\n",
    "    head, tail = ntpath.split(i)\n",
    "    sign, extension = tail.split('.') #Eg. sign == \"9\"\n",
    "\n",
    "    sign_list = [] #List of damages, which are each list of signs\n",
    "    for dmg in damage_types: #damage_types is from 'def damage_images:' cell\n",
    "        sign_list.append(list_for_sign_x(sign, dmg, directories))\n",
    "    final_directories.append(sign_list) #List of types -> lists of damages -> lists of signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This generates all of the combined sign + background images (>1,000,000 originally)\n",
    "direct = \"SGTSD/Images\"\n",
    "folders = load_paths(direct)\n",
    "n = [] #Numbers from the folder names for the signs\n",
    "for folder in folders:\n",
    "    head, tail = ntpath.split(folder)\n",
    "    n.append(tail)\n",
    "    \n",
    "i = 0\n",
    "for signs in final_directories: #Iterating through sign types\n",
    "    print(\"Processed: \" + str(float(i) / float(len(final_directories)) * 100) + \" %\")\n",
    "    j = 0\n",
    "    for damages in signs: #Iterating through damage types\n",
    "        k = 0\n",
    "        for dirs in damages: #dirs == the foreground and background images for one generated sign\n",
    "            image = new_data(dirs[0], dirs[1]) #Combining background with exposure modified sign foreground\n",
    "#             image.save(direct+\"/\"+n[i]+\"/\"+n[i]+\"_\"+str(j)+\"/\"+n[i]+\"_\"+str(j)+\"_\"+str(k)+\".png\")\n",
    "            cv2.imwrite(direct+\"/\"+n[i]+\"/\"+n[i]+\"_\"+str(j)+\"/\"+n[i]+\"_\"+str(j)+\"_\"+str(k)+\".png\", image)\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i+1\n",
    "print(\"Processed: \"+str(100)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "-------------------------------------\n",
    "BREAKDOWN OF FILES GENERATED BY CLASS\n",
    "-------------------------------------\n",
    "'''\n",
    "total = 0\n",
    "for i in range(len(final_directories)):\n",
    "    current = 0\n",
    "    for j in range(len(final_directories[i])):\n",
    "        current = current + len(final_directories[i][j])\n",
    "    s = \"Generated \" + str(current) + \" examples for sign class \" + str(i + 1)\n",
    "    string = string + '\\n' + s + '\\n'\n",
    "    total = total + current\n",
    "string = string + '\\n' + \"TOTAL: \" + str(total) + '\\n' + \"Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M\") + '\\n'\n",
    "string = string + \"-------------------------------------\"\n",
    "text_file = open(\"SGTSD/generated_images_about.txt\", \"w\")\n",
    "text_file.write(string)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_to_jpeg(filepath):\n",
    "    dirs = filepath.split('/')\n",
    "    title,extension = dirs[-1].split('.')\n",
    "    del dirs[-1]\n",
    "    string = '/'.join(dirs)\n",
    "    string = string + '/' + title + \".jpg\"\n",
    "    png = Image.open(filepath)\n",
    "    png.load() # required for png.split()\n",
    "    background = Image.new(\"RGB\", png.size, (255, 255, 255))\n",
    "    background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
    "    background.save(string, 'JPEG', quality=100)\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = direct = \"SGTSD/Images\"\n",
    "i = 1\n",
    "for path in load_paths(dirs):\n",
    "    print(\"Processed: \" + str(float(i - 1) / float(len(final_directories)) * 100) + \" %\")\n",
    "    for image in load_paths(path):\n",
    "        if (image.endswith(\"png\")):\n",
    "            png_to_jpeg(image)\n",
    "    i = i + 1\n",
    "print(\"Processed: \" + str(100) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Exposure_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Fade_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Templates/4_Transformed_Images\")\n",
    "shutil.rmtree(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "shutil.rmtree(\"Traffic_Signs_Templates/2_Processed_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"SGTSD\") #Be careful with this one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}