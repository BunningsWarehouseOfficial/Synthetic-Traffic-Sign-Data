{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import color, exposure, transform\n",
    "from PIL import Image, ImageChops, ImageDraw, ImageOps, ImageFilter, ImageStat, ImageEnhance \n",
    "import imutils\n",
    "import argparse\n",
    "import ntpath\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import random\n",
    "import blend_modes #Originally 'from blend_modes import blend_modes'\n",
    "\n",
    "original = True #Whether we are using the original exposure_manipulation code or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paths(directory): #Loading the paths of all the files in the directory\n",
    "    paths = []\n",
    "    for files in os.listdir(directory): #Retrieving names of files in directory\n",
    "        if (files != \".DS_Store\"): # ???\n",
    "            paths.append(directory+'/'+files) #Creating path strings for files in directory\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_borders(img,pix):\n",
    "    borders = []    \n",
    "    for y in range(0,img.size[1]):\n",
    "        found = False\n",
    "        has_grey = False\n",
    "        \n",
    "        for x in range(1,img.size[0]):\n",
    "            r = pix[x,y][0]\n",
    "            g = pix[x,y][1]\n",
    "            b = pix[x,y][2]\n",
    "                \n",
    "            prev_r = pix[x-1,y][0]\n",
    "            prev_g = pix[x-1,y][1]\n",
    "            prev_b = pix[x-1,y][2]\n",
    "            \n",
    "            r_g = abs(r-g)\n",
    "            r_b = abs(r-b)\n",
    "            g_b = abs(g-b)\n",
    "            \n",
    "            if (r_g<=15 and r_b<=15 and g_b<=15):\n",
    "                has_grey = True\n",
    "                \n",
    "            if ((abs(prev_r-r)>=20) or (abs(prev_g-g)>=20) or (abs(prev_b-b)>=20)):\n",
    "                    \n",
    "                for i in range(0,x):\n",
    "                    borders.append([i,y])\n",
    "                found = True\n",
    "                break   \n",
    "            \n",
    "        if ((not found)and(has_grey)):\n",
    "            for i in range(0,img.size[0]-1):\n",
    "                borders.append([i,y])  \n",
    "              \n",
    "        for x in range(img.size[0]-1,1,-1):\n",
    "            r = pix[x,y][0]\n",
    "            g = pix[x,y][1]\n",
    "            b = pix[x,y][2]\n",
    "                \n",
    "            prev_r = pix[x-1,y][0]\n",
    "            prev_g = pix[x-1,y][1]\n",
    "            prev_b = pix[x-1,y][2]\n",
    "                \n",
    "            if ((abs(prev_r-r)>=20) or (abs(prev_g-g)>=20) or (abs(prev_b-b)>=20)):\n",
    "                for i in range(x,img.size[0]-1):\n",
    "                    borders.append([i,y])\n",
    "                break\n",
    "                    \n",
    "    return borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_images(paths):\n",
    "    for image_path in paths:\n",
    "        img = Image.open(image_path)\n",
    "        img = img.convert('RGB')\n",
    "        pix = img.load()\n",
    "        \n",
    "        borders = find_borders(img,pix)\n",
    "        \n",
    "        #img.save(image_path)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        channels = cv2.split(image)\n",
    "        \n",
    "        alpha_channel = np.ones(channels[0].shape, dtype=channels[0].dtype) * 255\n",
    "        image_RGBA = cv2.merge((channels[0], channels[1], channels[2], alpha_channel))\n",
    "        \n",
    "        height, width, channels = image.shape\n",
    "        \n",
    "        #Deleting white perimeter of shape so that images have transparent background\n",
    "        for i in range(0,img.size[0]-1):\n",
    "            image_RGBA[0,i][3] = 0\n",
    "            image_RGBA[img.size[1]-1,i][3] = 0\n",
    "        \n",
    "        for i in range(0,img.size[1]-1):\n",
    "            image_RGBA[i,0][3] = 0\n",
    "            image_RGBA[i,img.size[0]-1][3] = 0\n",
    "        \n",
    "        for border in borders:\n",
    "            image_RGBA[border[1],border[0]][3] = 0\n",
    "        \n",
    "        head, tail = ntpath.split(image_path)\n",
    "        \n",
    "        title,extension = tail.split('.')\n",
    "        cv2.imwrite(\"Traffic_Signs_Templates/2_Processed_Images/\"+title+\".png\", image_RGBA) #Where is 'title' from?\n",
    "        img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seana's code\n",
    "# overlays foreground image on background image, keeping transparency\n",
    "# image overlay code credit to fireant:\n",
    "# https://stackoverflow.com/questions/14063070/overlay-a-smaller-image-on-a-larger-image-python-opencv\n",
    "# RGB to RGBA conversion credit to kaanoner:\n",
    "# https://stackoverflow.com/questions/32290096/python-opencv-add-alpha-channel-to-rgb-image\n",
    "# https://docs.opencv.org/3.4.2/d3/df2/tutorial_py_basic_ops.html used as reference\n",
    "def overlay(fg, bg, x1 = -1, y1 = -1):\n",
    "    # if the background doesn't have an alpha channel, add one, but keep the\n",
    "    # entire image opaque\n",
    "    if len(cv2.split(bg)) == 3:\n",
    "        bg = cv2.cvtColor(bg, cv.COLOR_RGB2RGBA)\n",
    "        bg[:, :, 3] = 255\n",
    "    # make a copy of the background for altering\n",
    "    newImage = bg.copy()\n",
    "\n",
    "    # retrieve image dimentions\n",
    "    heightFG, widthFG, _ = fg.shape  # discard channel\n",
    "    heightBG, widthBG, _ = bg.shape\n",
    "\n",
    "    # if either of the coordinates were omitted, calculate start/end positions\n",
    "    # using the difference in image size, centring foreground on background\n",
    "    if x1 == -1 or y1 == -1:\n",
    "        # calculate start coordinates \n",
    "        x1 = (widthBG - widthFG) // 2    # floor division to truncate as\n",
    "        y1 = (heightBG - heightFG) // 2  # coordinates don't need to be exact\n",
    "    # calculate end coordinates\n",
    "    x2 = x1 + widthFG\n",
    "    y2 = y1 + heightFG\n",
    "\n",
    "    ### start of code from fireant ###\n",
    "    # retrieve an array of alpha values from the foreground image\n",
    "    # colons with no numbers mean loop over entire image, 3 means alpha channel\n",
    "    # divide by 255 to end up with values between 0.0 and 1.0\n",
    "    alpha = fg[:, :, 3] / 255.0\n",
    "    beta = 1.0 - alpha\n",
    "\n",
    "    # loop over BGR channels (not alpha)\n",
    "    for ch in range(0, 3):\n",
    "        newImage[y1:y2, x1:x2, ch] = (alpha * fg[:, :, ch] +\n",
    "                                      beta * newImage[y1:y2, x1:x2, ch])\n",
    "    ### end of code from fireant ###\n",
    "\n",
    "    return newImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seana's code\n",
    "# resize the first image if it is larger than the second image\n",
    "def resize(fg, bg):\n",
    "    # retrieve dimentions of both images\n",
    "    heightFG, widthFG, _ = fg.shape  # discard channel\n",
    "    heightBG, widthBG, _ = bg.shape\n",
    "#     print(\"Is\", fg.shape, \"larger than\", bg.shape, \"?\")\n",
    "\n",
    "    # resize if foreground is taller than background\n",
    "    if heightFG > heightBG:\n",
    "#         print(\"It's taller\")\n",
    "        fg = resizeOnHeight(fg, heightBG)\n",
    "        heightFG, widthFG, _ = fg.shape  #re-retrieve dimentions\n",
    "    # or wider\n",
    "    if widthFG > widthBG:\n",
    "#         print(\"It's wider\")\n",
    "        fg = resizeOnWidth(fg, widthBG)\n",
    "\n",
    "    return fg, bg\n",
    "\n",
    "def resizeOnHeight(img, newHeight):\n",
    "    oldHeight, oldWidth, _ = img.shape  # discard channel\n",
    "    newWidth = int( round( newHeight / oldHeight * oldWidth ) )\n",
    "    img = cv2.resize(img, (newHeight, newWidth))\n",
    "#    old = (oldHeight, oldWidth)\n",
    "#    new = (newHeight, newWidth)\n",
    "#    print(\"Resized on height:\", old, \"->\", new)\n",
    "    return img\n",
    "\n",
    "def resizeOnWidth(img, newWidth):\n",
    "    oldWidth, oldHeight, _ = img.shape  # discard channel\n",
    "    newHeight = int( round( newWidth / oldWidth * oldHeight ) )\n",
    "    img = cv2.resize(img, (newWidth, newHeight))\n",
    "#    old = (oldHeight, oldWidth)\n",
    "#    new = (newHeight, newWidth)\n",
    "#    print(\"Resized on height:\", old, \"->\", new)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_types = [\"_ORIGINAL\", \"_QUADRANT_4\", \"_BOTTOM_HOLE\", \"_HOLES\", \"_YELLOW\",\n",
    "                \"_GRAFFITI\"] #Used for SGTSD\n",
    "\n",
    "def damage_images(paths):\n",
    "    for image_path in paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        img = img.astype('uint8')\n",
    "        height, width, ch = img.shape\n",
    "        centre_x = int(width/2)\n",
    "        centre_y = int(height/2)\n",
    "    \n",
    "        #Creating Mask\n",
    "        sign = img.copy() \n",
    "        alpha = sign[:, :, 3] #Extract the alpha channel from sign\n",
    "        _, alpha = cv2.threshold(alpha, 5, 255, cv2.THRESH_BINARY) #Remove gradual transparency\n",
    "        mask = alpha\n",
    "        \n",
    "        #Info for Writing\n",
    "        head, tail = ntpath.split(image_path)\n",
    "        title, extension = tail.split('.')\n",
    "        path = \"Traffic_Signs_Templates/3_Damaged_Images/\"\n",
    "        \n",
    "        \n",
    "        #NOT DAMAGED\n",
    "        if \"_ORIGINAL\" in damage_types:\n",
    "            dmg = img\n",
    "            \n",
    "            cv2.imwrite(path + title + \"_ORIGINAL.png\", dmg)\n",
    "        \n",
    "        \n",
    "        #TOP-LEFT QUADRANT\n",
    "        if \"_QUADRANT_4\" in damage_types:\n",
    "            top_left = mask.copy()\n",
    "            cv2.rectangle(top_left, (0,0), (centre_x,centre_y), (0,0,0), -1)\n",
    "            dmg1 = cv2.bitwise_and(img, img, mask=top_left)\n",
    "            \n",
    "            cv2.imwrite(path + title + \"_QUADRANT_4.png\", dmg1)\n",
    "        \n",
    "        \n",
    "        #BOTTOM HOLE\n",
    "        if \"_BOTTOM_HOLE\" in damage_types:\n",
    "            bottom_hole = mask.copy()\n",
    "            cv2.circle(bottom_hole, (centre_x,height), int(height / 2), (0,0,0), -1)\n",
    "            dmg2 = cv2.bitwise_and(img, img, mask=bottom_hole)\n",
    "            \n",
    "            cv2.imwrite(path + title + \"_BOTTOM_HOLE.png\", dmg2)\n",
    "        \n",
    "        \n",
    "        #RANDOMISED BULLET HOLES\n",
    "        if \"_HOLES\" in damage_types:\n",
    "            bullet_holes = mask.copy()\n",
    "            painted = img.copy() #Another copy of original sign to draw 'flaking paint' from holes on\n",
    "            numHoles = random.randint(7, 30)\n",
    "            for x in range(numHoles):\n",
    "                size = random.randint(2, 12) #Random hole size\n",
    "                h_x = random.randint(0, height) #Random hole position\n",
    "                h_y = random.randint(0, height)\n",
    "                c = random.randint(0, 150) #How black/grey the 'hole' is if it didn't penetrate\n",
    "                s = random.uniform(1.6, 2.2) #Random annulus size\n",
    "\n",
    "                C = 200 #Colour of damaged 'paint' outer annulus\n",
    "                cv2.circle(painted, (h_x, h_y), int(size * s), (C,C,C,255), -1) #Hole annulus to represent damaged 'paint'\n",
    "                if (size < 6): #Did the bullet penetrate through the sign?\n",
    "                    cv2.circle(painted, (h_x, h_y), size, (c,c,c,255), -1) #If not, grey out rather than make transparent\n",
    "                else:\n",
    "                    cv2.circle(bullet_holes, (h_x, h_y), size, (0,0,0), -1)\n",
    "            dmg3 = cv2.bitwise_and(painted, painted, mask=bullet_holes)\n",
    "            \n",
    "            cv2.imwrite(path + title + \"_HOLES.png\", dmg3)\n",
    "        \n",
    "        \n",
    "        #TINTED YELLOW\n",
    "        if \"_YELLOW\" in damage_types:\n",
    "            yellow = np.zeros((height,width,ch), np.uint8)\n",
    "            yellow[:,:] = (0,210,210,255)\n",
    "            dmg4 = cv2.bitwise_and(img, yellow)\n",
    "            \n",
    "            cv2.imwrite(path + title + \"_YELLOW.png\", dmg4)\n",
    "            \n",
    "            \n",
    "        #GRAFFITI\n",
    "        if \"_GRAFFITI\" in damage_types:\n",
    "            fg = cv2.imread(\"Traffic_Signs_Templates/Graffiti/graffiti_black.png\", cv2.IMREAD_UNCHANGED)\n",
    "            bg = img.copy()\n",
    "            fg, bg = resize(fg, bg) #Correct graffiti size to fit onto sign\n",
    "            dmg5 = overlay(fg, bg)\n",
    "            \n",
    "            cv2.imwrite(path + title + \"_GRAFFITI.png\", dmg5)\n",
    "            \n",
    "        \n",
    "        #FADE (doesn't work as a damage type with Stergiou's exposure_manipulation)\n",
    "#         if \"_FADE\" in damage_types:\n",
    "#             #Retrieve alpha data from original image\n",
    "#             splitImg = cv2.split(img)\n",
    "#             if len(splitImg) is 4:\n",
    "#                 alphaData = splitImg[3]\n",
    "#                \n",
    "#             for ii in range(5):\n",
    "#                 dmg6 = img.copy()\n",
    "#                 alpha = 1 - (ii * 0.19)\n",
    "#                 beta = (ii + 1) * 40\n",
    "#                 cv2.convertScaleAbs(img, dmg6, alpha, beta) #Scale the contrast and brightness\n",
    "#                 dmg6[:, :, 3] = alphaData\n",
    "#                \n",
    "#                 cv2.imwrite(path + title + \"_FADE-\" + str(ii) + \".png\", dmg6)\n",
    "            \n",
    "        \n",
    "        #CRACKS (thin crack lines across the sign?)\n",
    "        #MISSING SECTIONS (removed polygons on edges of sign?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating processed images with transparent background\n",
    "directory = 'Traffic_Signs_Templates/1_Images' #Template images\n",
    "if (not os.path.exists(\"Traffic_Signs_Templates/2_Processed_Images\")):\n",
    "    os.mkdir(\"Traffic_Signs_Templates/2_Processed_Images\") #Create directory for processed images\n",
    "paths = load_paths(directory)\n",
    "manipulate_images(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating damaged sign templates based on processed images of original templates\n",
    "directory = 'Traffic_Signs_Templates/2_Processed_Images' #Processed images\n",
    "if (not os.path.exists(\"Traffic_Signs_Templates/3_Damaged_Images\")):\n",
    "    os.mkdir(\"Traffic_Signs_Templates/3_Damaged_Images\") #Create directory for processed images\n",
    "paths = load_paths(directory)\n",
    "damage_images(paths) #FIXME: Only applies damage to one sign type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(paths):\n",
    "    for image_path in paths:\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        rows,cols,ch = img.shape\n",
    "        t = []\n",
    "        for i in range(0,100):\n",
    "            t.append(i)\n",
    "            \n",
    "        #FORWARD FACING\n",
    "        dst = img\n",
    "        \n",
    "        #EAST FACING\n",
    "        pts1 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols/10,rows/2]])\n",
    "        pts2 = np.float32([[cols/5,rows/5],[cols/2,rows/8],[cols/5,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts1,pts2)\n",
    "        dst1 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #NORTH-WEST FACING\n",
    "        pts3 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts4 = np.float32([[cols*4.5/5,rows/5],[cols/2,rows/8],[cols*4.5/5,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts3,pts4)\n",
    "        dst2 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #LEFT TILTED FORWARD FACING\n",
    "        pts5 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols/10,rows/2]])\n",
    "        pts6 = np.float32([[cols/12,rows/6],[cols/2.1,rows/8],[cols/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts5,pts6)\n",
    "        dst3 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #RIGHT TILTED FORWARD FACING\n",
    "        pts7 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts8 = np.float32([[cols*10/12,rows/6],[cols/2.2,rows/8],[cols*8.4/10,rows/1.8]])\n",
    "        M = cv2.getAffineTransform(pts7,pts8)\n",
    "        dst4 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #WEST FACING\n",
    "        pts9 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts10 = np.float32([[cols/9.95,rows/10],[cols/2.05,rows/9.95],[cols*9/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts9,pts10)\n",
    "        dst5 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #RIGHT TILTED FORWARD FACING\n",
    "        pts11 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts12 = np.float32([[cols*9/10,rows/10],[cols/2,rows/9],[cols*8.95/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts11,pts12)\n",
    "        dst6 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION\n",
    "        pts13 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts14 = np.float32([[cols/9.8,rows/9.8],[cols/2,rows/9.8],[cols*8.8/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts13,pts14)\n",
    "        dst7 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 2\n",
    "        pts15 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts16 = np.float32([[cols/11,rows/10],[cols/2.1,rows/10],[cols*8.5/10,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts15,pts16)\n",
    "        dst8 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 3\n",
    "        pts17 = np.float32([[cols/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts18 = np.float32([[cols/11,rows/11],[cols/2.1,rows/10],[cols*10/11,rows/1.95]])\n",
    "        M = cv2.getAffineTransform(pts17,pts18)\n",
    "        dst9 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 4\n",
    "        pts19 = np.float32([[cols*9.5/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts20 = np.float32([[cols*9.35/10,rows/9.99],[cols/2.05,rows/9.95],[cols*9.05/10,rows/2.03]])\n",
    "        M = cv2.getAffineTransform(pts19,pts20)\n",
    "        dst10 = cv2.warpAffine(img,M,(cols,rows))\n",
    "         \n",
    "        #FORWARD FACING W/ DISTORTION 5\n",
    "        pts21 = np.float32([[cols*9.5/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts22 = np.float32([[cols*9.65/10,rows/9.95],[cols/1.95,rows/9.95],[cols*9.1/10,rows/2.02]])\n",
    "        M = cv2.getAffineTransform(pts21,pts22)\n",
    "        dst11 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 6\n",
    "        pts23 = np.float32([[cols*9.25/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts24 = np.float32([[cols*9.55/10,rows/9.85],[cols/1.9,rows/10],[cols*9.3/10,rows/2.04]])\n",
    "        M = cv2.getAffineTransform(pts23,pts24)\n",
    "        dst12 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #SHRINK 1\n",
    "        pts25 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts26 = np.float32([[cols*8/10,rows/10],[cols*1.34/3,rows/10.5],[cols*8.24/10,rows/2.5]])\n",
    "        M = cv2.getAffineTransform(pts25,pts26)\n",
    "        dst13 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #SHRINK 2\n",
    "        pts27 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts28 = np.float32([[cols*8.5/10,rows*3.1/10],[cols/2,rows*3/10],[cols*8.44/10,rows*1.55/2.5]])\n",
    "        M = cv2.getAffineTransform(pts27,pts28)\n",
    "        dst14 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 7\n",
    "        pts29 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts30 = np.float32([[cols*8.85/10,rows/9.3],[cols/1.9,rows/10.5],[cols*8.8/10,rows/2.11]])\n",
    "        M = cv2.getAffineTransform(pts29,pts30)\n",
    "        dst15 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 8\n",
    "        pts31 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts32 = np.float32([[cols*8.75/10,rows/9.1],[cols/1.95,rows/8],[cols*8.5/10,rows/2.05]])\n",
    "        M = cv2.getAffineTransform(pts31,pts32)\n",
    "        dst16 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 9\n",
    "        pts33 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts34 = np.float32([[cols*8.75/10,rows/9.1],[cols/1.95,rows/9],[cols*8.5/10,rows/2.2]])\n",
    "        M = cv2.getAffineTransform(pts33,pts34)\n",
    "        dst17 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 10\n",
    "        pts35 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts36 = np.float32([[cols*8.75/10,rows/8],[cols/1.95,rows/8],[cols*8.75/10,rows/2]])\n",
    "        M = cv2.getAffineTransform(pts35,pts36)\n",
    "        dst18 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        #FORWARD FACING W/ DISTORTION 11\n",
    "        pts37 = np.float32([[cols*9/10,rows/10],[cols/2,rows/10],[cols*9/10,rows/2]])\n",
    "        pts38 = np.float32([[cols*8.8/10,rows/7],[cols/1.95,rows/7],[cols*8.8/10,rows/2]])\n",
    "        M = cv2.getAffineTransform(pts37,pts38)\n",
    "        dst19 = cv2.warpAffine(img,M,(cols,rows))\n",
    "        \n",
    "        head, tail = ntpath.split(image_path)\n",
    "        title, extension = tail.split('.')\n",
    "        name = title.split('_') #name[0] gives us the sign number\n",
    "        \n",
    "        path = \"Traffic_Signs_Templates/4_Transformed_Images/\" + name[0] + \"/\" + title + \"/\"\n",
    "        cv2.imwrite(path + str(1) + \".png\", dst)\n",
    "        cv2.imwrite(path + str(2) + \".png\", dst1)\n",
    "        cv2.imwrite(path + str(3) + \".png\", dst2)\n",
    "        cv2.imwrite(path + str(4) + \".png\", dst3)\n",
    "        cv2.imwrite(path + str(4) + \".png\", dst4)\n",
    "        cv2.imwrite(path + str(5) + \".png\", dst5)\n",
    "        cv2.imwrite(path + str(6) + \".png\", dst6)\n",
    "        cv2.imwrite(path + str(7) + \".png\", dst7)\n",
    "        cv2.imwrite(path + str(8) + \".png\", dst8)\n",
    "        cv2.imwrite(path + str(9) + \".png\", dst9)\n",
    "        cv2.imwrite(path + str(10) + \".png\", dst10)\n",
    "        cv2.imwrite(path + str(11) + \".png\", dst11)\n",
    "        cv2.imwrite(path + str(12) + \".png\", dst12)\n",
    "        cv2.imwrite(path + str(13) + \".png\", dst13)\n",
    "        cv2.imwrite(path + str(14) + \".png\", dst14)\n",
    "        cv2.imwrite(path + str(15) + \".png\", dst15)\n",
    "        cv2.imwrite(path + str(16) + \".png\", dst16)\n",
    "        cv2.imwrite(path + str(17) + \".png\", dst17)\n",
    "        cv2.imwrite(path + str(18) + \".png\", dst18)\n",
    "        cv2.imwrite(path + str(19) + \".png\", dst19)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = 'Traffic_Signs_Templates/1_Images'\n",
    "directory2 = 'Traffic_Signs_Templates/3_Damaged_Images' #Changed from /2_Processed_Images\n",
    "if (not os.path.exists(\"Traffic_Signs_Templates/4_Transformed_Images\")):\n",
    "    for path1 in load_paths(directory1):\n",
    "        head, tail = ntpath.split(path1)\n",
    "        title, extenstion = tail.split('.')\n",
    "        os.makedirs(\"Traffic_Signs_Templates/4_Transformed_Images/\" + title)\n",
    "    for path2 in load_paths(directory2):\n",
    "        head, tail = ntpath.split(path2)    \n",
    "        title, extension = tail.split('.')\n",
    "        name = title.split('_') #name[0] gives us the sign number\n",
    "        os.makedirs(\"Traffic_Signs_Templates/4_Transformed_Images/\" + name[0] + \"/\" + title)\n",
    "paths = load_paths(directory2)\n",
    "img_transform(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_exposure(paths,channels):\n",
    "    exposures = []\n",
    "    for image_path in paths:\n",
    "        img = Image.open(image_path)\n",
    "        im = Image.open(image_path).convert('LA')\n",
    "        \n",
    "        stat = ImageStat.Stat(im)\n",
    "        \n",
    "        #Average pixel brighness\n",
    "        avg = stat.mean[0]\n",
    "        \n",
    "        #RMS pixel brighness\n",
    "        rms = stat.rms[0]\n",
    "        \n",
    "        stat2 = ImageStat.Stat(img)\n",
    "        \n",
    "        #Consider the number of channels\n",
    "        #background may have RGB while traffic sign has RGBA\n",
    "        if (channels==3):\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "        else:\n",
    "            #Average pixels preceived brightness\n",
    "            r,g,b,a = stat2.mean\n",
    "            avg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            #RMS pixels perceived brightness\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2)) \n",
    "\n",
    "            l = [image_path,avg,rms,avg_perceived,rms_perceived]\n",
    "            exposures.append(l)\n",
    "\n",
    "    return exposures     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_png(directory):\n",
    "    for files in load_paths(directory):\n",
    "        title,extension = files.split('.')\n",
    "        img = Image.open(files).convert('RGBA')\n",
    "        if (not extension == \"png\"):\n",
    "            os.remove(files)\n",
    "        img.save(title+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#to_png(\"Google_search_backgrounds/UK_urban\")\n",
    "to_png(\"Google_search_backgrounds/UK_rural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths,4)\n",
    "    signs_exposures = find_image_exposure(signs_paths,4)\n",
    "    \n",
    "    for i in range(0,len(background_paths)):\n",
    "        print(\"Processed: \" + str(float(i) / float(len(background_paths)) * 100) + \" %\")\n",
    "        \n",
    "        img = Image.open(background_exposures[i][0])\n",
    "\n",
    "        for sign_path in signs_paths:        \n",
    "            dirc,sub,el = background_exposures[i][0].split('/')\n",
    "            title,extension = el.split('.')\n",
    "\n",
    "            parent_dir,sub_dir,folder,folder2,element = sign_path.split('/')\n",
    "            head,tail = element.split('.')\n",
    "\n",
    "            \n",
    "            ###   ORIGINAL EXPOSURE IMPLEMENTATION   ###\n",
    "            brightness_avrg = 1.0\n",
    "            brightness_rms = 1.0\n",
    "            brightness_avrg_perceived = 1.0\n",
    "            brightness_rms_perceived = 1.0\n",
    "            brightness_avrg2 = 1.0\n",
    "            brightness_rms2 = 1.0\n",
    "\n",
    "            # abs(desired_brightness - actual_brightness) / abs(brightness_float_value) = ratio\n",
    "            avrg_ratio = 11.0159464507\n",
    "\n",
    "            rms_ratio = 8.30320014372\n",
    "\n",
    "            percieved_avrg_ratio = 3.85546373056\n",
    "\n",
    "            percieved_rms_ratio = 35.6344530649\n",
    "\n",
    "            avrg2_ratio = 1.20354549572\n",
    "\n",
    "            rms2_ratio = 40.1209106864\n",
    "\n",
    "            peak = Image.open(sign_path).convert('LA')\n",
    "            peak2 = Image.open(sign_path).convert('RGBA')\n",
    "\n",
    "            stat = ImageStat.Stat(peak)\n",
    "            avrg = stat.mean[0]\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            #IMAGE MANIPULATION MAIN CODE STARTS\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(avrg-float(background_exposures[i][1]))\n",
    "            \n",
    "            brightness_avrg = margin/avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright = enhancer.enhance(brightness_avrg)\n",
    "            stat = ImageStat.Stat(avrg_bright)\n",
    "            avrg = stat.mean[0]\n",
    "\n",
    "            \n",
    "            #MINIMISE MARGIN BASED ON ROOT MEAN SQUARE FOR TWO CHANNEL BRIGNESS VARIATION\n",
    "            margin = abs(rms-float(background_exposures[i][2]))\n",
    "\n",
    "            brightness_rms = margin/rms_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright = enhancer.enhance(brightness_rms)\n",
    "            stat = ImageStat.Stat(rms_bright)\n",
    "            rms = stat.rms[0]\n",
    "\n",
    "            \n",
    "            #MINIMISE MARGIN BASED ON AVERAGE FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            stat2 = ImageStat.Stat(peak2)\n",
    "            r,g,b,a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "            margin = abs(avrg_perceived-float(background_exposures[i][3]))\n",
    "            \n",
    "            brightness_avrg_perceived = margin/percieved_avrg_ratio \n",
    "            \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright_perceived = enhancer.enhance(brightness_avrg_perceived)\n",
    "            stat2 = ImageStat.Stat(avrg_bright_perceived)\n",
    "            r,g,b,a = stat2.mean\n",
    "            avrg_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))        \n",
    "\n",
    "\n",
    "            #MINIMISE MARGIN BASED ON RMS FOR RGBA (\"PERCEIVED BRIGHNESS\")\n",
    "            #REFERENCE FOR ALGORITHM USED: http://alienryderflex.com/hsp.html\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "            margin = abs(rms_perceived-float(background_exposures[i][4]))\n",
    "\n",
    "            brightness_rms_perceived = margin/percieved_rms_ratio \n",
    "\n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright_perceived = enhancer.enhance(brightness_rms_perceived)\n",
    "            stat2 = ImageStat.Stat(rms_bright_perceived)\n",
    "            r,g,b,a = stat2.rms\n",
    "            rms_perceived = math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))        \n",
    "\n",
    "            \n",
    "            stat3 = ImageStat.Stat(peak2)\n",
    "            avrg2 = stat3.mean[0]\n",
    "            rms2 = stat3.rms[0]\n",
    "\n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO AVERAGING METHODS\n",
    "            margin = abs(avrg2-float(background_exposures[i][1]))\n",
    "            brightness_avrg2 = margin/avrg2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            avrg_bright2 = enhancer.enhance(brightness_avrg2)\n",
    "            stat3 = ImageStat.Stat(avrg_bright2)\n",
    "            avrg2 = stat3.mean[0]       \n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            #FUSION OF THE TWO RMS METHODS\n",
    "            margin = abs(rms2-float(background_exposures[i][2]))\n",
    "            brightness_rms2 = margin/rms2_ratio \n",
    "            enhancer = ImageEnhance.Brightness(peak2)\n",
    "            rms_bright2 = enhancer.enhance(brightness_rms2)\n",
    "            stat3 = ImageStat.Stat(rms_bright2)\n",
    "            rms2 = stat3.rms[0]\n",
    "            \"\"\"\n",
    "            \n",
    "            avrg_bright = avrg_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright = rms_bright.resize((150,150), Image.ANTIALIAS)\n",
    "            avrg_bright_perceived = avrg_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            rms_bright_perceived = rms_bright_perceived.resize((150,150), Image.ANTIALIAS)\n",
    "            #avrg_bright2 = avrg_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "            #rms_bright2 = rms_bright2.resize((150,150), Image.ANTIALIAS)\n",
    "\n",
    "            \n",
    "            exp_dir = \"Traffic_Signs_Exposure_Manipulation/\"\n",
    "            avrg_bright.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE.\"+tail)\n",
    "            rms_bright.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS.\"+tail)\n",
    "            avrg_bright_perceived.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE_PERCEIVED.\"+tail)\n",
    "            rms_bright_perceived.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS_PERCEIVED.\"+tail)\n",
    "            #avrg_bright2.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_AVERAGE2.\"+tail)\n",
    "            #rms_bright2.save(exp_dir+sub+\"/\"+title+\"/SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_RMS2.\"+tail)\n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_manipulation(signs_paths, backgrounds_paths):\n",
    "    background_exposures = find_image_exposure(background_paths,4)\n",
    "    signs_exposures = find_image_exposure(signs_paths,4)\n",
    "    \n",
    "    print(\"Processed: 0.0 %\")\n",
    "    ii = 0\n",
    "    prev = 0\n",
    "    for sign_path in signs_paths:\n",
    "        progress = float(ii) / float(len(signs_paths)) * 100\n",
    "        if progress >= prev + 5: #Prevent spamming of progress prints\n",
    "            prev = prev + 5\n",
    "            print(\"Processed: \" + str(progress) + \" %\")\n",
    "\n",
    "        dirc,sub,el = background_exposures[0][0].split('/')\n",
    "        title,extension = el.split('.')\n",
    "\n",
    "        parent_dir,sub_dir,folder,folder2,element = sign_path.split('/')\n",
    "        head,tail = element.split('.')\n",
    "\n",
    "        img = cv2.imread(sign_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "        ###   GRADUAL FADE IMPLEMENTATION   ###\n",
    "        #Retrieve alpha data from original image\n",
    "        splitImg = cv2.split(img)\n",
    "        if len(splitImg) is 4:\n",
    "            alphaData = splitImg[3]\n",
    "\n",
    "        for jj in range(0,5): #Changed from range(0,6); I thought the last one was too bright\n",
    "            dmg6 = img.copy()\n",
    "            alpha = 1 - (jj * 0.19)\n",
    "            beta = (jj + 1) * 40\n",
    "            if jj > 0:\n",
    "                cv2.convertScaleAbs(img, dmg6, alpha, beta) #Scale the contrast and brightness\n",
    "                dmg6[:, :, 3] = alphaData\n",
    "\n",
    "            dmg6 = cv2.resize(dmg6, (150,150))\n",
    "            fad_dir = \"Traffic_Signs_Fade_Manipulation/\"\n",
    "            cv2.imwrite(fad_dir+\"SIGN_\"+folder+\"/\"+folder2+\"/\"+head+\"_FADE-\"+str(jj)+\".\"+tail, dmg6)\n",
    "        ii = ii + 1\n",
    "    \n",
    "    \n",
    "    print(\"Processed: \" + str(100) + \" %\")\n",
    "    print(\"Process was successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bg_dir = \"Google_search_backgrounds\"\n",
    "\n",
    "for dirs in load_paths(bg_dir):\n",
    "    initial, subd = dirs.split('/')\n",
    "    \n",
    "    if original is True:\n",
    "        for background in load_paths(dirs):\n",
    "            initial, subd, element = background.split('/')\n",
    "            title, extension = element.split('.')\n",
    "\n",
    "            for signp in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "                for sign in load_paths(signp):\n",
    "                    d,s,f,e = sign.split('/') #Eg. s = 4_Transformed_Images, f = 9, e = 9_BOTTOM_HOLE\n",
    "\n",
    "                    exp_dir = \"Traffic_Signs_Exposure_Manipulation/\"\n",
    "                    if (not os.path.exists(exp_dir + subd + \"/\" + title + \"/SIGN_\" + f + \"/\" + e)):\n",
    "                        os.makedirs(exp_dir + subd + \"/\" + title + \"/SIGN_\" + f + \"/\" + e)\n",
    "    else:\n",
    "        for signp in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "            for sign in load_paths(signp):\n",
    "                d,s,f,e = sign.split('/')\n",
    "\n",
    "                fad_dir = \"Traffic_Signs_Fade_Manipulation/\"\n",
    "                if (not os.path.exists(fad_dir + \"SIGN_\" + f + \"/\" + e)):\n",
    "                    os.makedirs(fad_dir + \"SIGN_\" + f + \"/\" + e)\n",
    "\n",
    "signs_paths = []\n",
    "for p in load_paths(\"Traffic_Signs_Templates/4_Transformed_Images\"):\n",
    "    for d in load_paths(p):\n",
    "        signs_paths = signs_paths + load_paths(d)\n",
    "\n",
    "background_paths = load_paths(\"Google_search_backgrounds/UK_rural\")\n",
    "if original is True:\n",
    "    exposure_manipulation(signs_paths, background_paths)\n",
    "else:\n",
    "    fade_manipulation(signs_paths, background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#background_paths = load_paths(\"Google_search_backgrounds/UK_urban\") #Don't do, takes too long to do both\n",
    "#exposure_manipulation(signs_paths,background_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avrg_pixel_rgb(image,chanels):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    if (chanels == 4):\n",
    "        r,g,b,a = stat.rms\n",
    "    else:\n",
    "        r,g,b = stat.rms\n",
    "    \n",
    "    return [r,g,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bw_images(directory):\n",
    "    images = []\n",
    "    for signs in load_paths(directory):\n",
    "        img = Image.open(signs).convert('RGBA')\n",
    "        rgb = avrg_pixel_rgb(img,4)\n",
    "        rg = abs(rgb[0]-rgb[1])\n",
    "        rb = abs(rgb[0]-rgb[2])\n",
    "        gb = abs(rgb[1]-rgb[2])\n",
    "        \n",
    "        temp = signs.split('/')\n",
    "        head,tail = temp[-1].split('.')\n",
    "                \n",
    "        if (rg<=1 and rb<=1 and gb<=1):\n",
    "            images.append(head)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_useful_signs(directory): #Removes bad signs, such as those which are all white or all black\n",
    "    bw_images = find_bw_images(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "    for background_dir in load_paths(directory):\n",
    "        for signs in load_paths(background_dir):\n",
    "            for dmgs in load_paths(signs):\n",
    "                temp = []\n",
    "                for imgs in load_paths(dmgs):\n",
    "                    temp.append(imgs)\n",
    "                exposures = find_image_exposure(temp,4)\n",
    "                \n",
    "                i = 0\n",
    "                for images in load_paths(dmgs):\n",
    "                    #Find brightness\n",
    "                    img = Image.open(images).convert('RGBA')\n",
    "\n",
    "                    rgb = avrg_pixel_rgb(img,4)\n",
    "                    rg = abs(rgb[0]-rgb[1])\n",
    "                    rb = abs(rgb[0]-rgb[2])\n",
    "                    gb = abs(rgb[1]-rgb[2])\n",
    "\n",
    "                    is_bw = False\n",
    "\n",
    "                    for s in bw_images:\n",
    "                        if s in exposures[i][0]:\n",
    "                            is_bw = True\n",
    "\n",
    "                    if (rg<=16 and rb<=16 and gb<=16):\n",
    "                        if (not is_bw):\n",
    "                            os.remove(images)\n",
    "                        #Threshold values for black and white images\n",
    "                        elif (rgb[0]<70 and rgb[1]<70 and rgb[2]<70):\n",
    "                            os.remove(images)\n",
    "                        elif (rgb[0]>155 and rgb[1]>155 and rgb[2]>155):\n",
    "                            os.remove(images)\n",
    "\n",
    "                    elif (not is_bw):\n",
    "                        #Delete light blue images\n",
    "                        if(rgb[2]>rgb[0] and rgb[2]>=rgb[1]):\n",
    "                            if (gb<=10):\n",
    "                                os.remove(images)\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directory = \"Traffic_Signs_Exposure_Manipulation/UK_rural\"\n",
    "    find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if original is True:\n",
    "#     directory= \"Traffic_Signs_Exposure_Manipulation/UK_urban\"\n",
    "#     find_useful_signs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_poisson_noise (image):\n",
    "    vals = len(np.unique(image))\n",
    "    vals = 2.05 ** np.ceil(np.log2(vals))\n",
    "    noisy = np.random.poisson(image * vals) / float(vals)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_Gaussian_noise (image):\n",
    "    row,col,ch= image.shape\n",
    "    mean = 0\n",
    "    var = 0.5\n",
    "    sigma = var**0.5\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_speckle_noise (image):\n",
    "    row,col,ch = image.shape\n",
    "    gauss = np.random.randn(row,col,ch)\n",
    "    gauss = gauss.reshape(row,col,ch)        \n",
    "    noisy = image + image * gauss\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise_method (image):\n",
    "    \"\"\"\n",
    "    i = random.randint(1, 3)\n",
    "    if (i == 1):\n",
    "        return insert_poisson_noise(image)\n",
    "    elif (i==2):\n",
    "        return insert_Gaussian_noise(image)\n",
    "    else:\n",
    "        return insert_speckle_noise(image)\n",
    "    \"\"\"\n",
    "    image.setflags(write=1)\n",
    "    #Add noise in every pixel w/ random probability 0.4\n",
    "    for im in image:\n",
    "        px = 0\n",
    "        for pixel in im:\n",
    "            apply_noise = random.randint(0,100)\n",
    "            #if random probability\n",
    "            if apply_noise > 40:\n",
    "                #RGB values\n",
    "                R = pixel[0]\n",
    "                G = pixel[1]\n",
    "                B = pixel[2]\n",
    "                A = pixel[3]\n",
    "                #find current relative lumination for brighness\n",
    "                #based on: https://en.wikipedia.org/wiki/Relative_luminance\n",
    "                relative_lumination = 0.2126*R + 0.7152*G + 0.0722*B\n",
    "                #find differences between RGB values     \n",
    "                R_to_G = float(R)/float(G)\n",
    "                RG = False\n",
    "                if (R_to_G >= 1): RG=True\n",
    "                R_to_B = float(R)/float(B)\n",
    "                RB = False\n",
    "                if (R_to_B >= 1): RB=True\n",
    "                G_to_B = float(G)/float(B)\n",
    "                GB = False\n",
    "                if (G_to_B >= 1): GB=True\n",
    "                equal = False\n",
    "                if (R==G==B):equal==True\n",
    "\n",
    "                #In order to determine the margin in which the new brighness\n",
    "                #should be within, the upper and lower limits need to be foun\n",
    "                #The Relative luminance in colorimetric spaces has normilised\n",
    "                #values between 0 and 255\n",
    "                upper_limit = 255\n",
    "                lower_limit = 0\n",
    "                if (relative_lumination + 40 < 255):\n",
    "                    upper_limit = relative_lumination + 40\n",
    "                if (relative_lumination - 40 > 0):\n",
    "                    lower_limit = relative_lumination - 40\n",
    "\n",
    "                #Compute new brighness value\n",
    "                new_lumination = random.randint(int(lower_limit),int(upper_limit))\n",
    "\n",
    "                #find the three possible solutions that satisfy\n",
    "                #->The new lumination chosen based on the Relative luminance equation\n",
    "                #->The precentages computed between every RGB value\n",
    "\n",
    "                solutions = []\n",
    "\n",
    "                for r in range(1,255):\n",
    "                    for g in range(1,255):\n",
    "                        for b in range(1,255):\n",
    "                            r_to_g = float(r)/float(g)\n",
    "                            rg = False\n",
    "                            if (r_to_g >= 1): rg=True\n",
    "                            r_to_b = float(r)/float(b)\n",
    "                            rb = False\n",
    "                            if (r_to_b >= 1): rb=True\n",
    "                            g_to_b = float(g)/float(b)\n",
    "                            gb = False\n",
    "                            if (g_to_b >= 1): gb=True\n",
    "                            e = False\n",
    "                            if(r==g==b):\n",
    "                                e=True\n",
    "                            if (0.2126*r + 0.7152*g + 0.0722*b == 100) and rg==RG and rb==RB and gb==GB and e==equal:\n",
    "                                solutions.append([r,g,b])\n",
    "\n",
    "                #Find the solution that precentage wise is closer to the original\n",
    "                #difference between the values\n",
    "                percentages = []\n",
    "\n",
    "                for solution in solutions:\n",
    "                    r = solution[0]\n",
    "                    g = solution[1]\n",
    "                    b = solution[2]\n",
    "                    percentages.append((float(r)/float(g))+(float(r)/float(b))+(float(g)/float(b)))\n",
    "\n",
    "                i = 0\n",
    "                pos = 0\n",
    "                best = percentages[0]\n",
    "                for p in percentages[1:]:\n",
    "                    if p < best:\n",
    "                        pos = i\n",
    "                    i = i +1\n",
    "\n",
    "                #Assign new pixel values\n",
    "                im[px] = [solutions[pos][0],solutions[pos][1],solutions[pos][2],A]\n",
    "            px = px+1\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORIGINAL ###\n",
    "# def new_data(image_dir,bg_dir): #Blends synthetic signs with backgrounds\n",
    "#     # Import background image\n",
    "#     background_img_raw = Image.open(bg_dir).convert('RGBA')  \n",
    "#     background_img_raw = background_img_raw.resize((150,150), Image.ANTIALIAS)\n",
    "#     background_img = np.array(background_img_raw)  \n",
    "#     background_img_float = background_img.astype(float)  \n",
    "\n",
    "#     # Import foreground image\n",
    "#     foreground_img_raw = Image.open(image_dir)  \n",
    "#     foreground_img = np.array(foreground_img_raw)  \n",
    "#     foreground_img_float = foreground_img.astype(float)  \n",
    "\n",
    "#     # Blend images\n",
    "#     opacity = 1  \n",
    "#     blended_img_float = blend_modes.grain_merge(background_img_float, foreground_img_float, opacity)\n",
    "\n",
    "#     # Convert blended image back into PIL image\n",
    "#     blended_img = np.uint8(blended_img_float)\n",
    "#     blended_img_raw = Image.fromarray(blended_img)  \n",
    "    \n",
    "#     foreground_img_raw = foreground_img_raw.resize((149,149), Image.ANTIALIAS)\n",
    "#     blended_img_raw.paste(foreground_img_raw, (0, 0), foreground_img_raw)\n",
    "#     blended_img_raw = blended_img_raw.resize((48,48), Image.ANTIALIAS)\n",
    "    \n",
    "#     #temp = np.uint8(blended_img_raw)\n",
    "#     #temp = random_noise_method(temp)\n",
    "    \n",
    "#     #blended_img_raw = Image.fromarray(np.uint8(temp)) \n",
    "    \n",
    "#     return blended_img_raw\n",
    "\n",
    "### FULL BACKGROUND ###\n",
    "def new_data(image_dir,bg_dir): #Blends synthetic signs with backgrounds\n",
    "    bg = cv2.imread(bg_dir, cv2.IMREAD_UNCHANGED)\n",
    "    fg = cv2.imread(image_dir, cv2.IMREAD_UNCHANGED)\n",
    "    return overlay(fg, bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the required directories in SGTSD\n",
    "directory = 'SGTSD/Images'\n",
    "if (not os.path.exists(\"SGTSD/Images\")):\n",
    "    #Numbered Version\n",
    "    for sign in load_paths(\"Traffic_Signs_Templates/1_Images\"): #Folders for each sign type\n",
    "        head, tail = sign.split('.')\n",
    "        name = head.split('/')\n",
    "        os.makedirs(\"SGTSD/Images/\" + name[-1])\n",
    "        j = 0\n",
    "        for dmg in range(len(damage_types)): #Folders for each damage type\n",
    "            os.makedirs(\"SGTSD/Images/\" + name[-1] + \"/\" + name[-1] + \"_\" + str(j))\n",
    "            j = j + 1\n",
    "    \n",
    "    #Named Version\n",
    "#     for sign in load_paths(\"Traffic_Signs_Templates/1_Images\"): #Folders for each sign type\n",
    "#         head, tail = sign.split('.')\n",
    "#         name = head.split('/')\n",
    "#         os.makedirs(\"SGTSD/Images/\" + name[-1])\n",
    "#         for dmg in load_paths(\"Traffic_Signs_Templates/3_Damaged_Images\"): #Folders for each damage type\n",
    "#             headD,tailD = dmg.split('.')\n",
    "#             nameD = headD.split('/')\n",
    "#             os.makedirs(\"SGTSD/Images/\" + name[-1] + \"/\" + nameD[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a README file\n",
    "content = '''\n",
    "-----------------------------------------------\n",
    "|                     -*-                     |\n",
    "|Synthetically Generated Traffic Sign Dataset |\n",
    "|                     -*-                     |\n",
    "-----------------------------------------------\n",
    "\n",
    "This directory contains the training set for\n",
    "The Convolutional Neural Network (CNN)\n",
    "Used in this project\n",
    "\n",
    "However, it can be used for any classifier\n",
    "desired by the person using the code and\n",
    "additionally, it is not limited to a specific\n",
    "traffic sign templates.\n",
    " \n",
    "\n",
    "----------------------------------------------\n",
    "Content\n",
    "----------------------------------------------\n",
    "\n",
    "The number of example is based on the number:\n",
    "->of traffic signs that were used as templates\n",
    "->of the image manipulation processes\n",
    "->of the brighness variations values used\n",
    "->of the blending procedures\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Image format and naming\n",
    "----------------------------------------------\n",
    "The images created are of \"jpg\" format\n",
    "with RGBA channels\n",
    "\n",
    "   SIGN_X/XXX_YYY.jpg\n",
    "\n",
    "The initial part (X) is used to distinguish the\n",
    "sign class, while the remaining (XXX_YYY) firstly\n",
    "indicated the sign in the file itself and the\n",
    "example number.\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Additional information\n",
    "----------------------------------------------\n",
    "\n",
    "contact email: \n",
    "    \n",
    "\tasterga@essex.ac.uk\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "Alexandros Stergiou\n",
    "\"The Driver's Assistant\"\n",
    "\n",
    "University of Essex,\n",
    "School of Computer Science and\n",
    "Electronic Engineering,\n",
    "UK\n",
    "----------------------------------------------\n",
    "'''\n",
    "text_file = open(\"SGTSD/Readme_Images.txt\", \"w\")\n",
    "text_file.write(content)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using exposure_manipulation\n",
    "def create_paths_list(imgs_directory, bg_directory):\n",
    "    directories = []\n",
    "    for places in load_paths(imgs_directory): #List of places: either UK_rural or UK_urban\n",
    "        for imgs in load_paths(places): #Folder for each bg image: eg. IMG_0\n",
    "            dr = imgs.split('/')\n",
    "            bg = bg_directory + '/' + dr[-2] + '/' + dr[-1] + \".png\" #Retrieving relevant bg image\n",
    "            for signs in load_paths(imgs): #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 0_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, bg])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of paths for all SGTSD relevant files using fade_manipulation; backgrounds are assigned to \n",
    "def create_assigned_paths_list(imgs_directory, bg_directory): #TODO: is this the same as above?\n",
    "    directories = []\n",
    "    for places in load_paths(bg_directory): #Folder for each place: eg. UK_rural\n",
    "        for imgs in load_paths(places): #Iterate through each b.g. image: eg. IMG_0\n",
    "            for signs in load_paths(imgs_directory):  #Folder for each sign type: eg. SIGN_9\n",
    "                for dmgs in load_paths(signs): #Folder for each damage type: eg. 9_HOLES\n",
    "                    for png in load_paths(dmgs):\n",
    "                        directories.append([png, imgs])\n",
    "    return directories #Directory for every single FILE and it's relevant bg FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if original is True:\n",
    "    directories = create_paths_list(\"Traffic_Signs_Exposure_Manipulation\",\"Google_search_backgrounds\")\n",
    "else:\n",
    "    directories = create_assigned_paths_list(\"Traffic_Signs_Fade_Manipulation\",\"Google_search_backgrounds\")\n",
    "print(\"Files to be generated: \" + str(len(directories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths of images needed to generate examples for 'sign' with damage 'dmg'\n",
    "def list_for_sign_x(sign, dmg, directories):\n",
    "    l = []\n",
    "    for elements in directories:\n",
    "        foreground = elements[0].split('/')\n",
    "        if (foreground[-2] == sign + dmg): #Eg. if (9_YELLOW == 4_ORIGINAL)\n",
    "            l.append(elements)\n",
    "    return l #Directory for every single sign and it's relevant background image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_directories = [] #Reformat list to have each sign and damage as their own dimensions\n",
    "signs = load_paths('Traffic_Signs_Templates/1_Images')\n",
    "dmgs = load_paths('Traffic_Signs_Templates/3_Damaged_Images')\n",
    "for i in signs:\n",
    "    head, tail = ntpath.split(i)\n",
    "    sign, extension = tail.split('.') #Eg. sign == \"9\"\n",
    "\n",
    "    sign_list = [] #List of damages, which are each list of signs\n",
    "    for dmg in damage_types: #damage_types is from 'def damage_images:' cell\n",
    "        sign_list.append(list_for_sign_x(sign, dmg, directories))\n",
    "    final_directories.append(sign_list) #List of types -> lists of damages -> lists of signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This generates all of the combined sign + background images (>1,000,000 originally)\n",
    "direct = \"SGTSD/Images\"\n",
    "folders = load_paths(direct)\n",
    "n = [] #Numbers from the folder names for the signs\n",
    "for folder in folders:\n",
    "    head, tail = ntpath.split(folder)\n",
    "    n.append(tail)\n",
    "    \n",
    "i = 0\n",
    "for signs in final_directories: #Iterating through sign types\n",
    "    print(\"Processed: \" + str(float(i) / float(len(final_directories)) * 100) + \" %\")\n",
    "    j = 0\n",
    "    for damages in signs: #Iterating through damage types\n",
    "        k = 0\n",
    "        for dirs in damages: #dirs == the foreground and background images for one generated sign\n",
    "            image = new_data(dirs[0], dirs[1]) #Combining background with exposure modified sign foreground\n",
    "#             image.save(direct+\"/\"+n[i]+\"/\"+n[i]+\"_\"+str(j)+\"/\"+n[i]+\"_\"+str(j)+\"_\"+str(k)+\".png\")\n",
    "            cv2.imwrite(direct+\"/\"+n[i]+\"/\"+n[i]+\"_\"+str(j)+\"/\"+n[i]+\"_\"+str(j)+\"_\"+str(k)+\".png\", image)\n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i+1\n",
    "print(\"Processed: \"+str(100)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "-------------------------------------\n",
    "BREAKDOWN OF FILES GENERATED BY CLASS\n",
    "-------------------------------------\n",
    "'''\n",
    "total = 0\n",
    "for i in range(len(final_directories)):\n",
    "    current = 0\n",
    "    for j in range(len(final_directories[i])):\n",
    "        current = current + len(final_directories[i][j])\n",
    "    s = \"Generated \" + str(current) + \" examples for sign class \" + str(i + 1)\n",
    "    string = string + '\\n' + s + '\\n'\n",
    "    total = total + current\n",
    "string = string + '\\n' + \"TOTAL: \" + str(total) + '\\n' + \"Generated on: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M\") + '\\n'\n",
    "string = string + \"-------------------------------------\"\n",
    "text_file = open(\"SGTSD/generated_images_about.txt\", \"w\")\n",
    "text_file.write(string)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def png_to_jpeg(filepath):\n",
    "    dirs = filepath.split('/')\n",
    "    title,extension = dirs[-1].split('.')\n",
    "    del dirs[-1]\n",
    "    string = '/'.join(dirs)\n",
    "    string = string + '/' + title + \".jpg\"\n",
    "    png = Image.open(filepath)\n",
    "    png.load() # required for png.split()\n",
    "    background = Image.new(\"RGB\", png.size, (255, 255, 255))\n",
    "    background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
    "    background.save(string, 'JPEG', quality=100)\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = direct = \"SGTSD/Images\"\n",
    "i = 1\n",
    "for path in load_paths(dirs):\n",
    "    print(\"Processed: \" + str(float(i - 1) / float(len(final_directories)) * 100) + \" %\")\n",
    "    for image in load_paths(path):\n",
    "        if (image.endswith(\"png\")):\n",
    "            png_to_jpeg(image)\n",
    "    i = i + 1\n",
    "print(\"Processed: \" + str(100) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Exposure_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Fade_Manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Traffic_Signs_Templates/4_Transformed_Images\")\n",
    "shutil.rmtree(\"Traffic_Signs_Templates/3_Damaged_Images\")\n",
    "shutil.rmtree(\"Traffic_Signs_Templates/2_Processed_Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"SGTSD\") #Be careful with this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
